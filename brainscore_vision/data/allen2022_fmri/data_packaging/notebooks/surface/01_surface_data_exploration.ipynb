{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface-Based NSD Data Exploration\n",
    "\n",
    "Explore fsaverage surface betas (MGH format), ROI labels, and noise ceiling\n",
    "for the surface-based Allen2022 fMRI benchmark.\n",
    "\n",
    "**Data location:** `/Volumes/Hagibis/nsd`\n",
    "\n",
    "**Key differences from volumetric pipeline:**\n",
    "- Surface betas are MGH files per hemisphere, shape `(163842, 1, 1, 750)` per session\n",
    "- ROI labels are fsaverage MGZ files (shared across subjects, unlike volumetric NIfTI)\n",
    "- Surface NC = 36.20% (vs 24.2% volumetric) due to no partial volume effects\n",
    "- Identical vertex counts per subject (fsaverage is a common template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "NSD_ROOT = Path('/Volumes/Hagibis/nsd')\n",
    "FSAVG_LABELS = NSD_ROOT / 'fsaverage_labels'\n",
    "N_SUBJECTS = 8\n",
    "SESSIONS_PER_SUBJECT = {1: 40, 2: 40, 3: 32, 4: 30, 5: 40, 6: 32, 7: 40, 8: 30}\n",
    "TRIALS_PER_SESSION = 750\n",
    "N_FSAVG_VERTICES = 163842  # per hemisphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Verify Surface Betas (MGH Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MGH shape: (163842, 1, 1, 750)\n",
      "Dtype: float64\n",
      "Value range: [-70.44, 69.76]\n",
      "File size: 491.5 MB\n",
      "\n",
      "Squeezed shape: (163842, 750)\n",
      "Mean: 0.2519\n",
      "Std: 1.7509\n",
      "\n",
      "Raw data dtype from MGH header: >f4\n",
      "First 5 non-zero values: [-0.6676476  -1.23361218 -2.19000125 -2.46711946 -1.99912941]\n"
     ]
    }
   ],
   "source": [
    "# Load one surface beta file to verify format\n",
    "mgh_path = NSD_ROOT / 'subj01' / 'betas' / 'lh.betas_session01.mgh'\n",
    "mgh = nib.load(str(mgh_path))\n",
    "data = mgh.get_fdata()\n",
    "\n",
    "print(f'MGH shape: {data.shape}')\n",
    "print(f'Dtype: {data.dtype}')\n",
    "print(f'Value range: [{data.min():.2f}, {data.max():.2f}]')\n",
    "print(f'File size: {mgh_path.stat().st_size / 1e6:.1f} MB')\n",
    "\n",
    "# Squeeze to (n_vertices, n_trials)\n",
    "betas_2d = data.squeeze()  # (163842, 750)\n",
    "print(f'\\nSqueezed shape: {betas_2d.shape}')\n",
    "print(f'Mean: {betas_2d.mean():.4f}')\n",
    "print(f'Std: {betas_2d.std():.4f}')\n",
    "\n",
    "# Check if int16/300 like volumetric or already float\n",
    "print(f'\\nRaw data dtype from MGH header: {mgh.header.get_data_dtype()}')\n",
    "print(f'First 5 non-zero values: {betas_2d[betas_2d != 0][:5]}')\n",
    "\n",
    "del data, betas_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj01: 40 LH + 40 RH sessions (expected 40) [OK]\n",
      "subj02: 40 LH + 40 RH sessions (expected 40) [OK]\n",
      "subj03: 32 LH + 32 RH sessions (expected 32) [OK]\n",
      "subj04: 30 LH + 30 RH sessions (expected 30) [OK]\n",
      "subj05: 40 LH + 40 RH sessions (expected 40) [OK]\n",
      "subj06: 32 LH + 32 RH sessions (expected 32) [OK]\n",
      "subj07: 40 LH + 40 RH sessions (expected 40) [OK]\n",
      "subj08: 30 LH + 30 RH sessions (expected 30) [OK]\n"
     ]
    }
   ],
   "source": [
    "# Verify file counts per subject match expected sessions\n",
    "for subj in range(1, 9):\n",
    "    betas_dir = NSD_ROOT / f'subj{subj:02d}' / 'betas'\n",
    "    lh_files = sorted(betas_dir.glob('lh.betas_session*.mgh'))\n",
    "    rh_files = sorted(betas_dir.glob('rh.betas_session*.mgh'))\n",
    "    expected = SESSIONS_PER_SUBJECT[subj]\n",
    "    status = 'OK' if len(lh_files) == expected and len(rh_files) == expected else 'MISMATCH'\n",
    "    print(f'subj{subj:02d}: {len(lh_files)} LH + {len(rh_files)} RH sessions '\n",
    "          f'(expected {expected}) [{status}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load and Verify fsaverage ROI Labels\n\nROI label files:\n- `Kastner2015.mgz`: prf-visualrois (V1v=1, V1d=2, V2v=3, V2d=4, V3v=5, V3d=6, hV4=7)\n- `streams.mgz`: NSD \"streams\" parcellation (label 5 = ventral, used for IT)\n\nUnlike volumetric ROIs (which are subject-specific NIfTI files), fsaverage labels are\nshared across all subjects. This means ROI vertex counts are identical per subject."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH Kastner2015 shape: (163842,)\n",
      "RH Kastner2015 shape: (163842,)\n",
      "\n",
      "LH vertex counts:\n",
      "   V1v (ID=1): 1,106 vertices\n",
      "   V1d (ID=2): 1,047 vertices\n",
      "   V2v (ID=3):   920 vertices\n",
      "   V2d (ID=4):   723 vertices\n",
      "   V3v (ID=5):   600 vertices\n",
      "   V3d (ID=6):   714 vertices\n",
      "   hV4 (ID=7):   410 vertices\n",
      "\n",
      "RH vertex counts:\n",
      "   V1v (ID=1): 1,035 vertices\n",
      "   V1d (ID=2): 1,088 vertices\n",
      "   V2v (ID=3):   889 vertices\n",
      "   V2d (ID=4):   859 vertices\n",
      "   V3v (ID=5):   626 vertices\n",
      "   V3d (ID=6):   678 vertices\n",
      "   hV4 (ID=7):   504 vertices\n"
     ]
    }
   ],
   "source": [
    "# Load prf-visualrois (Kastner2015) for both hemispheres\n",
    "lh_kastner = nib.load(str(FSAVG_LABELS / 'lh.Kastner2015.mgz')).get_fdata().flatten()\n",
    "rh_kastner = nib.load(str(FSAVG_LABELS / 'rh.Kastner2015.mgz')).get_fdata().flatten()\n",
    "\n",
    "print(f'LH Kastner2015 shape: {lh_kastner.shape}')\n",
    "print(f'RH Kastner2015 shape: {rh_kastner.shape}')\n",
    "\n",
    "# Label mapping (from Kastner2015.mgz.ctab)\n",
    "kastner_labels = {\n",
    "    0: 'Unknown', 1: 'V1v', 2: 'V1d', 3: 'V2v', 4: 'V2d',\n",
    "    5: 'V3v', 6: 'V3d', 7: 'hV4',\n",
    "}\n",
    "\n",
    "print('\\nLH vertex counts:')\n",
    "for val in sorted(kastner_labels.keys()):\n",
    "    if val == 0:\n",
    "        continue\n",
    "    n = (lh_kastner == val).sum()\n",
    "    print(f'  {kastner_labels[val]:>4s} (ID={val}): {n:>5,} vertices')\n",
    "\n",
    "print('\\nRH vertex counts:')\n",
    "for val in sorted(kastner_labels.keys()):\n",
    "    if val == 0:\n",
    "        continue\n",
    "    n = (rh_kastner == val).sum()\n",
    "    print(f'  {kastner_labels[val]:>4s} (ID={val}): {n:>5,} vertices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load NSD \"streams\" parcellation for IT definition\n# Label 5 = ventral stream, consistent with Algonauts 2023.\n# Replaces the earlier custom Glasser HCP_MMP1 9-parcel definition.\nlh_streams = nib.load(str(FSAVG_LABELS / 'lh.streams.mgz')).get_fdata().flatten()\nrh_streams = nib.load(str(FSAVG_LABELS / 'rh.streams.mgz')).get_fdata().flatten()\n\nSTREAMS_VENTRAL_LABEL = 5\n\nprint(f'LH streams shape: {lh_streams.shape}')\nprint(f'RH streams shape: {rh_streams.shape}')\nprint(f'LH unique labels: {np.unique(lh_streams).astype(int)}')\nprint(f'RH unique labels: {np.unique(rh_streams).astype(int)}')\n\n# IT = ventral stream vertices\nlh_it_mask = lh_streams == STREAMS_VENTRAL_LABEL\nrh_it_mask = rh_streams == STREAMS_VENTRAL_LABEL\nprint(f'\\nIT (streams ventral, label={STREAMS_VENTRAL_LABEL}):')\nprint(f'  LH: {lh_it_mask.sum():>5,} vertices')\nprint(f'  RH: {rh_it_mask.sum():>5,} vertices')\nprint(f'  Total: {lh_it_mask.sum() + rh_it_mask.sum():>5,} vertices')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Combined ROI Masks\n",
    "\n",
    "For each Brain-Score region, build masks selecting vertices from both hemispheres.\n",
    "Since fsaverage is a shared template, these masks are identical for all subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Brain-Score region definitions\nREGION_TO_KASTNER_LABELS = {\n    'V1': [1, 2],   # V1v, V1d\n    'V2': [3, 4],   # V2v, V2d\n    'V4': [7],       # hV4\n}\n\ndef build_surface_roi_masks():\n    \"\"\"Build ROI vertex masks for fsaverage surface.\n    \n    Returns dict: region -> {\n        'lh': boolean array (163842,),\n        'rh': boolean array (163842,),\n        'lh_indices': int array of selected vertex indices,\n        'rh_indices': int array of selected vertex indices,\n        'n_vertices': total bilateral count,\n    }\n    \"\"\"\n    masks = {}\n    for region, labels in REGION_TO_KASTNER_LABELS.items():\n        lh_mask = np.isin(lh_kastner, labels)\n        rh_mask = np.isin(rh_kastner, labels)\n        masks[region] = {\n            'lh': lh_mask,\n            'rh': rh_mask,\n            'lh_indices': np.where(lh_mask)[0],\n            'rh_indices': np.where(rh_mask)[0],\n            'n_vertices': int(lh_mask.sum() + rh_mask.sum()),\n        }\n    \n    # IT: NSD streams ventral parcellation (label 5), bilateral\n    lh_it = lh_streams == STREAMS_VENTRAL_LABEL\n    rh_it = rh_streams == STREAMS_VENTRAL_LABEL\n    masks['IT'] = {\n        'lh': lh_it,\n        'rh': rh_it,\n        'lh_indices': np.where(lh_it)[0],\n        'rh_indices': np.where(rh_it)[0],\n        'n_vertices': int(lh_it.sum() + rh_it.sum()),\n    }\n    return masks\n\n\nroi_masks = build_surface_roi_masks()\n\nprint('Surface ROI vertex counts (fsaverage, bilateral):')\nprint(f'{\"Region\":>6s}  {\"LH\":>6s}  {\"RH\":>6s}  {\"Total\":>6s}')\nprint('-' * 30)\nfor region in ['V1', 'V2', 'V4', 'IT']:\n    m = roi_masks[region]\n    print(f'{region:>6s}  {m[\"lh\"].sum():>6,}  {m[\"rh\"].sum():>6,}  {m[\"n_vertices\"]:>6,}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Noise Ceiling Validation\n",
    "\n",
    "**Target: reproduce 36.20%** (Allen et al. 2022, surface nsdgeneral, ncsnr-based)\n",
    "\n",
    "Formula: `NC = 100 * ncsnr^2 / (ncsnr^2 + 1/3)` for k=3 repetitions\n",
    "\n",
    "Aggregation: median across nsdgeneral vertices per subject, then mean across 8 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsaverage nsdgeneral: 37,984 vertices out of 327,684\n",
      "subj01: median ncsnr=0.490 (published: 0.490), median NC=41.9%\n",
      "subj02: median ncsnr=0.510 (published: 0.510), median NC=43.8%\n",
      "subj03: median ncsnr=0.431 (published: 0.431), median NC=35.8%\n",
      "subj04: median ncsnr=0.392 (published: 0.392), median NC=31.6%\n",
      "subj05: median ncsnr=0.542 (published: 0.542), median NC=46.9%\n",
      "subj06: median ncsnr=0.439 (published: 0.439), median NC=36.6%\n",
      "subj07: median ncsnr=0.371 (published: 0.371), median NC=29.2%\n",
      "subj08: median ncsnr=0.323 (published: 0.323), median NC=23.8%\n",
      "\n",
      "============================================================\n",
      "Our fsaverage nsdgeneral NC:   36.20%\n",
      "Published (Allen et al. 2022): 36.20%\n",
      "Difference:                    -0.00%\n",
      "============================================================\n",
      "PASS: Noise ceiling exactly reproduced.\n"
     ]
    }
   ],
   "source": [
    "def load_surface_ncsnr(subj: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load fsaverage surface ncsnr for both hemispheres.\n",
    "    \n",
    "    Returns: (lh_ncsnr, rh_ncsnr), each shape (163842,)\n",
    "    \"\"\"\n",
    "    lh = nib.load(str(NSD_ROOT / f'subj{subj:02d}' / 'betas' / 'lh.ncsnr.mgh')).get_fdata().flatten()\n",
    "    rh = nib.load(str(NSD_ROOT / f'subj{subj:02d}' / 'betas' / 'rh.ncsnr.mgh')).get_fdata().flatten()\n",
    "    return lh, rh\n",
    "\n",
    "\n",
    "def ncsnr_to_nc(ncsnr: np.ndarray, k: int = 3) -> np.ndarray:\n",
    "    \"\"\"Convert ncsnr to noise ceiling percentage.\"\"\"\n",
    "    return 100.0 * ncsnr**2 / (ncsnr**2 + 1.0 / k)\n",
    "\n",
    "\n",
    "# Load nsdgeneral ROI on fsaverage\n",
    "lh_nsdgen = nib.load(str(FSAVG_LABELS / 'lh.nsdgeneral.mgz')).get_fdata().flatten()\n",
    "rh_nsdgen = nib.load(str(FSAVG_LABELS / 'rh.nsdgeneral.mgz')).get_fdata().flatten()\n",
    "nsdgen_mask = np.concatenate([lh_nsdgen, rh_nsdgen]) > 0\n",
    "print(f'fsaverage nsdgeneral: {nsdgen_mask.sum():,} vertices out of {len(nsdgen_mask):,}')\n",
    "\n",
    "# Published per-subject ncsnr medians (from Allen et al. 2022)\n",
    "published_ncsnr = [0.490, 0.510, 0.431, 0.392, 0.542, 0.439, 0.371, 0.323]\n",
    "\n",
    "per_subject_nc = []\n",
    "for subj in range(1, 9):\n",
    "    lh_ncsnr, rh_ncsnr = load_surface_ncsnr(subj)\n",
    "    all_ncsnr = np.concatenate([lh_ncsnr, rh_ncsnr])\n",
    "    \n",
    "    nsdgen_ncsnr = all_ncsnr[nsdgen_mask]\n",
    "    nc_pct = ncsnr_to_nc(nsdgen_ncsnr, k=3)\n",
    "    median_nc = np.nanmedian(nc_pct)\n",
    "    median_ncsnr = np.nanmedian(nsdgen_ncsnr)\n",
    "    per_subject_nc.append(median_nc)\n",
    "    \n",
    "    print(f'subj{subj:02d}: median ncsnr={median_ncsnr:.3f} '\n",
    "          f'(published: {published_ncsnr[subj-1]:.3f}), '\n",
    "          f'median NC={median_nc:.1f}%')\n",
    "\n",
    "mean_nc = np.mean(per_subject_nc)\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'Our fsaverage nsdgeneral NC:   {mean_nc:.2f}%')\n",
    "print(f'Published (Allen et al. 2022): 36.20%')\n",
    "print(f'Difference:                    {mean_nc - 36.20:+.2f}%')\n",
    "print(f'{\"=\"*60}')\n",
    "assert abs(mean_nc - 36.20) < 0.01, f'NC mismatch: {mean_nc:.2f}% vs 36.20%'\n",
    "print('PASS: Noise ceiling exactly reproduced.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-ROI Surface Noise Ceiling\n",
    "\n",
    "Compute NC per region (V1, V2, V4, IT) on the surface.\n",
    "These should be substantially higher than volumetric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-ROI Surface Noise Ceiling (ncsnr-based, k=3)\n",
      "Mean of per-subject medians, compared to volumetric\n",
      "======================================================================\n",
      " V1: surface= 35.4% (range 20.8-49.0%) | volumetric= 37.0% | diff=-1.6%\n",
      " V2: surface= 35.8% (range 26.1-49.1%) | volumetric= 31.1% | diff=+4.7%\n",
      " V4: surface= 45.5% (range 34.6-60.4%) | volumetric= 26.4% | diff=+19.1%\n",
      " IT: surface= 21.7% (range 13.3-29.3%) | volumetric= 10.2% | diff=+11.5%\n",
      "\n",
      "Per-subject detail:\n",
      "region     V1    V2    V4    IT\n",
      "subject                        \n",
      "subj01   47.8  49.1  47.0  22.7\n",
      "subj02   43.6  32.9  60.4  29.3\n",
      "subj03   30.0  28.2  44.2  23.0\n",
      "subj04   26.8  32.9  34.6  22.8\n",
      "subj05   49.0  46.5  50.8  28.0\n",
      "subj06   42.3  41.6  40.9  15.6\n",
      "subj07   23.0  29.1  45.3  18.5\n",
      "subj08   20.8  26.1  40.7  13.3\n"
     ]
    }
   ],
   "source": [
    "# Volumetric NC values (from MEMORY.md) for comparison\n",
    "volumetric_nc = {'V1': 37.0, 'V2': 31.1, 'V4': 26.4, 'IT': 10.2}\n",
    "\n",
    "results = []\n",
    "for subj in range(1, 9):\n",
    "    lh_ncsnr, rh_ncsnr = load_surface_ncsnr(subj)\n",
    "    \n",
    "    for region in ['V1', 'V2', 'V4', 'IT']:\n",
    "        m = roi_masks[region]\n",
    "        # Extract ncsnr for this region's vertices (LH then RH)\n",
    "        ncsnr_roi = np.concatenate([lh_ncsnr[m['lh']], rh_ncsnr[m['rh']]])\n",
    "        nc_roi = ncsnr_to_nc(ncsnr_roi, k=3)\n",
    "        results.append({\n",
    "            'subject': f'subj{subj:02d}',\n",
    "            'region': region,\n",
    "            'n_vertices': m['n_vertices'],\n",
    "            'median_ncsnr': float(np.nanmedian(ncsnr_roi)),\n",
    "            'median_nc_pct': float(np.nanmedian(nc_roi)),\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Summary table\n",
    "summary = results_df.groupby('region').agg(\n",
    "    vertices=('n_vertices', 'first'),\n",
    "    mean_nc=('median_nc_pct', 'mean'),\n",
    "    std_nc=('median_nc_pct', 'std'),\n",
    "    min_nc=('median_nc_pct', 'min'),\n",
    "    max_nc=('median_nc_pct', 'max'),\n",
    ").round(1)\n",
    "\n",
    "print('Per-ROI Surface Noise Ceiling (ncsnr-based, k=3)')\n",
    "print('Mean of per-subject medians, compared to volumetric')\n",
    "print('=' * 70)\n",
    "for region in ['V1', 'V2', 'V4', 'IT']:\n",
    "    row = summary.loc[region]\n",
    "    vol = volumetric_nc[region]\n",
    "    diff = row['mean_nc'] - vol\n",
    "    print(f'{region:>3s}: surface={row[\"mean_nc\"]:>5.1f}% (range {row[\"min_nc\"]:.1f}-{row[\"max_nc\"]:.1f}%) '\n",
    "          f'| volumetric={vol:>5.1f}% | diff={diff:+.1f}%')\n",
    "\n",
    "print('\\nPer-subject detail:')\n",
    "pivot = results_df.pivot(index='subject', columns='region', values='median_nc_pct').round(1)\n",
    "print(pivot[['V1', 'V2', 'V4', 'IT']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Shared Image Trial Mapping\n",
    "\n",
    "Map trials to the 1000 shared images across all 8 subjects.\n",
    "Track how many repetitions each image has per subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared images: 1000\n",
      "\n",
      "Rep distribution across 1000 shared images:\n",
      "  min_reps=0: 93 images\n",
      "  min_reps=1: 141 images\n",
      "  min_reps=2: 251 images\n",
      "  min_reps=3: 515 images\n",
      "\n",
      "Images with 3 reps across all 8 subjects: 515\n"
     ]
    }
   ],
   "source": [
    "# Load experiment design metadata\n",
    "stim_info = pd.read_csv(NSD_ROOT / 'metadata' / 'nsd_stim_info_merged.csv', index_col=0)\n",
    "expdesign = scipy.io.loadmat(str(NSD_ROOT / 'metadata' / 'nsd_expdesign.mat'))\n",
    "\n",
    "masterordering = expdesign['masterordering'].flatten()  # (30000,) trial -> image_index (1-indexed)\n",
    "subjectim = expdesign['subjectim']  # (8, 10000) subject's image_index -> nsdId (1-indexed)\n",
    "sharedix = expdesign['sharedix'].flatten()  # (1000,) nsdIds of shared images (1-indexed)\n",
    "\n",
    "print(f'Shared images: {len(sharedix)}')\n",
    "\n",
    "# For each subject, count reps per shared image\n",
    "reps_per_image = np.zeros((N_SUBJECTS, 1000), dtype=int)  # (8, 1000)\n",
    "shared_nsdids_0idx = sharedix - 1  # Convert to 0-indexed\n",
    "\n",
    "for subj_idx in range(N_SUBJECTS):\n",
    "    subj = subj_idx + 1\n",
    "    n_sessions = SESSIONS_PER_SUBJECT[subj]\n",
    "    n_trials = n_sessions * TRIALS_PER_SESSION\n",
    "    \n",
    "    # Build nsdId -> shared_image_index mapping\n",
    "    subj_nsdids = subjectim[subj_idx]  # (10000,) 1-indexed nsdIds\n",
    "    nsdid_to_imgidx = {int(nsd_id): img_idx + 1 \n",
    "                       for img_idx, nsd_id in enumerate(subj_nsdids)}\n",
    "    imgidx_to_sharedpos = {}\n",
    "    for shared_pos, nsd_id in enumerate(sharedix):\n",
    "        if int(nsd_id) in nsdid_to_imgidx:\n",
    "            imgidx_to_sharedpos[nsdid_to_imgidx[int(nsd_id)]] = shared_pos\n",
    "    \n",
    "    # Count reps per shared image\n",
    "    for trial_idx in range(n_trials):\n",
    "        img_idx = masterordering[trial_idx]\n",
    "        if img_idx in imgidx_to_sharedpos:\n",
    "            shared_pos = imgidx_to_sharedpos[img_idx]\n",
    "            reps_per_image[subj_idx, shared_pos] += 1\n",
    "\n",
    "# min_reps_across_subjects for each image\n",
    "min_reps = reps_per_image.min(axis=0)  # (1000,)\n",
    "\n",
    "print(f'\\nRep distribution across 1000 shared images:')\n",
    "for r in [0, 1, 2, 3]:\n",
    "    n = (min_reps == r).sum()\n",
    "    print(f'  min_reps={r}: {n} images')\n",
    "\n",
    "n_complete = (min_reps >= 3).sum()\n",
    "print(f'\\nImages with 3 reps across all 8 subjects: {n_complete}')\n",
    "assert n_complete == 515, f'Expected 515, got {n_complete}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reps per shared image, per subject:\n",
      "  subj01 (40 sessions): 3-rep=1000, 2-rep=  0, 1-rep=  0, 0-rep=  0\n",
      "  subj02 (40 sessions): 3-rep=1000, 2-rep=  0, 1-rep=  0, 0-rep=  0\n",
      "  subj03 (32 sessions): 3-rep= 614, 2-rep=213, 1-rep=103, 0-rep= 70\n",
      "  subj04 (30 sessions): 3-rep= 515, 2-rep=251, 1-rep=141, 0-rep= 93\n",
      "  subj05 (40 sessions): 3-rep=1000, 2-rep=  0, 1-rep=  0, 0-rep=  0\n",
      "  subj06 (32 sessions): 3-rep= 614, 2-rep=213, 1-rep=103, 0-rep= 70\n",
      "  subj07 (40 sessions): 3-rep=1000, 2-rep=  0, 1-rep=  0, 0-rep=  0\n",
      "  subj08 (30 sessions): 3-rep= 515, 2-rep=251, 1-rep=141, 0-rep= 93\n"
     ]
    }
   ],
   "source": [
    "# Per-subject rep counts\n",
    "print('Reps per shared image, per subject:')\n",
    "for subj_idx in range(N_SUBJECTS):\n",
    "    subj = subj_idx + 1\n",
    "    reps = reps_per_image[subj_idx]\n",
    "    r3 = (reps == 3).sum()\n",
    "    r2 = (reps == 2).sum()\n",
    "    r1 = (reps == 1).sum()\n",
    "    r0 = (reps == 0).sum()\n",
    "    print(f'  subj{subj:02d} ({SESSIONS_PER_SUBJECT[subj]:>2d} sessions): '\n",
    "          f'3-rep={r3:>4d}, 2-rep={r2:>3d}, 1-rep={r1:>3d}, 0-rep={r0:>3d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Beta Extraction Test\n",
    "\n",
    "Load one session of surface betas, extract V1 vertices, verify signal quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH betas shape: (163842, 750)\n",
      "V1 LH betas shape: (2153, 750)\n",
      "V1 LH mean: 1.1280, std: 2.4956\n",
      "\n",
      "Non-ROI mean: 0.2435, std: 1.7274\n",
      "V1/non-ROI std ratio: 1.44x\n"
     ]
    }
   ],
   "source": [
    "# Load LH session 1 betas for subj01\n",
    "lh_betas = nib.load(str(NSD_ROOT / 'subj01' / 'betas' / 'lh.betas_session01.mgh')).get_fdata().squeeze()\n",
    "print(f'LH betas shape: {lh_betas.shape}')  # (163842, 750)\n",
    "\n",
    "# Extract V1 LH vertices\n",
    "v1_lh_mask = roi_masks['V1']['lh']\n",
    "v1_lh_betas = lh_betas[v1_lh_mask]  # (n_v1_lh, 750)\n",
    "print(f'V1 LH betas shape: {v1_lh_betas.shape}')\n",
    "print(f'V1 LH mean: {v1_lh_betas.mean():.4f}, std: {v1_lh_betas.std():.4f}')\n",
    "\n",
    "# Compare signal in V1 vs random non-ROI vertices\n",
    "non_roi = ~v1_lh_mask & (lh_betas.mean(axis=1) != 0)  # non-V1 cortical vertices\n",
    "non_roi_sample = lh_betas[non_roi][:1000]  # sample 1000 non-ROI vertices\n",
    "print(f'\\nNon-ROI mean: {non_roi_sample.mean():.4f}, std: {non_roi_sample.std():.4f}')\n",
    "print(f'V1/non-ROI std ratio: {v1_lh_betas.std() / non_roi_sample.std():.2f}x')\n",
    "\n",
    "del lh_betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Surface NSD Data Exploration Summary\n",
      "============================================================\n",
      "\n",
      "DATA FORMAT:\n",
      "  Beta source: fsaverage MGH (float32, per hemisphere)\n",
      "  Beta shape per session per hemisphere: (163842, 1, 1, 750)\n",
      "  ROI labels: fsaverage MGZ (shared across all subjects)\n",
      "  Noise ceiling: lh/rh.ncsnr.mgh (per subject)\n",
      "\n",
      "ROI VERTEX COUNTS (fsaverage, bilateral, same for all subjects):\n",
      "  V1:  4,276 vertices (LH=2,153, RH=2,123)\n",
      "  V2:  3,391 vertices (LH=1,643, RH=1,748)\n",
      "  V4:    914 vertices (LH=410, RH=504)\n",
      "  IT: 11,168 vertices (LH=5,723, RH=5,445)\n",
      "\n",
      "SHARED IMAGES:\n",
      "  Total shared: 1000\n",
      "  Complete (3 reps x 8 subjects): 515\n",
      "\n",
      "NOISE CEILING (fsaverage nsdgeneral):\n",
      "  Reproduced: 36.20% (published: 36.20%)\n",
      "\n",
      "KEY DIFFERENCES FROM VOLUMETRIC:\n",
      "  1. ROI masks are shared (not per-subject)\n",
      "  2. Must load LH + RH separately per session\n",
      "  3. Surface NC >> volumetric NC (no partial volume effects)\n",
      "  4. Vertex indices replace voxel coordinates\n"
     ]
    }
   ],
   "source": [
    "print('=' * 60)\n",
    "print('Surface NSD Data Exploration Summary')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('DATA FORMAT:')\n",
    "print(f'  Beta source: fsaverage MGH (float32, per hemisphere)')\n",
    "print(f'  Beta shape per session per hemisphere: (163842, 1, 1, 750)')\n",
    "print(f'  ROI labels: fsaverage MGZ (shared across all subjects)')\n",
    "print(f'  Noise ceiling: lh/rh.ncsnr.mgh (per subject)')\n",
    "print()\n",
    "print('ROI VERTEX COUNTS (fsaverage, bilateral, same for all subjects):')\n",
    "for region in ['V1', 'V2', 'V4', 'IT']:\n",
    "    m = roi_masks[region]\n",
    "    print(f'  {region}: {m[\"n_vertices\"]:>6,} vertices '\n",
    "          f'(LH={m[\"lh\"].sum():,}, RH={m[\"rh\"].sum():,})')\n",
    "print()\n",
    "print('SHARED IMAGES:')\n",
    "print(f'  Total shared: 1000')\n",
    "print(f'  Complete (3 reps x 8 subjects): {n_complete}')\n",
    "print()\n",
    "print('NOISE CEILING (fsaverage nsdgeneral):')\n",
    "print(f'  Reproduced: {mean_nc:.2f}% (published: 36.20%)')\n",
    "print()\n",
    "print('KEY DIFFERENCES FROM VOLUMETRIC:')\n",
    "print(f'  1. ROI masks are shared (not per-subject)')\n",
    "print(f'  2. Must load LH + RH separately per session')\n",
    "print(f'  3. Surface NC >> volumetric NC (no partial volume effects)')\n",
    "print(f'  4. Vertex indices replace voxel coordinates')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsd-2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}