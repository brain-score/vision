{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Data Preparation\n",
    "\n",
    "Extract session-z-scored, rep-averaged surface betas for ALL 1000 shared images,\n",
    "all 4 ROIs (V1, V2, V4, IT), all 8 subjects.\n",
    "\n",
    "**Output:** Per-region xarray DataArrays saved as netCDF.\n",
    "- `Allen2022_surface.V1.nc`, `Allen2022_surface.V2.nc`, etc.\n",
    "- Contains 1000 images (filtered to 515 at benchmark load time)\n",
    "- `min_reps_across_subjects` coordinate enables filtering\n",
    "\n",
    "**Processing chain:** raw MGH betas -> session z-score -> average reps\n",
    "\n",
    "**Estimated runtime:** ~35 min (568 MGH file loads from external drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport xarray as xr\nimport nibabel as nib\nimport scipy.io\nimport time\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nNSD_ROOT = Path('/Volumes/Hagibis/nsd')\nFSAVG_LABELS = NSD_ROOT / 'fsaverage_labels'\nOUTPUT_DIR = NSD_ROOT / 'assemblies'\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# --- Subject Configuration ---\n# Use all 8 subjects (515 shared images with 3 reps):\nSUBJECT_LIST = [1, 2, 3, 4, 5, 6, 7, 8]\n# Or use only 4 complete subjects (40 sessions each, ~1000 images):\n# SUBJECT_LIST = [1, 2, 5, 7]\n\nN_SUBJECTS = len(SUBJECT_LIST)\nALL_SESSIONS = {1: 40, 2: 40, 3: 32, 4: 30, 5: 40, 6: 32, 7: 40, 8: 30}\nSESSIONS_PER_SUBJECT = {s: ALL_SESSIONS[s] for s in SUBJECT_LIST}\n\nTRIALS_PER_SESSION = 750\nN_FSAVG_VERTICES = 163842"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ROI Masks and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load ROI labels\nlh_kastner = nib.load(str(FSAVG_LABELS / 'lh.Kastner2015.mgz')).get_fdata().flatten()\nrh_kastner = nib.load(str(FSAVG_LABELS / 'rh.Kastner2015.mgz')).get_fdata().flatten()\nlh_streams = nib.load(str(FSAVG_LABELS / 'lh.streams.mgz')).get_fdata().flatten()\nrh_streams = nib.load(str(FSAVG_LABELS / 'rh.streams.mgz')).get_fdata().flatten()\n\nREGION_TO_KASTNER_LABELS = {\n    'V1': [1, 2],   # V1v, V1d\n    'V2': [3, 4],   # V2v, V2d\n    'V4': [7],       # hV4\n}\n\n# IT: NSD \"streams\" ventral parcellation (label 5), consistent with Algonauts 2023.\n# Replaces the earlier custom Glasser HCP_MMP1 9-parcel definition.\nSTREAMS_VENTRAL_LABEL = 5\n\nREGIONS = ['V1', 'V2', 'V4', 'IT']\n\n# Build masks: region -> {'lh': bool(163842,), 'rh': bool(163842,)}\nroi_masks = {}\nfor region, labels in REGION_TO_KASTNER_LABELS.items():\n    roi_masks[region] = {\n        'lh': np.isin(lh_kastner, labels),\n        'rh': np.isin(rh_kastner, labels),\n    }\nroi_masks['IT'] = {\n    'lh': lh_streams == STREAMS_VENTRAL_LABEL,\n    'rh': rh_streams == STREAMS_VENTRAL_LABEL,\n}\n\nfor region in REGIONS:\n    n_lh = roi_masks[region]['lh'].sum()\n    n_rh = roi_masks[region]['rh'].sum()\n    print(f'{region}: LH={n_lh}, RH={n_rh}, total={n_lh + n_rh}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Trial Mapping for All 1000 Shared Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load experiment design\nstim_info = pd.read_csv(NSD_ROOT / 'metadata' / 'nsd_stim_info_merged.csv', index_col=0)\nexpdesign = scipy.io.loadmat(str(NSD_ROOT / 'metadata' / 'nsd_expdesign.mat'))\nmasterordering = expdesign['masterordering'].flatten()\nsubjectim = expdesign['subjectim']\nsharedix = expdesign['sharedix'].flatten()  # 1-indexed nsdIds\n\n\ndef get_shared_trial_info(subj_idx: int) -> pd.DataFrame:\n    \"\"\"Find trial indices for shared images for a given subject.\n    \n    Returns DataFrame with columns:\n        nsd_id: 0-indexed NSD image ID\n        shared_pos: 0-indexed position in the 1000 shared images\n        rep: repetition number (0, 1, 2)\n        session: 1-indexed session number\n        trial_in_session: 0-indexed position within session\n    \"\"\"\n    subj = subj_idx + 1\n    n_sessions = SESSIONS_PER_SUBJECT[subj]\n    n_total_trials = n_sessions * TRIALS_PER_SESSION\n    \n    subj_nsdids = subjectim[subj_idx]\n    nsdid_to_imgidx = {int(nsd_id): img_idx + 1 \n                       for img_idx, nsd_id in enumerate(subj_nsdids)}\n    \n    # Map shared nsdIds to image indices for this subject\n    shared_imgidxs = {}\n    for shared_pos, nsd_id in enumerate(sharedix):\n        nsd_id_int = int(nsd_id)\n        if nsd_id_int in nsdid_to_imgidx:\n            shared_imgidxs[nsdid_to_imgidx[nsd_id_int]] = (shared_pos, nsd_id_int - 1)\n    \n    records = []\n    rep_counter = {}\n    for trial_idx in range(n_total_trials):\n        img_idx = masterordering[trial_idx]\n        if img_idx in shared_imgidxs:\n            shared_pos, nsd_id_0 = shared_imgidxs[img_idx]\n            rep = rep_counter.get(img_idx, 0)\n            rep_counter[img_idx] = rep + 1\n            session = trial_idx // TRIALS_PER_SESSION + 1\n            trial_in_session = trial_idx % TRIALS_PER_SESSION\n            records.append({\n                'nsd_id': nsd_id_0,\n                'shared_pos': shared_pos,\n                'rep': rep,\n                'session': session,\n                'trial_in_session': trial_in_session,\n            })\n    \n    return pd.DataFrame(records)\n\n\n# Build trial info for all subjects in SUBJECT_LIST\nall_trial_info = {}\nfor subj in SUBJECT_LIST:\n    subj_idx = subj - 1\n    df = get_shared_trial_info(subj_idx)\n    all_trial_info[subj] = df\n    n_images = df['shared_pos'].nunique()\n    n_trials = len(df)\n    rep_counts = df.groupby('shared_pos')['rep'].max().value_counts().sort_index()\n    print(f'subj{subj:02d}: {n_trials} trials, {n_images} unique images, '\n          f'rep distribution: {dict(rep_counts)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute min_reps_across_subjects for each of the 1000 shared images\nreps_per_image = np.zeros((N_SUBJECTS, 1000), dtype=int)\nfor i, subj in enumerate(SUBJECT_LIST):\n    df = all_trial_info[subj]\n    for shared_pos, group in df.groupby('shared_pos'):\n        reps_per_image[i, shared_pos] = len(group)\n\nmin_reps = reps_per_image.min(axis=0)\nshared_nsdids = sharedix - 1  # 0-indexed\n\nprint(f'Subjects: {SUBJECT_LIST}')\nprint(f'min_reps distribution:')\nfor r in range(4):\n    print(f'  {r}: {(min_reps == r).sum()} images')\nprint(f'\\nImages with 3 complete reps: {(min_reps >= 3).sum()}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Extraction\n",
    "\n",
    "For each subject:\n",
    "1. Walk through sessions, load LH and RH surface betas\n",
    "2. Extract ROI vertices\n",
    "3. Z-score within session (750 trials per vertex)\n",
    "4. Collect shared-image trial betas\n",
    "5. Average across repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_surface_betas_for_subject(subj: int, trial_info: pd.DataFrame):\n    \"\"\"Extract shared-image betas for all regions from surface data.\n    \n    Returns:\n        region_betas: dict[region -> (1000, n_vertices) averaged betas]\n        region_per_rep: dict[region -> (1000, max_reps, n_vertices) per-rep betas]\n    \"\"\"\n    n_sessions = SESSIONS_PER_SUBJECT[subj]\n    \n    # Pre-allocate per-region storage\n    # For each region, collect (shared_pos, rep, beta_vector)\n    region_trials = {region: [] for region in REGIONS}\n    \n    for session in tqdm(range(1, n_sessions + 1), desc=f'subj{subj:02d}', leave=False):\n        session_trials = trial_info[trial_info['session'] == session]\n        if len(session_trials) == 0:\n            continue\n        \n        # Load LH betas, extract ROI vertices, z-score, then free memory\n        lh_path = NSD_ROOT / f'subj{subj:02d}' / 'betas' / f'lh.betas_session{session:02d}.mgh'\n        lh_betas = nib.load(str(lh_path)).get_fdata().squeeze()  # (163842, 750)\n        \n        rh_path = NSD_ROOT / f'subj{subj:02d}' / 'betas' / f'rh.betas_session{session:02d}.mgh'\n        rh_betas = nib.load(str(rh_path)).get_fdata().squeeze()  # (163842, 750)\n        \n        for region in REGIONS:\n            lh_mask = roi_masks[region]['lh']\n            rh_mask = roi_masks[region]['rh']\n            \n            # Extract ROI vertices: (n_trials, n_lh + n_rh)\n            roi_betas = np.concatenate([\n                lh_betas[lh_mask].T,   # (750, n_lh)\n                rh_betas[rh_mask].T,   # (750, n_rh)\n            ], axis=1)                 # (750, n_total)\n            \n            # Stage 1 of 2: Session z-score (Allen et al. 2022, Extended Data Fig. 8).\n            # Normalize each vertex to mean=0, std=1 within each 750-trial session.\n            # Removes within-session non-stationarities and equalizes units across\n            # vertices. This is the only normalization the NSD paper prescribes.\n            # Stage 2 (global z-score per subject) is applied later in NB03.\n            mean = roi_betas.mean(axis=0, keepdims=True)\n            std = roi_betas.std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            roi_betas = (roi_betas - mean) / std\n            \n            # Collect shared-image trials\n            for _, row in session_trials.iterrows():\n                region_trials[region].append((\n                    row['shared_pos'],\n                    row['rep'],\n                    roi_betas[row['trial_in_session']].copy(),\n                ))\n        \n        del lh_betas, rh_betas\n    \n    # Organize into arrays\n    region_averaged = {}\n    region_per_rep = {}\n    \n    for region in REGIONS:\n        n_verts = roi_masks[region]['lh'].sum() + roi_masks[region]['rh'].sum()\n        \n        # Build (1000, max_reps, n_verts) array\n        max_reps = max(r for _, r, _ in region_trials[region]) + 1 if region_trials[region] else 0\n        per_rep = np.full((1000, max(max_reps, 3), n_verts), np.nan, dtype=np.float32)\n        \n        for shared_pos, rep, beta_vec in region_trials[region]:\n            per_rep[shared_pos, rep] = beta_vec\n        \n        # Average across available reps (nanmean handles missing reps)\n        averaged = np.nanmean(per_rep, axis=1)  # (1000, n_verts)\n        \n        region_averaged[region] = averaged\n        region_per_rep[region] = per_rep\n    \n    return region_averaged, region_per_rep"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract betas for all subjects\n# This is the main compute loop: ~35 min for 568 MGH file loads\n\nall_averaged = {region: [] for region in REGIONS}  # region -> list of (1000, n_verts) per subject\nall_per_rep = {region: [] for region in REGIONS}   # region -> list of (1000, 3, n_verts) per subject\n\nt0 = time.time()\n\nfor subj in SUBJECT_LIST:\n    print(f'\\nProcessing subject {subj} ({SUBJECT_LIST.index(subj)+1}/{N_SUBJECTS})...')\n    trial_info = all_trial_info[subj]\n    averaged, per_rep = extract_surface_betas_for_subject(subj, trial_info)\n    \n    for region in REGIONS:\n        all_averaged[region].append(averaged[region])\n        all_per_rep[region].append(per_rep[region])\n    \n    elapsed = time.time() - t0\n    print(f'  Done. Elapsed: {elapsed/60:.1f} min')\n\ntotal_time = time.time() - t0\nprint(f'\\nTotal extraction time: {total_time/60:.1f} min')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Surface Noise Ceiling per Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ncsnr_to_nc(ncsnr: np.ndarray, k: int = 3) -> np.ndarray:\n    \"\"\"Convert ncsnr to noise ceiling percentage.\"\"\"\n    return 100.0 * ncsnr**2 / (ncsnr**2 + 1.0 / k)\n\n\n# Build per-vertex NC for all subjects, all regions\n# Since ROI masks are shared across subjects (fsaverage), the vertex indices\n# are the same for all subjects. But ncsnr differs per subject.\n\nnc_per_subject_region = {}  # (subj, region) -> nc array\n\nfor subj in SUBJECT_LIST:\n    lh_ncsnr = nib.load(str(NSD_ROOT / f'subj{subj:02d}' / 'betas' / 'lh.ncsnr.mgh')).get_fdata().flatten()\n    rh_ncsnr = nib.load(str(NSD_ROOT / f'subj{subj:02d}' / 'betas' / 'rh.ncsnr.mgh')).get_fdata().flatten()\n    \n    for region in REGIONS:\n        lh_mask = roi_masks[region]['lh']\n        rh_mask = roi_masks[region]['rh']\n        ncsnr_roi = np.concatenate([lh_ncsnr[lh_mask], rh_ncsnr[rh_mask]])\n        nc_roi = ncsnr_to_nc(ncsnr_roi, k=3)\n        nc_per_subject_region[(subj, region)] = nc_roi\n        \n        if subj == SUBJECT_LIST[0]:\n            print(f'subj{subj:02d} {region}: {len(nc_roi)} vertices, '\n                  f'median NC={np.nanmedian(nc_roi):.1f}%')\n\nprint('\\nNC computed for all subjects and regions.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Per-Region xarray DataArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_region_assembly(region: str) -> xr.DataArray:\n    \"\"\"Build a DataArray for one region with all 1000 shared images, all subjects.\n    \n    Dimensions: (presentation=1000, neuroid=n_verts_per_subj * N_SUBJECTS)\n    \"\"\"\n    lh_mask = roi_masks[region]['lh']\n    rh_mask = roi_masks[region]['rh']\n    n_lh = lh_mask.sum()\n    n_rh = rh_mask.sum()\n    n_verts = n_lh + n_rh\n    lh_indices = np.where(lh_mask)[0]\n    rh_indices = np.where(rh_mask)[0]\n    \n    # Stack subjects along neuroid dim: (1000, n_verts * N_SUBJECTS)\n    data_list = all_averaged[region]  # list of (1000, n_verts) per subject\n    data = np.concatenate(data_list, axis=1)  # (1000, n_verts * N_SUBJECTS)\n    \n    # Build neuroid coordinates\n    neuroid_ids = []\n    subjects = []\n    hemispheres = []\n    vertex_indices = []\n    regions_coord = []\n    nc_values = []\n    \n    for subj in SUBJECT_LIST:\n        nc = nc_per_subject_region[(subj, region)]\n        v_idx = 0\n        for hemi, indices in [('lh', lh_indices), ('rh', rh_indices)]:\n            for fsavg_idx in indices:\n                neuroid_ids.append(f'subj{subj:02d}_{hemi}_{region}_v{v_idx:05d}')\n                subjects.append(f'subj{subj:02d}')\n                hemispheres.append(hemi)\n                vertex_indices.append(int(fsavg_idx))\n                regions_coord.append(region)\n                nc_values.append(float(nc[v_idx]))\n                v_idx += 1\n    \n    # Build presentation coordinates\n    stimulus_ids = [f'nsd_{int(nid):05d}' for nid in shared_nsdids]\n    \n    da = xr.DataArray(\n        data.astype(np.float32),\n        dims=['presentation', 'neuroid'],\n        coords={\n            # Presentation coords\n            'stimulus_id': ('presentation', stimulus_ids),\n            'nsd_id': ('presentation', shared_nsdids.astype(int)),\n            'min_reps_across_subjects': ('presentation', min_reps),\n            # Neuroid coords\n            'neuroid_id': ('neuroid', neuroid_ids),\n            'subject': ('neuroid', subjects),\n            'hemisphere': ('neuroid', hemispheres),\n            'vertex_index': ('neuroid', vertex_indices),\n            'region': ('neuroid', regions_coord),\n            'nc_testset': ('neuroid', nc_values),\n        },\n    )\n    \n    return da\n\n\n# Build and save per-region assemblies\nfor region in REGIONS:\n    print(f'\\nBuilding {region} assembly...')\n    da = build_region_assembly(region)\n    print(f'  Shape: {da.shape}')\n    print(f'  Size: {da.nbytes / 1e6:.1f} MB')\n    \n    # Validate\n    assert da.shape[0] == 1000\n    assert len(np.unique(da.coords['subject'].values)) == N_SUBJECTS\n    n_nan = np.isnan(da.values).sum()\n    n_complete = (da.coords['min_reps_across_subjects'].values >= 3).sum()\n    print(f'  NaN values: {n_nan} ({100*n_nan/da.values.size:.1f}%)')\n    print(f'  Complete images (min_reps>=3): {n_complete}')\n    \n    # Save\n    out_path = OUTPUT_DIR / f'Allen2022_surface.{region}.nc'\n    da.to_netcdf(str(out_path))\n    print(f'  Saved: {out_path} ({out_path.stat().st_size / 1e6:.1f} MB)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1:\n",
      "  Full: (1000, 34208), Complete: (515, 34208)\n",
      "  Subj01 median NC: 47.8%\n",
      "  NaN in complete subset: 986\n",
      "\n",
      "V2:\n",
      "  Full: (1000, 27128), Complete: (515, 27128)\n",
      "  Subj01 median NC: 49.1%\n",
      "  NaN in complete subset: 343\n",
      "\n",
      "V4:\n",
      "  Full: (1000, 7312), Complete: (515, 7312)\n",
      "  Subj01 median NC: 47.0%\n",
      "  NaN in complete subset: 0\n",
      "\n",
      "IT:\n",
      "  Full: (1000, 89344), Complete: (515, 89344)\n",
      "  Subj01 median NC: 22.7%\n",
      "  NaN in complete subset: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reload and verify\n",
    "for region in REGIONS:\n",
    "    path = OUTPUT_DIR / f'Allen2022_surface.{region}.nc'\n",
    "    da = xr.open_dataarray(str(path))\n",
    "    da.load()\n",
    "    \n",
    "    # Check NC values\n",
    "    nc_vals = da.coords['nc_testset'].values\n",
    "    subj1_mask = da.coords['subject'].values == 'subj01'\n",
    "    nc_subj1 = nc_vals[subj1_mask]\n",
    "    \n",
    "    # Filter to complete images\n",
    "    complete_mask = da.coords['min_reps_across_subjects'].values >= 3\n",
    "    da_complete = da.isel(presentation=complete_mask)\n",
    "    \n",
    "    print(f'{region}:')\n",
    "    print(f'  Full: {da.shape}, Complete: {da_complete.shape}')\n",
    "    print(f'  Subj01 median NC: {np.nanmedian(nc_subj1):.1f}%')\n",
    "    print(f'  NaN in complete subset: {np.isnan(da_complete.values).sum()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface data preparation complete.\n",
      "Assemblies saved to: /Volumes/Hagibis/nsd/assemblies\n",
      "\n",
      "Files:\n",
      "  Allen2022_surface.V1.nc: 145.7 MB\n",
      "  Allen2022_surface.V2.nc: 115.6 MB\n",
      "  Allen2022_surface.V4.nc: 31.2 MB\n",
      "  Allen2022_surface.IT.nc: 380.3 MB\n"
     ]
    }
   ],
   "source": [
    "print('Surface data preparation complete.')\n",
    "print(f'Assemblies saved to: {OUTPUT_DIR}')\n",
    "print(f'\\nFiles:')\n",
    "for region in REGIONS:\n",
    "    path = OUTPUT_DIR / f'Allen2022_surface.{region}.nc'\n",
    "    print(f'  {path.name}: {path.stat().st_size / 1e6:.1f} MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsd-2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}