{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - NSD Packaging Preparation\n",
    "\n",
    "Build Brain-Score-ready assemblies with per-rep test data for InternalConsistency ceiling.\n",
    "\n",
    "**Inputs:**\n",
    "- Notebook 02 per-region averaged assemblies (`Allen2022.{V1,V2,V4,IT}.nc`)\n",
    "- Raw NSD HDF5 betas (for per-rep test extraction)\n",
    "- `nsd_stimuli.hdf5` (for stimulus images)\n",
    "\n",
    "**Outputs** (to `/Volumes/Hagibis/nsd/brainscore/`):\n",
    "- `Allen2022_fmri_train.nc` -- train assembly: (412, ~57K, 1)\n",
    "- `Allen2022_fmri_test.nc` -- test assembly: (309, ~57K, 1) with repetition coord\n",
    "- `stimuli_train/` and `stimuli_test/` -- extracted stimulus images\n",
    "- `stimulus_metadata_{train,test}.csv`\n",
    "\n",
    "**Key decision:** Test assembly keeps 3 reps separate (103 images x 3 = 309 presentations)\n",
    "so that `InternalConsistency` can compute split-half ceiling, matching the Hebart2023 pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport xarray as xr\nimport h5py\nimport nibabel as nib\nimport scipy.io\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport time\nimport gc\n\nNSD_ROOT = Path('/Volumes/Hagibis/nsd')\nASSEMBLIES_DIR = NSD_ROOT / 'assemblies'  # notebook 02 output\nOUTPUT_DIR = NSD_ROOT / 'brainscore'\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# --- Subject Configuration (must match NB02) ---\nSUBJECT_LIST = [1, 2, 3, 4, 5, 6, 7, 8]\n# SUBJECT_LIST = [1, 2, 5, 7]\n\nN_SUBJECTS = len(SUBJECT_LIST)\nALL_SESSIONS = {1: 40, 2: 40, 3: 32, 4: 30, 5: 40, 6: 32, 7: 40, 8: 30}\nSESSIONS_PER_SUBJECT = {s: ALL_SESSIONS[s] for s in SUBJECT_LIST}\n\nTRIALS_PER_SESSION = 750\nN_REPS = 3\nREGIONS = ['V1', 'V2', 'V4', 'IT']\n\n# ROI definitions (same as notebook 02)\nREGION_TO_PRF_LABELS = {'V1': [1, 2], 'V2': [3, 4], 'V4': [7]}\nSTREAMS_VENTRAL_LABEL = 5\n\n# Load train/test split\nsplit_df = pd.read_csv(ASSEMBLIES_DIR / 'train_test_split.csv')\ntrain_ids = set(split_df.loc[split_df['split'] == 'train', 'stimulus_id'].values)\ntest_ids = set(split_df.loc[split_df['split'] == 'test', 'stimulus_id'].values)\ntest_nsd_ids = sorted(split_df.loc[split_df['split'] == 'test', 'nsd_id'].values)\nall_nsd_ids = sorted(split_df['nsd_id'].values)\n\nprint(f'Train: {len(train_ids)}, Test: {len(test_ids)}')\nprint(f'Output: {OUTPUT_DIR}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Notebook 02 Averaged Assemblies\n",
    "\n",
    "These contain per-session z-scored, rep-averaged betas for all 515 images.\n",
    "We use them for: (a) train data, (b) global z-score statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1: (515, 9039)\n",
      "V2: (515, 8792)\n",
      "V4: (515, 3982)\n",
      "IT: (515, 35429)\n",
      "\n",
      "Total images: 515\n"
     ]
    }
   ],
   "source": [
    "# Load per-region assemblies from notebook 02\n",
    "nb02_assemblies = {}\n",
    "for region in REGIONS:\n",
    "    path = ASSEMBLIES_DIR / f'Allen2022.{region}.nc'\n",
    "    da = xr.open_dataarray(str(path))\n",
    "    da.load()\n",
    "    nb02_assemblies[region] = da\n",
    "    print(f'{region}: {da.shape}')\n",
    "\n",
    "N_USABLE_IMAGES = nb02_assemblies['V1'].sizes['presentation']\n",
    "print(f'\\nTotal images: {N_USABLE_IMAGES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Per-Rep Test Betas\n",
    "\n",
    "Re-extract betas for the 103 test images, keeping 3 reps separate.\n",
    "Same per-session z-scoring as notebook 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Data loading utilities (replicated from notebook 02) ---\n\ndef load_roi(subj: int, roi_name: str) -> np.ndarray:\n    path = NSD_ROOT / f'subj{subj:02d}' / 'rois' / f'{roi_name}.nii.gz'\n    return nib.load(str(path)).get_fdata().T  # (81,104,83) -> (83,104,81)\n\n\ndef load_session_betas(subj: int, session: int) -> np.ndarray:\n    path = NSD_ROOT / f'subj{subj:02d}' / 'betas' / f'betas_session{session:02d}.hdf5'\n    with h5py.File(str(path), 'r') as f:\n        betas = f['betas'][:]  # (750, 83, 104, 81) int16\n    return betas.astype(np.float32) / 300.0\n\n\ndef get_roi_masks(subj: int) -> dict:\n    prf = load_roi(subj, 'prf-visualrois')\n    lh_streams = load_roi(subj, 'lh.streams')\n    rh_streams = load_roi(subj, 'rh.streams')\n    masks = {}\n    for region, labels in REGION_TO_PRF_LABELS.items():\n        masks[region] = np.isin(prf, labels)\n    masks['IT'] = (lh_streams == STREAMS_VENTRAL_LABEL) | (rh_streams == STREAMS_VENTRAL_LABEL)\n    return masks\n\n\n# --- Trial mapping (replicated from notebook 02) ---\n\nexpdesign = scipy.io.loadmat(NSD_ROOT / 'metadata' / 'nsd_expdesign.mat')\nmasterordering = expdesign['masterordering'].flatten()\nsubjectim = expdesign['subjectim']\nsharedix = expdesign['sharedix'].flatten()\n\n\ndef get_shared_trial_info(subj_idx: int, target_nsd_ids: set) -> pd.DataFrame:\n    \"\"\"Find trial indices for shared images for a given subject.\n    Only returns rows for images in target_nsd_ids.\"\"\"\n    n_sessions = SESSIONS_PER_SUBJECT[subj_idx + 1]\n    n_total_trials = n_sessions * TRIALS_PER_SESSION\n    subj_nsdids = subjectim[subj_idx]\n    nsdid_to_imgidx = {int(nsd_id): img_idx + 1 for img_idx, nsd_id in enumerate(subj_nsdids)}\n\n    shared_imgidxs = set()\n    for nsd_id in sharedix:\n        if int(nsd_id) in nsdid_to_imgidx:\n            shared_imgidxs.add(nsdid_to_imgidx[int(nsd_id)])\n\n    records = []\n    rep_counter = {}\n    for trial_idx in range(n_total_trials):\n        img_idx = masterordering[trial_idx]\n        if img_idx in shared_imgidxs:\n            nsd_id = int(subj_nsdids[img_idx - 1] - 1)  # 0-indexed\n            if nsd_id not in target_nsd_ids:\n                rep_counter[img_idx] = rep_counter.get(img_idx, 0) + 1\n                continue\n            rep = rep_counter.get(img_idx, 0)\n            rep_counter[img_idx] = rep + 1\n            session = trial_idx // TRIALS_PER_SESSION + 1\n            trial_in_session = trial_idx % TRIALS_PER_SESSION\n            records.append({\n                'nsd_id': nsd_id,\n                'rep': rep,\n                'session': session,\n                'trial_in_session': trial_in_session,\n            })\n    return pd.DataFrame(records)\n\n\nprint('Utilities loaded.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract per-rep test betas for all subjects, all regions\n# Same per-session z-scoring as notebook 02\n\ntest_nsd_id_set = set(test_nsd_ids)\nn_test = len(test_nsd_ids)\ntest_nsd_id_to_idx = {nsd_id: idx for idx, nsd_id in enumerate(sorted(test_nsd_ids))}\n\ntest_per_rep = {}  # {subj: {region: (n_test, 3, n_voxels)}}\nall_masks = {}     # {subj: {region: mask}}\n\ntotal_t0 = time.time()\n\nfor subj in SUBJECT_LIST:\n    print(f'\\nProcessing subj{subj:02d} ({SESSIONS_PER_SUBJECT[subj]} sessions)...')\n    \n    masks = get_roi_masks(subj)\n    all_masks[subj] = masks\n    \n    # Get trial info for test images\n    trial_info = get_shared_trial_info(subj - 1, test_nsd_id_set)\n    assert len(trial_info) == n_test * N_REPS, (\n        f'subj{subj:02d}: expected {n_test * N_REPS} test trials, got {len(trial_info)}')\n    \n    # Find which sessions we need\n    sessions_needed = sorted(trial_info['session'].unique())\n    print(f'  Need {len(sessions_needed)} sessions for {n_test} test images')\n    \n    # Pre-allocate per-rep storage\n    subj_per_rep = {}\n    for region, mask in masks.items():\n        subj_per_rep[region] = np.zeros((n_test, N_REPS, mask.sum()), dtype=np.float32)\n    \n    t0 = time.time()\n    for session in sessions_needed:\n        session_trials = trial_info[trial_info['session'] == session]\n        if len(session_trials) == 0:\n            continue\n        \n        session_betas = load_session_betas(subj, session)  # (750, 83, 104, 81)\n        \n        for region, mask in masks.items():\n            roi_betas = session_betas[:, mask]  # (750, n_voxels)\n            \n            # Session z-score (same as NB02 -- see comment there)\n            mean = roi_betas.mean(axis=0, keepdims=True)\n            std = roi_betas.std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            roi_betas = (roi_betas - mean) / std\n            \n            for _, row in session_trials.iterrows():\n                img_idx = test_nsd_id_to_idx[row['nsd_id']]\n                subj_per_rep[region][img_idx, row['rep']] = roi_betas[row['trial_in_session']]\n        \n        del session_betas\n    \n    test_per_rep[subj] = subj_per_rep\n    elapsed = time.time() - t0\n    print(f'  Done in {elapsed:.0f}s')\n    \n    for region in REGIONS:\n        print(f'    {region}: {subj_per_rep[region].shape}')\n    \n    gc.collect()\n\ntotal_elapsed = time.time() - total_t0\nprint(f'\\nTotal extraction time: {total_elapsed/60:.1f} minutes')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Validate: mean of 3 per-rep test betas should match notebook 02 averaged values\nprint('Validation: per-rep mean vs notebook 02 averaged values')\nprint('=' * 60)\n\n# Get test image indices in the notebook 02 assembly\nnb02_stimulus_ids = nb02_assemblies['V1'].coords['stimulus_id'].values\ntest_stimulus_ids_sorted = [f'nsd_{nsd_id:05d}' for nsd_id in sorted(test_nsd_ids)]\ntest_mask_nb02 = np.array([sid in test_ids for sid in nb02_stimulus_ids])\n\nall_pass = True\nfor region in REGIONS:\n    nb02_test = nb02_assemblies[region].values[test_mask_nb02]  # (n_test, n_voxels)\n    \n    # Stack per-rep across subjects to get the same neuroid ordering\n    per_rep_means = []\n    for subj in SUBJECT_LIST:\n        mean_per_rep = test_per_rep[subj][region].mean(axis=1)  # (n_test, n_voxels)\n        per_rep_means.append(mean_per_rep)\n    per_rep_mean_all = np.concatenate(per_rep_means, axis=1)  # (n_test, total_voxels)\n    \n    max_diff = np.max(np.abs(nb02_test - per_rep_mean_all))\n    mean_diff = np.mean(np.abs(nb02_test - per_rep_mean_all))\n    status = 'PASS' if max_diff < 1e-4 else 'FAIL'\n    if status == 'FAIL':\n        all_pass = False\n    print(f'{region}: max_diff={max_diff:.2e}, mean_diff={mean_diff:.2e} [{status}]')\n\nprint(f'\\nOverall: {\"ALL PASS\" if all_pass else \"SOME FAILED\"}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Combined Assemblies\n",
    "\n",
    "Stack V1, V2, V4, IT along neuroid dimension. Apply global z-score per subject per region\n",
    "using statistics from all 515 averaged images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stage 2 of 2: Global z-score (per subject, per region).\n# After session z-scoring (NB02) and rep-averaging, subjects have different signal\n# distributions due to individual differences in BOLD responsiveness.\n# This normalization puts subjects on a comparable scale for pooled\n# cross-subject ridge regression. Not prescribed by Allen et al. 2022\n# (their analyses are per-subject), but standard for cross-subject encoding.\n\nzscore_stats = {}  # {subj: {region: (mean, std)}} -- per voxel\n\nfor subj in SUBJECT_LIST:\n    subj_label = f'subj{subj:02d}'\n    zscore_stats[subj] = {}\n    for region in REGIONS:\n        assembly = nb02_assemblies[region]\n        # Select this subject's voxels\n        subj_mask = assembly.coords['subject'].values == subj_label\n        data = assembly.values[:, subj_mask]  # (n_complete, n_voxels_subj)\n        \n        mean = data.mean(axis=0)  # (n_voxels_subj,)\n        std = data.std(axis=0)\n        std[std == 0] = 1.0\n        zscore_stats[subj][region] = (mean, std)\n\nprint('Global z-score statistics computed from all averaged images.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build train assembly: (n_train, total_neuroids, 1)\n# Data from notebook 02, split to train only, z-scored globally\n\ntrain_mask_nb02 = np.array([sid in train_ids for sid in nb02_stimulus_ids])\ntrain_stimulus_ids_sorted = list(nb02_stimulus_ids[train_mask_nb02])\ntrain_nsd_ids_sorted = list(nb02_assemblies['V1'].coords['nsd_id'].values[train_mask_nb02])\n\ntrain_data_blocks = []   # per-subject per-region data blocks\nneuroid_ids = []\nsubjects = []\nregions_coord = []\nnc_values = []\nvoxel_xs = []\nvoxel_ys = []\nvoxel_zs = []\n\nfor subj in SUBJECT_LIST:\n    subj_label = f'subj{subj:02d}'\n    total_voxels_subj = 0\n    for region in REGIONS:\n        assembly = nb02_assemblies[region]\n        subj_mask = assembly.coords['subject'].values == subj_label\n        \n        # Get train data for this subject's voxels\n        data = assembly.values[train_mask_nb02][:, subj_mask]  # (n_train, n_voxels_subj)\n        \n        # Apply global z-score\n        mean, std = zscore_stats[subj][region]\n        data = (data - mean) / std\n        \n        train_data_blocks.append(data)\n        \n        n_voxels = data.shape[1]\n        total_voxels_subj += n_voxels\n        \n        # Collect neuroid metadata\n        nc_subj = assembly.coords['nc_testset'].values[subj_mask]\n        vx_subj = assembly.coords['voxel_x'].values[subj_mask]\n        vy_subj = assembly.coords['voxel_y'].values[subj_mask]\n        vz_subj = assembly.coords['voxel_z'].values[subj_mask]\n        nids_subj = assembly.coords['neuroid_id'].values[subj_mask]\n        \n        neuroid_ids.extend(nids_subj.tolist())\n        subjects.extend([subj_label] * n_voxels)\n        regions_coord.extend([region] * n_voxels)\n        nc_values.extend(nc_subj.tolist())\n        voxel_xs.extend(vx_subj.tolist())\n        voxel_ys.extend(vy_subj.tolist())\n        voxel_zs.extend(vz_subj.tolist())\n\ntrain_data = np.concatenate(train_data_blocks, axis=1)  # (n_train, total_neuroids)\nn_train = train_data.shape[0]\nn_neuroids = train_data.shape[1]\nprint(f'Train data: {train_data.shape}')\nprint(f'Total neuroids: {n_neuroids}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build test assembly: (n_test_presentations, total_neuroids, 1)\n# n_test images x 3 reps, with repetition coord\n\ntest_data_blocks = []  # per-subject per-region blocks, reps interleaved\n\nfor subj in SUBJECT_LIST:\n    for region in REGIONS:\n        per_rep = test_per_rep[subj][region]  # (n_test, 3, n_voxels_subj)\n        \n        # Apply global z-score (same stats as train)\n        mean, std = zscore_stats[subj][region]\n        per_rep = (per_rep - mean) / std\n        \n        # Interleave reps: for each image, list rep0, rep1, rep2\n        # Result: (n_test_presentations, n_voxels_subj)\n        n_img, n_rep, n_vox = per_rep.shape\n        interleaved = per_rep.reshape(n_img * n_rep, n_vox)\n        test_data_blocks.append(interleaved)\n\ntest_data = np.concatenate(test_data_blocks, axis=1)  # (n_test_presentations, total_neuroids)\nassert test_data.shape[1] == n_neuroids, f'Neuroid count mismatch: {test_data.shape[1]} vs {n_neuroids}'\n\n# Build repetition and stimulus coords for test\nn_test_images = len(test_nsd_ids)\ntest_rep_coord = np.tile(np.arange(N_REPS), n_test_images)  # [0,1,2, 0,1,2, ...]\ntest_stimulus_id_coord = np.repeat(\n    [f'nsd_{nsd_id:05d}' for nsd_id in sorted(test_nsd_ids)], N_REPS\n)  # each stimulus_id repeated 3x\ntest_nsd_id_coord = np.repeat(sorted(test_nsd_ids), N_REPS)\n\nprint(f'Test data: {test_data.shape}')\nprint(f'Repetitions per image: {N_REPS}')\nprint(f'Test presentations: {n_test_images} images x {N_REPS} reps = {test_data.shape[0]}')"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train assembly: (412, 57242, 1), dims=('presentation', 'neuroid', 'time_bin')\n",
      "Test assembly:  (309, 57242, 1), dims=('presentation', 'neuroid', 'time_bin')\n",
      "\n",
      "Train coords: ['stimulus_id', 'nsd_id', 'neuroid_id', 'subject', 'region', 'nc_testset', 'voxel_x', 'voxel_y', 'voxel_z', 'time_bin_start', 'time_bin_end']\n",
      "Test coords:  ['stimulus_id', 'nsd_id', 'repetition', 'neuroid_id', 'subject', 'region', 'nc_testset', 'voxel_x', 'voxel_y', 'voxel_z', 'time_bin_start', 'time_bin_end']\n"
     ]
    }
   ],
   "source": [
    "# Save as plain xr.DataArray (NeuroidAssembly wrapping happens at packaging stage)\n",
    "# This preserves all coords cleanly without MultiIndex issues.\n",
    "\n",
    "# Shared neuroid coords (same for train and test)\n",
    "neuroid_coords = {\n",
    "    'neuroid_id': ('neuroid', neuroid_ids),\n",
    "    'subject': ('neuroid', subjects),\n",
    "    'region': ('neuroid', regions_coord),\n",
    "    'nc_testset': ('neuroid', nc_values),\n",
    "    'voxel_x': ('neuroid', voxel_xs),\n",
    "    'voxel_y': ('neuroid', voxel_ys),\n",
    "    'voxel_z': ('neuroid', voxel_zs),\n",
    "}\n",
    "\n",
    "# Train assembly\n",
    "train_assembly = xr.DataArray(\n",
    "    train_data.reshape(n_train, n_neuroids, 1),\n",
    "    dims=['presentation', 'neuroid', 'time_bin'],\n",
    "    coords={\n",
    "        'stimulus_id': ('presentation', train_stimulus_ids_sorted),\n",
    "        'nsd_id': ('presentation', train_nsd_ids_sorted),\n",
    "        **neuroid_coords,\n",
    "        'time_bin_start': ('time_bin', [70]),\n",
    "        'time_bin_end': ('time_bin', [170]),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Test assembly (with repetition coord)\n",
    "test_assembly = xr.DataArray(\n",
    "    test_data.reshape(test_data.shape[0], n_neuroids, 1),\n",
    "    dims=['presentation', 'neuroid', 'time_bin'],\n",
    "    coords={\n",
    "        'stimulus_id': ('presentation', list(test_stimulus_id_coord)),\n",
    "        'nsd_id': ('presentation', list(test_nsd_id_coord)),\n",
    "        'repetition': ('presentation', list(test_rep_coord)),\n",
    "        **neuroid_coords,\n",
    "        'time_bin_start': ('time_bin', [70]),\n",
    "        'time_bin_end': ('time_bin', [170]),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f'Train assembly: {train_assembly.shape}, dims={train_assembly.dims}')\n",
    "print(f'Test assembly:  {test_assembly.shape}, dims={test_assembly.dims}')\n",
    "print(f'\\nTrain coords: {list(train_assembly.coords.keys())}')\n",
    "print(f'Test coords:  {list(test_assembly.coords.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Stimulus Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extracting train images:   0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 412 images saved to /Volumes/Hagibis/nsd/brainscore/stimuli_train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Extracting test images:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 103 images saved to /Volumes/Hagibis/nsd/brainscore/stimuli_test\n"
     ]
    }
   ],
   "source": [
    "# Extract 515 stimulus images from HDF5 and save as JPEGs\n",
    "STIMULI_HDF5 = NSD_ROOT / 'stimuli' / 'nsd_stimuli.hdf5'\n",
    "\n",
    "for split_name, nsd_id_list in [('train', sorted(split_df.loc[split_df['split']=='train', 'nsd_id'])),\n",
    "                                 ('test', sorted(split_df.loc[split_df['split']=='test', 'nsd_id']))]:\n",
    "    out_dir = OUTPUT_DIR / f'stimuli_{split_name}'\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    with h5py.File(STIMULI_HDF5, 'r') as f:\n",
    "        img_brick = f['imgBrick']\n",
    "        for nsd_id in tqdm(nsd_id_list, desc=f'Extracting {split_name} images'):\n",
    "            img_array = img_brick[nsd_id]\n",
    "            img = Image.fromarray(img_array, 'RGB')\n",
    "            img.save(out_dir / f'nsd_{nsd_id:05d}.jpg', quality=95)\n",
    "    \n",
    "    print(f'{split_name}: {len(nsd_id_list)} images saved to {out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: metadata saved to /Volumes/Hagibis/nsd/brainscore/stimulus_metadata_train.csv\n",
      "test: metadata saved to /Volumes/Hagibis/nsd/brainscore/stimulus_metadata_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Create stimulus metadata CSVs\n",
    "for split_name in ['train', 'test']:\n",
    "    split_rows = split_df[split_df['split'] == split_name].copy()\n",
    "    split_rows['image_file_name'] = split_rows['nsd_id'].apply(lambda x: f'nsd_{x:05d}.jpg')\n",
    "    meta_path = OUTPUT_DIR / f'stimulus_metadata_{split_name}.csv'\n",
    "    split_rows[['stimulus_id', 'nsd_id', 'image_file_name']].to_csv(meta_path, index=False)\n",
    "    print(f'{split_name}: metadata saved to {meta_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Validate assemblies\nprint('Assembly Validation')\nprint('=' * 60)\n\nn_train_expected = len(train_ids)\nn_test_expected = len(test_ids) * N_REPS\n\n# Train\nassert train_assembly.dims == ('presentation', 'neuroid', 'time_bin')\nassert train_assembly.sizes['presentation'] == n_train_expected\nassert train_assembly.sizes['time_bin'] == 1\nassert not np.any(np.isnan(train_assembly.values))\nprint(f'Train: {train_assembly.shape} -- OK')\n\n# Test\nassert test_assembly.dims == ('presentation', 'neuroid', 'time_bin')\nassert test_assembly.sizes['presentation'] == n_test_expected\nassert test_assembly.sizes['time_bin'] == 1\nassert 'repetition' in test_assembly.coords\nassert list(sorted(set(test_assembly.coords['repetition'].values))) == [0, 1, 2]\nassert not np.any(np.isnan(test_assembly.values))\nprint(f'Test:  {test_assembly.shape} -- OK')\n\n# Neuroid consistency\nassert train_assembly.sizes['neuroid'] == test_assembly.sizes['neuroid']\nassert np.array_equal(\n    train_assembly.coords['neuroid_id'].values,\n    test_assembly.coords['neuroid_id'].values\n)\nprint(f'Neuroids consistent: {n_neuroids}')\n\n# Region counts\nfor region in REGIONS:\n    n = (np.array(regions_coord) == region).sum()\n    print(f'  {region}: {n} neuroids')\n\n# Stimulus ID consistency\nassert len(set(train_assembly.coords['stimulus_id'].values) & \n           set(test_assembly.coords['stimulus_id'].values)) == 0, \\\n    'Train/test stimulus_id overlap!'\nprint('\\nNo train/test overlap in stimulus_ids.')\n\n# Check subjects\nsubjects_set = set(train_assembly.coords['subject'].values)\nexpected_subjs = {f'subj{i:02d}' for i in SUBJECT_LIST}\nassert subjects_set == expected_subjs\nprint(f'All {N_SUBJECTS} subjects present.')"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: /Volumes/Hagibis/nsd/brainscore/Allen2022_fmri_train.nc (106.3 MB)\n",
      "Test:  /Volumes/Hagibis/nsd/brainscore/Allen2022_fmri_test.nc (82.7 MB)\n"
     ]
    }
   ],
   "source": [
    "# Save assemblies\n",
    "train_path = OUTPUT_DIR / 'Allen2022_fmri_train.nc'\n",
    "test_path = OUTPUT_DIR / 'Allen2022_fmri_test.nc'\n",
    "\n",
    "train_assembly.to_netcdf(str(train_path))\n",
    "test_assembly.to_netcdf(str(test_path))\n",
    "\n",
    "print(f'Train: {train_path} ({train_path.stat().st_size / 1e6:.1f} MB)')\n",
    "print(f'Test:  {test_path} ({test_path.stat().st_size / 1e6:.1f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload verification passed.\n"
     ]
    }
   ],
   "source": [
    "# Reload and verify\n",
    "train_reload = xr.open_dataarray(str(train_path))\n",
    "test_reload = xr.open_dataarray(str(test_path))\n",
    "\n",
    "assert train_reload.shape == train_assembly.shape\n",
    "assert test_reload.shape == test_assembly.shape\n",
    "assert np.allclose(train_reload.values, train_assembly.values, atol=1e-6)\n",
    "assert np.allclose(test_reload.values, test_assembly.values, atol=1e-6)\n",
    "\n",
    "train_reload.close()\n",
    "test_reload.close()\n",
    "\n",
    "print('Reload verification passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NSD Packaging Preparation Summary\n",
      "============================================================\n",
      "\n",
      "ASSEMBLIES:\n",
      "  Train: (412, 57242, 1) (106.3 MB)\n",
      "  Test:  (309, 57242, 1) (82.7 MB)\n",
      "  Total neuroids: 57242 across 8 subjects x 4 regions\n",
      "\n",
      "STIMULI:\n",
      "  Train images: 412\n",
      "  Test images:  103\n",
      "\n",
      "FILES:\n",
      "  Allen2022_fmri_test.nc: 82.7 MB\n",
      "  Allen2022_fmri_train.nc: 106.3 MB\n",
      "  stimuli_test/: 103 files\n",
      "  stimuli_train/: 412 files\n",
      "  stimulus_metadata_test.csv: 0.0 MB\n",
      "  stimulus_metadata_train.csv: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print('=' * 60)\n",
    "print('NSD Packaging Preparation Summary')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('ASSEMBLIES:')\n",
    "print(f'  Train: {train_assembly.shape} ({train_path.stat().st_size/1e6:.1f} MB)')\n",
    "print(f'  Test:  {test_assembly.shape} ({test_path.stat().st_size/1e6:.1f} MB)')\n",
    "print(f'  Total neuroids: {n_neuroids} across 8 subjects x 4 regions')\n",
    "print()\n",
    "print('STIMULI:')\n",
    "n_train_imgs = len(list((OUTPUT_DIR / 'stimuli_train').glob('*.jpg')))\n",
    "n_test_imgs = len(list((OUTPUT_DIR / 'stimuli_test').glob('*.jpg')))\n",
    "print(f'  Train images: {n_train_imgs}')\n",
    "print(f'  Test images:  {n_test_imgs}')\n",
    "print()\n",
    "print('FILES:')\n",
    "for f in sorted(OUTPUT_DIR.iterdir()):\n",
    "    if f.is_file():\n",
    "        print(f'  {f.name}: {f.stat().st_size/1e6:.1f} MB')\n",
    "    elif f.is_dir():\n",
    "        n_files = len(list(f.iterdir()))\n",
    "        print(f'  {f.name}/: {n_files} files')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsd-2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}