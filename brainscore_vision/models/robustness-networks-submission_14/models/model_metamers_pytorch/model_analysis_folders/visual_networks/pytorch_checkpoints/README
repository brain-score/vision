Directory contains heckpoints used for visual models that are not automatically downloaded with build_network.py scripts. 

1) ResNet50 Trained with L2-Norm EPS=3 training from https://github.com/MadryLab/robustness
command: 
wget -O resnet50_imagenet_l2_3_0_robustness.pt https://www.dropbox.com/s/knf4uimlqsi1yz8/imagenet_l2_3_0.pt?dl=1

2) ResNet50 Trained with Linf-Norm EPS=4 training from https://github.com/MadryLab/robustness
wget -O resnet50_imagenet_linf_4_0_robustness.pt https://www.dropbox.com/s/axfuary2w1cnyrg/imagenet_linf_4.pt?dl=1

3) ResNet50 Trained with Linf-Norm EPS=8 training from https://github.com/MadryLab/robustness
wget -O resnet50_imagenet_linf_8_0_robustness.pt https://www.dropbox.com/s/yxn15a9zklz3s8q/imagenet_linf_8.pt?dl=1

4) AlexNet Reduced Aliasing Architecture. Trained by jfeather on openmind cluster using 4 Titan-X GPUs and a total batch size of 256. Learning rate started at 0.01 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 Epochs. 
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/alexnet_reduced_aliasing_drop_after_50_epoch/b1b37fba-a2e5-4e2c-8350-b4211dfb7997/119_checkpoint.pt alexnet_reduced_aliasing.pt

5) ResNet50 with random Linf perturbations at eps=8/255 (perturbations are always to the corners of the linf ball, each value at +/- EPS). Trained by jfeather on openmind cluster using 2 Quadro RTX6000 GPUS and a total batch size of 256. Learning rate started at 0.1 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 Epochs.
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/resnet50_random_linf8_perturb/afe5955f-f318-4b41-a749-19d562d85c59/119_checkpoint.pt resnet50_random_linf8_perturb.pt

6) ResNet50 with random L2 perturbations at eps=3 (perturbations are always to the shell of the l2 ball, such that the l2 norm of the perturbation equal to eps). Trained by jfeather on openmind cluster using 4 V100 GPUS and a total batch size of 256. Learning rate started at 0.1 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 Epochs.
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/resnet50_random_l2_perturb/00396026-921d-4510-b8b7-ad5de4827812/119_checkpoint.pt resnet50_random_l2_perturb.pt

7) AlexNet with L2-Norm EPS=3 adversarial perturbations. Trained by jfeather on openmind cluster using 2 GEFORCEGTX1080TI GPUS with a total batch size of 256. Learning rate started at 0.01 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 Epochs. 
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/alexnet_l2_3_robust/d061f17b-af03-4a31-a5c2-8acd9145cd6d/119_checkpoint.pt alexnet_l2_3_robust_training.pt

8) AlexNet with Linf-Norm EPS=8/255 adversarial perturbations. Trained by jfeather on openmind cluster using 8 V100 GPUs (DGX machine) and a total batch size of 256. Learning rate started at 0.01 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 epochs.
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/alexnet_linf_8_robust/d0a79bb8-2a4e-4a63-8121-d6a24b1c1fe1/119_checkpoint.pt alexnet_linf_8_robust_training.pt

9) AlexNet with L2-Norm EPS=3 random perturbations (perturbations are always to the shell of the l2 ball, such that the l2 norm of the perturbation equal to eps). Trained by jfeather on openmind cluster using 2 GEFORCEGTX1080TI GPUS with a total batch size of 256.  Learning rate started at 0.01 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 Epochs.
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/alexnet_random_l2_3/bac9195c-d43f-4cd7-b7c1-edace5962ff9/119_checkpoint.pt alexnet_l2_3_random_perturb.pt

10) AlexNet with Linf-Norm EPS=8/255 random perturbations. Trained by jfeather on openmind cluster using 2 GEFORCEGTX1080TI GPUS with a total batch size of 256. Learning rate started at 0.01 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 Epochs.
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/alexnet_random_linf_8/b7524ba3-8842-42c9-bda5-5afad50555fb/119_checkpoint.pt alexnet_random_linf_8_perturb.pt

11) AlexNet with Linf-Norm EPS=4/255 adversarial perturbations. Trained by jfeather on openmind cluster using 4 V100 GPUS (DGX machine) with a total batch size of 256. Learning rate started at 0.01 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 Epochs.
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/alexnet_linf_4_robust/955782fc-cd43-40bc-820f-d3c623ed7580/119_checkpoint.pt alexnet_linf_4_robust_training.pt

12) ResNet50 SIMCLR model trained with self-supervised training. Backbone downloaded from openselfsup repository (https://github.com/open-mmlab/OpenSelfSup/blob/master/docs/MODEL_ZOO.md, https://drive.google.com/file/d/1aZ43nSdivdNxHbM9DKVoZYVhZ8TNnmPp/view?usp=sharing). Augmentations consisted of Random Crop, Random Flip, Random Color Jitter (brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2, probability=0.8), Random Greyscale (probability=0.2), and Random Gaussian Blur (sigma_min=0.1, sigma_max=0.2, probability=0.5). A linear readout was trained on the self-supervised backbone by jfeather on the openmind computing cluster using two QuadroRTX6000 GPUs for 100 Epochs of ImageNet, resulting in a model that performs close to the reported accuracy on openselfsup (59.204% vs. 60.06%). Linear readout learning rate started at 30.0 and was dropped by a factor of 10 after every 30 epochs, using a batch size of 256.
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/resnet50_simclr_transfer_training_higher_lr/56f26a96-d30b-4b3e-9fda-b8b4e8255503/99_checkpoint.pt resnet50_simclr.pt

13) ResNet50 MOCO_V2 model trained with self-supervised training. Backbone downloaded from openselfsup repository (https://github.com/open-mmlab/OpenSelfSup/blob/master/docs/MODEL_ZOO.md, https://drive.google.com/file/d/1ImO8A3uWbrTx21D1IqBDMUQvpN6wmv0d/view?usp=sharing). Augmentations consisted of Random Crop, Random Flip, Random Color Jitter (brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, probability=0.8), Random Greyscale (probability=0.2), and Random Gaussian Blur (sigma_min=0.1, sigma_max=0.2, probability=0.5). A linear readout was trained on the self-supervised backbone by jfeather on the openmind computing cluster using two QuadroRTX6000 GPUs for 100 Epochs of ImageNet, resulting in a model that performs close to the reported accuracy on openselfsup (67.832% vs. 67.69%). Linear readout learning rate started at 30.0 and was dropped by a factor of 10 after every 30 epochs, using a batch size of 256. 
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/resnet50_mocov2_transfer_training/55697ca7-199e-4788-a1f2-dc3ab1d73739/99_checkpoint.pt resnet50_moco_v2.pt

14) ResNet50 BYOL model trained with self-supervised training. Backbone downloaded from openselfsup repository (https://github.com/open-mmlab/OpenSelfSup/blob/master/docs/MODEL_ZOO.md, https://drive.google.com/file/d/12Zu9r3fE8qKF4OW6WQXa5Ec6VuA2m3j7/view?usp=sharing). Augmentations for backbone training consisted of Random Crop, Random Flip, Color Jitter (brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, probability=0.8), Random Greyscale (probability=0.2), Random Gaussian Blur (sigma_min=0.1, sigma_max=0.2, probability=1.0 for train pipeline 1 and probability=0.1 for train pipeline 2), and "Solarization" (probability=0 for train pipeline 1 and probability=0.2 for train pipeline 2). 
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/resnet50_byol/89fc165a-17f7-4697-a0bf-b61875e3fbdf/99_checkpoint.pt resnet50_byol.pt

15) Linear classifier for HMAX model trained on imagenet task. Final accuracy is not great (around 6%). Model implementation derived from here: https://github.com/wmvanvliet/pytorch_hmax and uses universal_patch_set.mat which is the same as that from https://maxlab.neuro.georgetown.edu/hmax.html. Training augmentations for the linear classifier included converting the image to greyscale, as HMAX operates on greyscale images. Model was trained for 30 EPOCHs at a batch size of 256 using eight Tesla-V100 GPUs (DGX machine). Learning rate started at 0.1 and was dropped by a factor of 10 after every 10 epochs. This model also had gradient clippingapplied, clipping the gradients to a maxnorm of 1.0, and a "warmup" learning rate for the first X gradient steps. 
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/hmax_standard_transfer_learning_after_changing_padding/b114304a-f36c-4936-ad7f-6705b83dd3e8/29_checkpoint.pt hmax_linear_classification.pt

16) VOneAlexNet with Gaussian noise of STD=4. Network defined using the vonenet repository (https://github.com/dicarlolab/vonenet) with additional modifications following (https://github.com/chung-neuroai-lab/adversarial-manifolds) for gaussian noise rather than poisson noise (which is easier to tune for robustness). Trained by jfeather on openmind cluster using 8 tesla-v100 GPUs (DGX machine) with a total batch size of 256. Learning rate started at 0.01 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 epochs. 
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/vonealexnet_gaussian_noise_std4/54778cad-4550-4e7b-843c-e8ebba9b6728/119_checkpoint.pt gvonealexnet_std4.pt

17) AlexNet standard architecture early checkpoint. Trained by jfeather on openmind cluster using 4 GEFORCEGTX1080TI GPUs with a total batch size of 256. Learning rate started at 0.01 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 epochs, but the early checkpoint was chosen so that it was as close to the ImageNet top 1% accuracy of the VOneAlexNet Guassian noise model with STD=4 but without being lower than that accuracy. 
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/alexnet_retrained/54b255b0-2811-4309-b4b8-1dea1954d032/50_checkpoint.pt alexnet_early_checkpoint_50.pt

18) AlexNet reduced aliasing architecture early checkpoint. Trained by jfeather on openmind cluster using 4 GEFORCEGTX1080TI GPUs with a total batch size of 256. Learning rate started at 0.01 and was dropped by a factor of 10 after every 50 epochs. Model was trained for a total of 120 epochs, but the early checkpoint was chosen so that it was as close to the ImageNet top 1% accuracy of the VOneAlexNet Guassian noise model with STD=4 but without being higher than that accuracy. 
cp /om4/group/mcdermott/user/jfeather/projects/robust_audio_networks/model_training_directory/imagenet_networks/alexnet_reduced_aliasing_drop_after_50_epoch/b1b37fba-a2e5-4e2c-8350-b4211dfb7997/38_checkpoint.pt alexnet_reduced_aliasing_early_checkpoint.pt
