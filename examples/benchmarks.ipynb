{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarks consist of a target assembly and a metric to compare assemblies.\n",
    "They accept a source assembly to compare against and yield a score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brainscore defines benchmarks, which can be run on brain models. To implement a model, the BrainModel interface has to be implemented by the model to be tested. A very simple implementation could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "from brainscore.model_interface import BrainModel\n",
    "from brainio_base.assemblies import DataAssembly\n",
    "\n",
    "class RandomITModel(BrainModel):\n",
    "    def __init__(self):\n",
    "        self._num_neurons = 50\n",
    "        # to note which time we are recording\n",
    "        self._time_bin_start = None\n",
    "        self._time_bin_end = None\n",
    "    \n",
    "    def look_at(self, stimuli):\n",
    "        print(f\"Looking at {len(stimuli)} stimuli\")\n",
    "        rnd = np.random.mtrand.RandomState(0)\n",
    "        recordings = DataAssembly(rnd.rand(len(stimuli), self._num_neurons, 1),\n",
    "                              coords={'image_id': ('presentation', stimuli['image_id']),\n",
    "                                      'object_name': ('presentation', stimuli['object_name']),\n",
    "                                      'neuroid_id': ('neuroid', np.arange(self._num_neurons)),\n",
    "                                      'region': ('neuroid', ['IT'] * self._num_neurons),\n",
    "                                      'time_bin_start': ('time_bin', [self._time_bin_start]),\n",
    "                                      'time_bin_end': ('time_bin', [self._time_bin_end])},\n",
    "                              dims=['presentation', 'neuroid', 'time_bin'])\n",
    "        recordings.name = 'random_it_model'\n",
    "        return recordings\n",
    "\n",
    "    def start_task(self, task, **kwargs):\n",
    "        print(f\"Starting task {task}\")\n",
    "        if task != BrainModel.Task.passive:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def start_recording(self, recording_target=BrainModel.RecordingTarget, time_bins=List[Tuple[int]]):\n",
    "        print(f\"Recording from {recording_target} during {time_bins} ms\")\n",
    "        if str(recording_target) != \"IT\":\n",
    "            raise NotImplementedError(f\"RandomITModel only supports IT, not {recording_target}\")\n",
    "        if len(time_bins) != 1:\n",
    "            raise NotImplementedError(f\"RandomITModel only supports a single start-end time-bin, not {time_bins}\")\n",
    "        time_bins = time_bins[0].tolist()\n",
    "        self._time_bin_start, self._time_bin_end = time_bins[0], time_bins[1]\n",
    "\n",
    "model = RandomITModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation maps a given brain region to a neural network layer. In the look_at method, the class just creates a mock result and returns it. The other two methods only check for correctness of the input values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines load the `dicarlo.Majaj2015.IT-pls` benchmark, consisting of neural recordings in macaque IT from `Majaj et al. 2015` and a neural predictivity metric based on PLS regression to compare between model predictions and actual data. Running the benchmark with the `RandomITModel` then returns a score of the model's brain-likeness under this particular benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "cross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording from IT during [array((70, 170), dtype=object)] ms\n",
      "Looking at 2560 stimuli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 10/10 [00:27<00:00,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([2.248462e-05, 4.558064e-03])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (aggregation: 2)>\\narray([-0.003897,  0.004558])\\...\n",
      "    ceiling:  <xarray.Score (aggregation: 2)>\\narray([8.218406e-01, 5.871342e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore.benchmarks import benchmark_pool\n",
    "it_benchmark = benchmark_pool['dicarlo.Majaj2015.IT-pls']\n",
    "score = it_benchmark(model)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark (1) recorded from the model as its response to 2560 stimuli, (2) applied the neural predictivity metric to compare the predicted model recordings with the actual primate recordings to yield a score, (3) normalized the score by the ceiling.\n",
    "Since the benchmark already cross-validated results, the resulting score now contains the center (i.e. the average of the splits, in this case the mean) and the error (in this case standard-error-of-the-mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.000+-0.005\n"
     ]
    }
   ],
   "source": [
    "center, error = score.sel(aggregation='center'), score.sel(aggregation='error')\n",
    "print(f\"score: {center.values:.3f}+-{error.values:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score tells us that random features don't predict IT recordings well.\n",
    "\n",
    "We can also check the raw unceiled values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([-0.003897,  0.004558])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (split: 10, neuroid: 168)>\\narray([[-0.038707,  0...\n"
     ]
    }
   ],
   "source": [
    "unceiled_scores = score.raw\n",
    "print(unceiled_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...as well as the per-neuroid, per-split correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (split: 10, neuroid: 168)>\n",
      "array([[-0.038707,  0.059978,  0.122436, ..., -0.030699,  0.0006  , -0.047434],\n",
      "       [-0.078828, -0.10048 , -0.055121, ..., -0.037108, -0.060356, -0.011921],\n",
      "       [ 0.002063, -0.072828, -0.03075 , ..., -0.127875, -0.139753, -0.160457],\n",
      "       ...,\n",
      "       [-0.01951 , -0.017674,  0.009499, ..., -0.020619,  0.03396 , -0.008168],\n",
      "       [-0.02456 ,  0.018796,  0.054428, ..., -0.016975, -0.037882, -0.025606],\n",
      "       [-0.040782, -0.116492, -0.049361, ...,  0.015116,  0.067055,  0.049239]])\n",
      "Coordinates:\n",
      "  * split       (split) int64 0 1 2 3 4 5 6 7 8 9\n",
      "  * neuroid     (neuroid) MultiIndex\n",
      "  - neuroid_id  (neuroid) object 'Chabo_L_A_2_4' ... 'Tito_L_M_9_8'\n",
      "  - arr         (neuroid) object 'A' 'A' 'A' 'A' 'A' 'A' ... 'M' 'M' 'M' 'M' 'M'\n",
      "  - col         (neuroid) int64 4 3 5 0 1 2 3 4 5 6 2 ... 4 5 6 7 8 1 3 4 5 7 8\n",
      "  - hemisphere  (neuroid) object 'L' 'L' 'L' 'L' 'L' 'L' ... 'L' 'L' 'L' 'L' 'L'\n",
      "  - subregion   (neuroid) object 'cIT' 'cIT' 'aIT' 'cIT' ... 'pIT' 'pIT' 'pIT'\n",
      "  - animal      (neuroid) object 'Chabo' 'Chabo' 'Chabo' ... 'Tito' 'Tito'\n",
      "  - y           (neuroid) float64 -1.0 -0.6 -0.6 -0.2 -0.2 ... 1.8 1.8 1.8 1.8\n",
      "  - x           (neuroid) float64 -0.2 -0.6 0.2 -1.8 -1.4 ... -0.2 0.2 1.0 1.4\n",
      "  - row         (neuroid) int64 2 3 3 4 4 4 4 4 4 4 5 ... 8 8 8 8 8 9 9 9 9 9 9\n",
      "  - region      (neuroid) object 'IT' 'IT' 'IT' 'IT' ... 'IT' 'IT' 'IT' 'IT'\n"
     ]
    }
   ],
   "source": [
    "raw_scores = score.raw.raw\n",
    "print(raw_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define our own benchmarks.\n",
    "\n",
    "The benchmark fulfills two purposes:\n",
    "1. reproduce the primate experiment on the model\n",
    "2. apply a similarity metric to compare predictions with actual measurements\n",
    "3. normalize the match with the ceiling, i.e. an upper bound on how well a model could do\n",
    "\n",
    "The following example implements a simple benchmark that show-cases these three steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task Task.passive\n",
      "Recording from IT during [array([100, 120])] ms\n",
      "Looking at 1000 stimuli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "cross-validation:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing model-match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 3/3 [00:07<00:00,  2.52s/it]\n",
      "cross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ceiling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 10/10 [00:36<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([0.000127, 0.000577])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (aggregation: 2)>\\narray([0.006688, 0.000577])\\nC...\n",
      "    ceiling:  <xarray.Score (aggregation: 2)>\\narray([0.593081, 0.003708])\\nC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import brainscore\n",
    "from brainscore.benchmarks import Benchmark\n",
    "from brainscore.metrics.regression import CrossRegressedCorrelation, pls_regression, pearsonr_correlation\n",
    "from brainscore.metrics.ceiling import InternalConsistency\n",
    "from brainscore.benchmarks.neural import explained_variance\n",
    "from brainscore.assemblies.private import average_repetition\n",
    "\n",
    "# Let's say, we want to test the model's match to IT recordings between 100-120 ms.\n",
    "# We'll use the same Majaj et al. 2015 data from primates passively fixating.\n",
    "\n",
    "class MyBenchmark(Benchmark):\n",
    "    def __init__(self):\n",
    "        # both StimulusSets as well as assemblies are packaged through https://github.com/brain-score/brainio_contrib\n",
    "        assembly = brainscore.get_assembly('dicarlo.Majaj2015.temporal')  # this will take a while to download and open\n",
    "        assembly = assembly[{'time_bin': [start == 100 for start in assembly['time_bin_start'].values]}]\n",
    "        # also, let's only look at a subset of the images\n",
    "        image_ids = np.unique(assembly['image_id'].values)[:1000]\n",
    "        assembly = assembly.loc[{'presentation': [image_id in image_ids for image_id in assembly['image_id'].values]}]\n",
    "        stimulus_set = assembly.stimulus_set  # assemblies always have a StimulusSet attached to them\n",
    "        stimulus_set = stimulus_set[stimulus_set['image_id'].isin(image_ids)]\n",
    "        assembly.attrs['stimulus_set'] = stimulus_set\n",
    "        # reduce to presentation x neuroid for simplicity (we only have one time_bin)\n",
    "        assembly = assembly.squeeze('time_bin')\n",
    "        self._assembly = assembly  # note that this assembly still has repetitions which we need for the ceiling\n",
    "        self._similarity_metric = CrossRegressedCorrelation(\n",
    "                                       regression=pls_regression(), correlation=pearsonr_correlation(),\n",
    "                                       crossvalidation_kwargs=dict(splits=3, stratification_coord='object_name'))\n",
    "        self._ceiler = InternalConsistency()\n",
    "    \n",
    "    @property\n",
    "    def identifier(self):  # for storing results\n",
    "        return \"my-dummy-benchmark\"\n",
    "    \n",
    "    def __call__(self, candidate: BrainModel):  \n",
    "        # since the candidate follows the BrainModel interface, we can easily treat all models the same way.\n",
    "        # (1) reproduce the experiment on the model. \n",
    "        candidate.start_task(task=BrainModel.Task.passive)\n",
    "        candidate.start_recording(recording_target=\"IT\", time_bins=[np.array((100, 120))])\n",
    "        predictions = candidate.look_at(stimuli=self._assembly.stimulus_set)\n",
    "        # (2) compute similarity between predictions and measurements\n",
    "        assembly = average_repetition(self._assembly)  # average over repetitions\n",
    "        predictions = predictions.squeeze('time_bin')\n",
    "        print(\"Computing model-match\")\n",
    "        unceiled_score = self._similarity_metric(predictions, assembly)\n",
    "        # (3) normalize by our estimate of how well the ideal model could do\n",
    "        ceiled_score = explained_variance(unceiled_score, self.ceiling)\n",
    "        return ceiled_score\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def ceiling(self):\n",
    "        print(\"Computing ceiling\")\n",
    "        return self._ceiler(self._assembly)\n",
    "\n",
    "my_benchmark = MyBenchmark()\n",
    "model = RandomITModel()  # we'll use the same model from before\n",
    "score = my_benchmark(model)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a custom benchmark from scratch, using our own methods.\n",
    "To interface with the rest of Brain-Score, it is easiest if we just provide those to the Benchmark class.\n",
    "(But we could also not inherit and define the `__call__` method ourselves)."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mkgu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.3.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
