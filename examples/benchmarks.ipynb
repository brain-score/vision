{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarks consist of a target assembly and a metric to compare assemblies.\n",
    "They accept a source assembly to compare against and yield a score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example loads the `dicarlo.Majaj2015.IT` benchmark (consisting of neural recordings in macaque IT and a neural predictivity metric to compare),\n",
    "and compares it against recordings from V4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  10%|█         | 1/10 [00:01<00:17,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:03<00:15,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:05<00:13,  1.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  40%|████      | 4/10 [00:07<00:11,  1.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:09<00:09,  1.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  60%|██████    | 6/10 [00:11<00:07,  1.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:13<00:05,  1.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  80%|████████  | 8/10 [00:15<00:03,  1.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:17<00:01,  1.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:19<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore import benchmarks\n",
    "\n",
    "it_benchmark = benchmarks.load('dicarlo.Majaj2015.IT')\n",
    "v4_data = benchmarks.load_assembly('dicarlo.Majaj2015').sel(region='V4')\n",
    "score = it_benchmark(v4_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark applied the neural predictivity metric to compare the two recordings, and, as you can see, already cross-validated to estimate errors.\n",
    "The resulting score now contains the center (i.e. the average of the splits, in this case the mean) and the error (in this case standard-error-of-the-mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.495+-0.003\n"
     ]
    }
   ],
   "source": [
    "center, error = score.sel(aggregation='center'), score.sel(aggregation='error')\n",
    "print(f\"score: {center.values:.3f}+-{error.values:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the raw values (correlations per neuroid, per split).\n",
    "These are saved in the attributes under 'raw'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataAssembly (split: 10, neuroid: 168)>\narray([[0.188818, 0.305646, 0.234136, ..., 0.6615  , 0.682484, 0.642704],\n       [0.278896, 0.281422, 0.319581, ..., 0.670958, 0.584426, 0.584051],\n       [0.223199, 0.260997, 0.184756, ..., 0.64629 , 0.732094, 0.69738 ],\n       ...,\n       [0.203238, 0.249746, 0.188271, ..., 0.697742, 0.648629, 0.590761],\n       [0.153523, 0.333198, 0.160535, ..., 0.721704, 0.694001, 0.636323],\n       [0.216352, 0.287344, 0.292694, ..., 0.690422, 0.718551, 0.61446 ]])\nCoordinates:\n  * split       (split) int64 0 1 2 3 4 5 6 7 8 9\n  * neuroid     (neuroid) MultiIndex\n  - neuroid_id  (neuroid) object 'Chabo_L_A_2_4' ... 'Chabo_L_A_8_4'\n  - arr         (neuroid) object 'A' 'A' 'A' 'A' 'A' 'A' ... 'A' 'A' 'A' 'A' 'A'\n  - col         (neuroid) int64 4 3 5 0 1 2 3 4 5 6 2 ... 6 1 2 3 4 5 6 7 2 3 4\n  - hemisphere  (neuroid) object 'L' 'L' 'L' 'L' 'L' 'L' ... 'L' 'L' 'L' 'L' 'L'\n  - subregion   (neuroid) object 'cIT' 'cIT' 'aIT' 'cIT' ... 'cIT' 'cIT' 'cIT'\n  - animal      (neuroid) object 'Chabo' 'Chabo' 'Chabo' ... 'Chabo' 'Chabo'\n  - y           (neuroid) float64 -1.0 -0.6 -0.6 -0.2 -0.2 ... 1.0 1.4 1.4 1.4\n  - x           (neuroid) float64 -0.2 -0.6 0.2 -1.8 -1.4 ... 1.0 -1.0 -0.6 -0.2\n  - row         (neuroid) int64 2 3 3 4 4 4 4 4 4 4 5 ... 6 7 7 7 7 7 7 7 8 8 8\n"
     ]
    }
   ],
   "source": [
    "raw_scores = score.attrs['raw']\n",
    "print(raw_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define our own benchmarks.\n",
    "\n",
    "One way is to put together existing assemblies and metrics in new ways using the `build` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  10%|█         | 1/10 [00:01<00:17,  1.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:04<00:18,  2.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:06<00:14,  2.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  40%|████      | 4/10 [00:08<00:12,  2.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:10<00:09,  1.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  60%|██████    | 6/10 [00:12<00:07,  1.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:14<00:05,  1.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  80%|████████  | 8/10 [00:16<00:03,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:18<00:01,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:20<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Score(_variable=<xarray.Variable (aggregation: 2)>\narray([0.494592, 0.003259])\nAttributes:\n    raw:      <xarray.DataAssembly (split: 10, neuroid: 168)>\\narray([[0.1888...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore import benchmarks\n",
    "my_benchmark = benchmarks.build(name='my-benchmark', assembly_name='dicarlo.Majaj2015', metric_name='rdm')\n",
    "v4_data = benchmarks.load_assembly('dicarlo.Majaj2015').sel(region='V4')\n",
    "score = it_benchmark(v4_data)\n",
    "print(\"\\n\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a custom benchmark from scratch, using our own methods.\n",
    "To interface with the rest of Brain-Score, it is easiest if we just provide those to the Benchmark class.\n",
    "(But we could also not inherit and define the `__call__` method ourselves)."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mkgu"
  },
  "kernelspec": {
   "display_name": "mkgu",
   "language": "python",
   "name": "mkgu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
