{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarks consist of a target assembly and a metric to compare assemblies.\n",
    "They accept a source assembly to compare against and yield a score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brainscore defines benchmarks, which can be run on brain models. To implement a model, the BrainModel interface has to be implemented by the model to be tested. A very simple implementation could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lookup from /braintree/home/msch/brainio_collection/brainio_collection/lookup.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from brainscore.benchmarks.screen import place_on_screen\n",
    "\n",
    "from brainscore.model_interface import BrainModel\n",
    "from brainio.assemblies import DataAssembly\n",
    "\n",
    "class RandomITModel(BrainModel):\n",
    "    def __init__(self):\n",
    "        self._num_neurons = 50\n",
    "        # to note which time we are recording\n",
    "        self._time_bin_start = None\n",
    "        self._time_bin_end = None\n",
    "    \n",
    "    def look_at(self, stimuli, **kwargs):\n",
    "        print(f\"Looking at {len(stimuli)} stimuli\")\n",
    "        rnd = np.random.RandomState(0)\n",
    "        recordings = DataAssembly(rnd.rand(len(stimuli), self._num_neurons, 1),\n",
    "                              coords={'image_id': ('presentation', stimuli['image_id']),\n",
    "                                      'object_name': ('presentation', stimuli['object_name']),\n",
    "                                      'neuroid_id': ('neuroid', np.arange(self._num_neurons)),\n",
    "                                      'region': ('neuroid', ['IT'] * self._num_neurons),\n",
    "                                      'time_bin_start': ('time_bin', [self._time_bin_start]),\n",
    "                                      'time_bin_end': ('time_bin', [self._time_bin_end])},\n",
    "                              dims=['presentation', 'neuroid', 'time_bin'])\n",
    "        recordings.name = 'random_it_model'\n",
    "        return recordings\n",
    "    \n",
    "    def start_task(self, task, **kwargs):\n",
    "        print(f\"Starting task {task}\")\n",
    "        if task != BrainModel.Task.passive:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def start_recording(self, recording_target=BrainModel.RecordingTarget, time_bins=List[Tuple[int]]):\n",
    "        print(f\"Recording from {recording_target} during {time_bins} ms\")\n",
    "        if str(recording_target) != \"IT\":\n",
    "            raise NotImplementedError(f\"RandomITModel only supports IT, not {recording_target}\")\n",
    "        if len(time_bins) != 1:\n",
    "            raise NotImplementedError(f\"RandomITModel only supports a single start-end time-bin, not {time_bins}\")\n",
    "        time_bins = time_bins[0].tolist()\n",
    "        self._time_bin_start, self._time_bin_end = time_bins[0], time_bins[1]\n",
    "    \n",
    "    def visual_degrees(self):\n",
    "        print(\"Declaring model to have a visual field size of 8 degrees\")\n",
    "        return 8\n",
    "\n",
    "model = RandomITModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation maps a given brain region to a neural network layer. In the look_at method, the class just creates a mock result and returns it. The other two methods only check for correctness of the input values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines load the public benchmark `MajajHong2015public.IT-pls`,\n",
    "consisting of neural recordings in macaque IT from `Majaj, Hong et al. 2015`\n",
    "and a neural predictivity metric based on PLS regression \n",
    "to compare between model predictions and actual data. \n",
    "Running the benchmark with the `RandomITModel` \n",
    "then returns a score of the model's brain-likeness under this particular benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([1.329276e-05, 4.042570e-03])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:                   <xarray.Score (aggregation: 2)>\\narray([-0.002974,...\n",
      "    ceiling:               <xarray.Score (aggregation: 2)>\\narray([0.815799, ...\n",
      "    model_identifier:      mymodel\n",
      "    benchmark_identifier:  dicarlo.MajajHong2015public.IT-pls\n"
     ]
    }
   ],
   "source": [
    "from brainscore import score_model\n",
    "score = score_model(model_identifier='mymodel', model=model, benchmark_identifier='dicarlo.MajajHong2015public.IT-pls')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark (1) recorded from the model as its response to 2560 stimuli, (2) applied the neural predictivity metric to compare the predicted model recordings with the actual primate recordings to yield a score, (3) normalized the score by the ceiling.\n",
    "Since the benchmark already cross-validated results, the resulting score now contains the center (i.e. the average of the splits, in this case the mean) and the error (in this case standard-error-of-the-mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.000+-0.004\n"
     ]
    }
   ],
   "source": [
    "center, error = score.sel(aggregation='center'), score.sel(aggregation='error')\n",
    "print(f\"score: {center.values:.3f}+-{error.values:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score tells us that random features don't predict IT recordings well.\n",
    "\n",
    "We can also check the raw unceiled values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([-0.002974,  0.004043])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (split: 10, neuroid: 168)>\\narray([[-0.070138, -0...\n"
     ]
    }
   ],
   "source": [
    "unceiled_scores = score.raw\n",
    "print(unceiled_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...as well as the per-neuroid, per-split correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (split: 10, neuroid: 168)>\n",
      "array([[-0.070138, -0.135265, -0.003982, ...,  0.012797,  0.004866,  0.059825],\n",
      "       [-0.024563,  0.026108,  0.036809, ...,  0.055484,  0.088264,  0.040734],\n",
      "       [-0.036944, -0.008812,  0.034396, ..., -0.005293,  0.032865,  0.060247],\n",
      "       ...,\n",
      "       [-0.024136, -0.004739,  0.012494, ..., -0.04916 ,  0.02889 , -0.041967],\n",
      "       [ 0.004991, -0.038394, -0.008638, ...,  0.095637,  0.101677,  0.046107],\n",
      "       [-0.022872, -0.02758 ,  0.043983, ...,  0.049692,  0.033474, -0.0456  ]])\n",
      "Coordinates:\n",
      "  * split       (split) int64 0 1 2 3 4 5 6 7 8 9\n",
      "  * neuroid     (neuroid) MultiIndex\n",
      "  - neuroid_id  (neuroid) object 'Chabo_L_A_2_4' ... 'Tito_L_M_9_8'\n",
      "  - arr         (neuroid) object 'A' 'A' 'A' 'A' 'A' 'A' ... 'M' 'M' 'M' 'M' 'M'\n",
      "  - col         (neuroid) int64 4 3 5 0 1 2 3 4 5 6 2 ... 4 5 6 7 8 1 3 4 5 7 8\n",
      "  - hemisphere  (neuroid) object 'L' 'L' 'L' 'L' 'L' 'L' ... 'L' 'L' 'L' 'L' 'L'\n",
      "  - subregion   (neuroid) object 'cIT' 'cIT' 'aIT' 'cIT' ... 'pIT' 'pIT' 'pIT'\n",
      "  - animal      (neuroid) object 'Chabo' 'Chabo' 'Chabo' ... 'Tito' 'Tito'\n",
      "  - y           (neuroid) float64 -1.0 -0.6 -0.6 -0.2 -0.2 ... 1.8 1.8 1.8 1.8\n",
      "  - x           (neuroid) float64 -0.2 -0.6 0.2 -1.8 -1.4 ... -0.2 0.2 1.0 1.4\n",
      "  - row         (neuroid) int64 2 3 3 4 4 4 4 4 4 4 5 ... 8 8 8 8 8 9 9 9 9 9 9\n",
      "  - region      (neuroid) object 'IT' 'IT' 'IT' 'IT' ... 'IT' 'IT' 'IT' 'IT'\n"
     ]
    }
   ],
   "source": [
    "raw_scores = score.raw.raw\n",
    "print(raw_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define our own benchmarks.\n",
    "\n",
    "The benchmark fulfills two purposes:\n",
    "1. reproduce the primate experiment on the model\n",
    "2. apply a similarity metric to compare predictions with actual measurements\n",
    "3. normalize the match with the ceiling, i.e. an upper bound on how well a model could do\n",
    "\n",
    "The following example implements a simple benchmark that show-cases these three steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task Task.passive\n",
      "Recording from IT during [array([100, 120])] ms\n",
      "Declaring model to have a visual field size of 8 degrees\n",
      "Looking at 1000 stimuli\n",
      "Computing model-match\n",
      "Computing ceiling\n",
      "<xarray.Score (aggregation: 2)>\n",
      "array([9.656137e-05, 1.310140e-02])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (aggregation: 2)>\\narray([0.006589, 0.013101])\\nC...\n",
      "    ceiling:  <xarray.Score (aggregation: 2)>\\narray([0.670578, 0.002014])\\nC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 3/3 [00:04<00:00,  1.65s/it]\n",
      "cross-validation: 100%|██████████| 10/10 [00:39<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import brainscore\n",
    "from brainscore.benchmarks import Benchmark\n",
    "from brainscore.metrics.regression import CrossRegressedCorrelation, pls_regression, pearsonr_correlation\n",
    "from brainscore.metrics.ceiling import InternalConsistency\n",
    "from brainscore.benchmarks._neural_common import explained_variance, average_repetition\n",
    "\n",
    "# Let's say, we want to test the model's match to IT recordings between 100-120 ms.\n",
    "# We'll use the same Majaj et al. 2015 data from primates passively fixating.\n",
    "\n",
    "class MyBenchmark(Benchmark):\n",
    "    def __init__(self):\n",
    "        # both StimulusSets as well as assemblies are packaged through https://github.com/brain-score/brainio\n",
    "        assembly = brainscore.get_assembly('dicarlo.MajajHong2015.temporal.public')  # this will take a while to download and open\n",
    "        assembly = assembly[{'time_bin': [start == 100 for start in assembly['time_bin_start'].values]}]\n",
    "        # also, let's only look at a subset of the images\n",
    "        image_ids = np.unique(assembly['image_id'].values)[:1000]\n",
    "        assembly = assembly.loc[{'presentation': [image_id in image_ids for image_id in assembly['image_id'].values]}]\n",
    "        stimulus_set = assembly.stimulus_set  # assemblies always have a StimulusSet attached to them\n",
    "        stimulus_set = stimulus_set[stimulus_set['image_id'].isin(image_ids)]\n",
    "        assembly.attrs['stimulus_set'] = stimulus_set\n",
    "        # reduce to presentation x neuroid for simplicity (we only have one time_bin)\n",
    "        assembly = assembly.squeeze('time_bin')\n",
    "        self._assembly = assembly  # note that this assembly still has repetitions which we need for the ceiling\n",
    "        self._similarity_metric = CrossRegressedCorrelation(\n",
    "                                       regression=pls_regression(), correlation=pearsonr_correlation(),\n",
    "                                       crossvalidation_kwargs=dict(splits=3, stratification_coord='object_name'))\n",
    "        self._ceiler = InternalConsistency()\n",
    "    \n",
    "    @property\n",
    "    def identifier(self):  # for storing results\n",
    "        return \"my-dummy-benchmark\"\n",
    "    \n",
    "    def __call__(self, candidate: BrainModel):\n",
    "        # since the candidate follows the BrainModel interface, we can easily treat all models the same way.\n",
    "        # (1) reproduce the experiment on the model. \n",
    "        candidate.start_task(task=BrainModel.Task.passive)\n",
    "        candidate.start_recording(recording_target=\"IT\", time_bins=[np.array((100, 120))])\n",
    "        # since different models can have different fields of view, we adjust the image sizes accordingly.\n",
    "        # for instance, a stimulus of 2 degree should take up little space for a model with a field of view of 10 degree\n",
    "        # while the same stimulus would take up much more space for a model of 4 degrees.\n",
    "        stimulus_set = place_on_screen(self._assembly.stimulus_set, target_visual_degrees=candidate.visual_degrees(),\n",
    "                                       # for reference, we know this experiment was run at 8 degrees for the primates.\n",
    "                                       source_visual_degrees=8)\n",
    "        predictions = candidate.look_at(stimuli=stimulus_set)\n",
    "        # (2) compute similarity between predictions and measurements\n",
    "        assembly = average_repetition(self._assembly)  # average over repetitions\n",
    "        predictions = predictions.squeeze('time_bin')\n",
    "        print(\"Computing model-match\")\n",
    "        unceiled_score = self._similarity_metric(predictions, assembly)\n",
    "        # (3) normalize by our estimate of how well the ideal model could do\n",
    "        ceiled_score = explained_variance(unceiled_score, self.ceiling)\n",
    "        return ceiled_score\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def ceiling(self):\n",
    "        print(\"Computing ceiling\")\n",
    "        return self._ceiler(self._assembly)\n",
    "\n",
    "my_benchmark = MyBenchmark()\n",
    "model = RandomITModel()  # we'll use the same model from before\n",
    "score = my_benchmark(model)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a custom benchmark from scratch, using our own methods.\n",
    "To interface with the rest of Brain-Score, it is easiest if we just provide those to the Benchmark class.\n",
    "(But we could also not inherit and define the `__call__` method ourselves)."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mkgu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}