{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "Benchmarks combine a dataset, a metric, and an experimental paradigm.\n",
    "They take a model candidate as input and output a score that measures how similar the model predictions under the experimental paradigm are to the data as evaluated by the metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined benchmarks\n",
    "\n",
    "The Brain-Score community has defined many benchmarks, which models can be scored on. We can thus easily test a new or existing model on a variety of experimental datasets. To achieve this scaling, all models implement the model interface, and all benchmarks in turn only use methods from this interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /braintree/home/msch/miniconda3/lib/python3.6/site-packages (from -r /braintree/home/msch/brain-score_vision/brainscore_vision/metrics/regression_correlation/requirements.txt (line 1)) (1.19.2)\n",
      "Requirement already satisfied: scipy in /braintree/home/msch/miniconda3/lib/python3.6/site-packages (from -r /braintree/home/msch/brain-score_vision/brainscore_vision/metrics/regression_correlation/requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: scikit-learn in /braintree/home/msch/miniconda3/lib/python3.6/site-packages (from -r /braintree/home/msch/brain-score_vision/brainscore_vision/metrics/regression_correlation/requirements.txt (line 3)) (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /braintree/home/msch/miniconda3/lib/python3.6/site-packages (from scikit-learn->-r /braintree/home/msch/brain-score_vision/brainscore_vision/metrics/regression_correlation/requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: scipy in /braintree/home/msch/miniconda3/lib/python3.6/site-packages (from -r /braintree/home/msch/brain-score_vision/brainscore_vision/metrics/internal_consistency/requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /braintree/home/msch/miniconda3/lib/python3.6/site-packages (from scipy->-r /braintree/home/msch/brain-score_vision/brainscore_vision/metrics/internal_consistency/requirements.txt (line 1)) (1.19.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 10/10 [04:16<00:00, 25.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<xarray.Score ()>\narray(0.50480277)\nAttributes:\n    error:                 <xarray.Score ()>\\narray(0.00269362)\n    raw:                   <xarray.Score ()>\\narray(0.57962116)\\nAttributes:\\...\n    ceiling:               <xarray.DataArray ()>\\narray(0.81579938)\\nAttribut...\n    model_identifier:      alexnet\n    benchmark_identifier:  MajajHong2015public.IT-pls\n    comment:               layers: {'IT': 'features.12'}",
      "text/html": "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n<defs>\n<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n</symbol>\n<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n</symbol>\n</defs>\n</svg>\n<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n *\n */\n\n:root {\n  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n  --xr-background-color: var(--jp-layout-color0, white);\n  --xr-background-color-row-even: var(--jp-layout-color1, white);\n  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n}\n\nhtml[theme=dark],\nbody.vscode-dark {\n  --xr-font-color0: rgba(255, 255, 255, 1);\n  --xr-font-color2: rgba(255, 255, 255, 0.54);\n  --xr-font-color3: rgba(255, 255, 255, 0.38);\n  --xr-border-color: #1F1F1F;\n  --xr-disabled-color: #515151;\n  --xr-background-color: #111111;\n  --xr-background-color-row-even: #111111;\n  --xr-background-color-row-odd: #313131;\n}\n\n.xr-wrap {\n  display: block !important;\n  min-width: 300px;\n  max-width: 700px;\n}\n\n.xr-text-repr-fallback {\n  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n  display: none;\n}\n\n.xr-header {\n  padding-top: 6px;\n  padding-bottom: 6px;\n  margin-bottom: 4px;\n  border-bottom: solid 1px var(--xr-border-color);\n}\n\n.xr-header > div,\n.xr-header > ul {\n  display: inline;\n  margin-top: 0;\n  margin-bottom: 0;\n}\n\n.xr-obj-type,\n.xr-array-name {\n  margin-left: 2px;\n  margin-right: 10px;\n}\n\n.xr-obj-type {\n  color: var(--xr-font-color2);\n}\n\n.xr-sections {\n  padding-left: 0 !important;\n  display: grid;\n  grid-template-columns: 150px auto auto 1fr 20px 20px;\n}\n\n.xr-section-item {\n  display: contents;\n}\n\n.xr-section-item input {\n  display: none;\n}\n\n.xr-section-item input + label {\n  color: var(--xr-disabled-color);\n}\n\n.xr-section-item input:enabled + label {\n  cursor: pointer;\n  color: var(--xr-font-color2);\n}\n\n.xr-section-item input:enabled + label:hover {\n  color: var(--xr-font-color0);\n}\n\n.xr-section-summary {\n  grid-column: 1;\n  color: var(--xr-font-color2);\n  font-weight: 500;\n}\n\n.xr-section-summary > span {\n  display: inline-block;\n  padding-left: 0.5em;\n}\n\n.xr-section-summary-in:disabled + label {\n  color: var(--xr-font-color2);\n}\n\n.xr-section-summary-in + label:before {\n  display: inline-block;\n  content: '►';\n  font-size: 11px;\n  width: 15px;\n  text-align: center;\n}\n\n.xr-section-summary-in:disabled + label:before {\n  color: var(--xr-disabled-color);\n}\n\n.xr-section-summary-in:checked + label:before {\n  content: '▼';\n}\n\n.xr-section-summary-in:checked + label > span {\n  display: none;\n}\n\n.xr-section-summary,\n.xr-section-inline-details {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n.xr-section-inline-details {\n  grid-column: 2 / -1;\n}\n\n.xr-section-details {\n  display: none;\n  grid-column: 1 / -1;\n  margin-bottom: 5px;\n}\n\n.xr-section-summary-in:checked ~ .xr-section-details {\n  display: contents;\n}\n\n.xr-array-wrap {\n  grid-column: 1 / -1;\n  display: grid;\n  grid-template-columns: 20px auto;\n}\n\n.xr-array-wrap > label {\n  grid-column: 1;\n  vertical-align: top;\n}\n\n.xr-preview {\n  color: var(--xr-font-color3);\n}\n\n.xr-array-preview,\n.xr-array-data {\n  padding: 0 5px !important;\n  grid-column: 2;\n}\n\n.xr-array-data,\n.xr-array-in:checked ~ .xr-array-preview {\n  display: none;\n}\n\n.xr-array-in:checked ~ .xr-array-data,\n.xr-array-preview {\n  display: inline-block;\n}\n\n.xr-dim-list {\n  display: inline-block !important;\n  list-style: none;\n  padding: 0 !important;\n  margin: 0;\n}\n\n.xr-dim-list li {\n  display: inline-block;\n  padding: 0;\n  margin: 0;\n}\n\n.xr-dim-list:before {\n  content: '(';\n}\n\n.xr-dim-list:after {\n  content: ')';\n}\n\n.xr-dim-list li:not(:last-child):after {\n  content: ',';\n  padding-right: 5px;\n}\n\n.xr-has-index {\n  font-weight: bold;\n}\n\n.xr-var-list,\n.xr-var-item {\n  display: contents;\n}\n\n.xr-var-item > div,\n.xr-var-item label,\n.xr-var-item > .xr-var-name span {\n  background-color: var(--xr-background-color-row-even);\n  margin-bottom: 0;\n}\n\n.xr-var-item > .xr-var-name:hover span {\n  padding-right: 5px;\n}\n\n.xr-var-list > li:nth-child(odd) > div,\n.xr-var-list > li:nth-child(odd) > label,\n.xr-var-list > li:nth-child(odd) > .xr-var-name span {\n  background-color: var(--xr-background-color-row-odd);\n}\n\n.xr-var-name {\n  grid-column: 1;\n}\n\n.xr-var-dims {\n  grid-column: 2;\n}\n\n.xr-var-dtype {\n  grid-column: 3;\n  text-align: right;\n  color: var(--xr-font-color2);\n}\n\n.xr-var-preview {\n  grid-column: 4;\n}\n\n.xr-var-name,\n.xr-var-dims,\n.xr-var-dtype,\n.xr-preview,\n.xr-attrs dt {\n  white-space: nowrap;\n  overflow: hidden;\n  text-overflow: ellipsis;\n  padding-right: 10px;\n}\n\n.xr-var-name:hover,\n.xr-var-dims:hover,\n.xr-var-dtype:hover,\n.xr-attrs dt:hover {\n  overflow: visible;\n  width: auto;\n  z-index: 1;\n}\n\n.xr-var-attrs,\n.xr-var-data {\n  display: none;\n  background-color: var(--xr-background-color) !important;\n  padding-bottom: 5px !important;\n}\n\n.xr-var-attrs-in:checked ~ .xr-var-attrs,\n.xr-var-data-in:checked ~ .xr-var-data {\n  display: block;\n}\n\n.xr-var-data > table {\n  float: right;\n}\n\n.xr-var-name span,\n.xr-var-data,\n.xr-attrs {\n  padding-left: 25px !important;\n}\n\n.xr-attrs,\n.xr-var-attrs,\n.xr-var-data {\n  grid-column: 1 / -1;\n}\n\ndl.xr-attrs {\n  padding: 0;\n  margin: 0;\n  display: grid;\n  grid-template-columns: 125px auto;\n}\n\n.xr-attrs dt,\n.xr-attrs dd {\n  padding: 0;\n  margin: 0;\n  float: left;\n  padding-right: 10px;\n  width: auto;\n}\n\n.xr-attrs dt {\n  font-weight: normal;\n  grid-column: 1;\n}\n\n.xr-attrs dt:hover span {\n  display: inline-block;\n  background: var(--xr-background-color);\n  padding-right: 10px;\n}\n\n.xr-attrs dd {\n  grid-column: 2;\n  white-space: pre-wrap;\n  word-break: break-all;\n}\n\n.xr-icon-database,\n.xr-icon-file-text2 {\n  display: inline-block;\n  vertical-align: middle;\n  width: 1em;\n  height: 1.5em !important;\n  stroke-width: 0;\n  stroke: currentColor;\n  fill: currentColor;\n}\n</style><pre class='xr-text-repr-fallback'>&lt;xarray.Score ()&gt;\narray(0.50480277)\nAttributes:\n    error:                 &lt;xarray.Score ()&gt;\\narray(0.00269362)\n    raw:                   &lt;xarray.Score ()&gt;\\narray(0.57962116)\\nAttributes:\\...\n    ceiling:               &lt;xarray.DataArray ()&gt;\\narray(0.81579938)\\nAttribut...\n    model_identifier:      alexnet\n    benchmark_identifier:  MajajHong2015public.IT-pls\n    comment:               layers: {&#x27;IT&#x27;: &#x27;features.12&#x27;}</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Score</div><div class='xr-array-name'></div></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-e89ab25f-58ce-4ad0-9f76-7501f12e6c94' class='xr-array-in' type='checkbox' checked><label for='section-e89ab25f-58ce-4ad0-9f76-7501f12e6c94' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0.5048</span></div><div class='xr-array-data'><pre>array(0.50480277)</pre></div></div></li><li class='xr-section-item'><input id='section-e58a6e75-fbaf-42cf-9871-90199e3e0f4e' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-e58a6e75-fbaf-42cf-9871-90199e3e0f4e' class='xr-section-summary'  title='Expand/collapse section'>Coordinates: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-e5cc168f-8f52-403e-b8cb-a25857cb3245' class='xr-section-summary-in' type='checkbox'  checked><label for='section-e5cc168f-8f52-403e-b8cb-a25857cb3245' class='xr-section-summary' >Attributes: <span>(6)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>error :</span></dt><dd>&lt;xarray.Score ()&gt;\narray(0.00269362)</dd><dt><span>raw :</span></dt><dd>&lt;xarray.Score ()&gt;\narray(0.57962116)\nAttributes:\n    raw:      &lt;xarray.Score (split: 10, neuroid: 168)&gt;\\narray([[0.34887369, 0...\n    error:    &lt;xarray.Score ()&gt;\\narray(0.00269362)</dd><dt><span>ceiling :</span></dt><dd>&lt;xarray.DataArray ()&gt;\narray(0.81579938)\nAttributes:\n    raw:      &lt;xarray.DataArray (split: 10, neuroid: 168)&gt;\\narray([[0.1951266...\n    error:    &lt;xarray.DataArray ()&gt;\\narray(0.00144955)</dd><dt><span>model_identifier :</span></dt><dd>alexnet</dd><dt><span>benchmark_identifier :</span></dt><dd>MajajHong2015public.IT-pls</dd><dt><span>comment :</span></dt><dd>layers: {&#x27;IT&#x27;: &#x27;features.12&#x27;}</dd></dl></div></li></ul></div></div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brainscore_vision import score\n",
    "\n",
    "similarity_score = score(model_identifier='alexnet', benchmark_identifier='MajajHong2015public.IT-pls')\n",
    "similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score already aggregates over neural sites and cross-validation splits, and normalizes with respect to an estimated noise ceiling.\n",
    "\n",
    "We can also check the un-ceiled, per-split, and per-neural-site values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T13:08:42.773487Z",
     "start_time": "2024-01-02T13:08:42.770466100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (split: 10, neuroid: 168)>\n",
      "array([[0.34887369, 0.46601301, 0.45883724, ..., 0.65668614, 0.70176186,\n",
      "        0.72235326],\n",
      "       [0.31144404, 0.52688285, 0.50384677, ..., 0.66639365, 0.67149065,\n",
      "        0.68811384],\n",
      "       [0.34135641, 0.43913901, 0.48682732, ..., 0.69500215, 0.7260355 ,\n",
      "        0.71180254],\n",
      "       ...,\n",
      "       [0.21055581, 0.50588129, 0.49815291, ..., 0.67644017, 0.66275208,\n",
      "        0.68777016],\n",
      "       [0.30868527, 0.3778854 , 0.47125337, ..., 0.70168043, 0.72231718,\n",
      "        0.73980201],\n",
      "       [0.26001712, 0.44765998, 0.42908116, ..., 0.6753077 , 0.72488176,\n",
      "        0.75500281]])\n",
      "Coordinates:\n",
      "  * split       (split) int64 0 1 2 3 4 5 6 7 8 9\n",
      "  * neuroid     (neuroid) MultiIndex\n",
      "  - neuroid_id  (neuroid) object 'Chabo_L_A_2_4' ... 'Tito_L_M_9_8'\n",
      "  - arr         (neuroid) object 'A' 'A' 'A' 'A' 'A' 'A' ... 'M' 'M' 'M' 'M' 'M'\n",
      "  - col         (neuroid) int64 4 3 5 0 1 2 3 4 5 6 2 ... 4 5 6 7 8 1 3 4 5 7 8\n",
      "  - hemisphere  (neuroid) object 'L' 'L' 'L' 'L' 'L' 'L' ... 'L' 'L' 'L' 'L' 'L'\n",
      "  - subregion   (neuroid) object 'cIT' 'cIT' 'aIT' 'cIT' ... 'pIT' 'pIT' 'pIT'\n",
      "  - animal      (neuroid) object 'Chabo' 'Chabo' 'Chabo' ... 'Tito' 'Tito'\n",
      "  - y           (neuroid) float64 -1.0 -0.6 -0.6 -0.2 -0.2 ... 1.8 1.8 1.8 1.8\n",
      "  - x           (neuroid) float64 -0.2 -0.6 0.2 -1.8 -1.4 ... -0.2 0.2 1.0 1.4\n",
      "  - row         (neuroid) int64 2 3 3 4 4 4 4 4 4 4 5 ... 8 8 8 8 8 9 9 9 9 9 9\n",
      "  - region      (neuroid) object 'IT' 'IT' 'IT' 'IT' ... 'IT' 'IT' 'IT' 'IT'\n"
     ]
    }
   ],
   "source": [
    "unceiled_score = similarity_score.raw\n",
    "individual_values = unceiled_score.raw\n",
    "print(individual_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom benchmarks\n",
    "\n",
    "We can also define our own benchmarks.\n",
    "To interface with Brain-Score, each benchmark needs to implement the [`Benchmark` interface](https://brain-score-core.readthedocs.io/en/latest/modules/benchmarks.html). We especially need to write a `__call__` method that takes a model candidate as input and outputs score.\n",
    "\n",
    "Each benchmark follows the following steps:\n",
    "1. reproduce the primate experiment on the model (e.g. show the same stimuli)\n",
    "2. apply a similarity metric to compare model predictions with biological measurements\n",
    "3. normalize the similarity score with the ceiling, i.e. an upper bound on how well we would expect the best possible model to perform\n",
    "\n",
    "The following example implements a simple benchmark that show-cases these three steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T13:57:00.046444300Z",
     "start_time": "2024-01-02T13:55:13.930424100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /braintree/home/msch/miniconda3/lib/python3.6/site-packages (from -r /braintree/home/msch/brain-score_vision/brainscore_vision/metrics/rdm/requirements.txt (line 1)) (1.19.2)\n",
      "Requirement already satisfied: scipy in /braintree/home/msch/miniconda3/lib/python3.6/site-packages (from -r /braintree/home/msch/brain-score_vision/brainscore_vision/metrics/rdm/requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: scipy in /braintree/home/msch/miniconda3/lib/python3.6/site-packages (from -r /braintree/home/msch/brain-score_vision/brainscore_vision/metrics/internal_consistency/requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /braintree/home/msch/miniconda3/lib/python3.6/site-packages (from scipy->-r /braintree/home/msch/brain-score_vision/brainscore_vision/metrics/internal_consistency/requirements.txt (line 1)) (1.19.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation:   0%|          | 0/10 [22:02<?, ?it/s]\n",
      "cross-validation: 100%|██████████| 10/10 [00:42<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ceiling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 10/10 [00:48<00:00,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score ()>\n",
      "array(0.36174365)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore_vision.benchmark_helpers.neural_common import average_repetition\n",
    "from brainscore_core import Score\n",
    "from brainscore_core.benchmarks import Benchmark\n",
    "from brainscore_vision import load_dataset, load_model, load_metric, load_ceiling, BrainModel\n",
    "from brainscore_vision.benchmark_helpers.screen import place_on_screen\n",
    "\n",
    "\n",
    "# Let's say, we want to test models' match to V1 recordings with an RDM metric.\n",
    "# We'll use the Freeman*, Ziemba*, et al. 2013 data.\n",
    "\n",
    "class MyBenchmark(Benchmark):\n",
    "    def __init__(self):\n",
    "        self._assembly = load_dataset('MajajHong2015.public').sel(region='IT').squeeze('time_bin')\n",
    "        self._metric = load_metric('rdm_cv')\n",
    "        self._ceiler = load_ceiling('internal_consistency')\n",
    "\n",
    "    @property\n",
    "    def identifier(self):\n",
    "        return \"my-benchmark-name\"\n",
    "\n",
    "    def __call__(self, candidate: BrainModel) -> Score:\n",
    "        # All candidate models follow the BrainModel interface, so we can easily treat all models the same way.\n",
    "        # (1) reproduce the experiment on the model. \n",
    "        candidate.start_task(task=BrainModel.Task.passive)\n",
    "        candidate.start_recording(recording_target=\"IT\", time_bins=[(70, 170)])\n",
    "        # since different models can have different fields of view, we adjust the image sizes accordingly.\n",
    "        stimulus_set = place_on_screen(self._assembly.stimulus_set,\n",
    "                                       target_visual_degrees=candidate.visual_degrees(), source_visual_degrees=8)\n",
    "        predictions = candidate.look_at(stimuli=stimulus_set)\n",
    "        # (2) compute similarity between predictions and measurements\n",
    "        assembly = average_repetition(self._assembly)  # average over repetitions\n",
    "        unceiled_score = self._metric(predictions, assembly)\n",
    "        # (3) normalize by our estimate of how well the ideal model could do\n",
    "        ceiled_score = unceiled_score / self.ceiling\n",
    "        return ceiled_score\n",
    "\n",
    "    @property\n",
    "    def ceiling(self):\n",
    "        print(\"Computing ceiling\")\n",
    "        return self._ceiler(self._assembly)\n",
    "\n",
    "\n",
    "my_benchmark = MyBenchmark()\n",
    "model = load_model('alexnet')\n",
    "score = my_benchmark(model)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mkgu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
