{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarks consist of a target assembly and a metric to compare assemblies.\n",
    "They accept a source assembly to compare against and yield a score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brainscore defines benchmarks, which can be run on brain models. To implement a model, the BrainModel interface has to be implemented by the model to be tested. A very simple implementation could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "from brainscore.model_interface import BrainModel\n",
    "from brainio_base.assemblies import DataAssembly\n",
    "\n",
    "class LayerModel(BrainModel):\n",
    "    def __init__(self, identifier, region, layer):\n",
    "        self.identifier = identifier\n",
    "        self.layer = layer\n",
    "        self.region = region\n",
    "\n",
    "    def look_at(self, stimuli):\n",
    "        rnd = np.random.mtrand.RandomState(0)\n",
    "        source = DataAssembly(rnd.rand(len(stimuli), 5, 1),\n",
    "                              coords={'image_id': ('presentation', stimuli['image_id']),\n",
    "                                      'object_name': ('presentation', stimuli['object_name']),\n",
    "                                      'neuroid_id': ('neuroid', np.arange(5)),\n",
    "                                      'time_bin_start': ('time_bin', [70]),\n",
    "                                      'time_bin_end': ('time_bin', [170])},\n",
    "                              dims=['presentation', 'neuroid', 'time_bin'])\n",
    "        source.name = 'dicarlo.mock'\n",
    "        return source\n",
    "\n",
    "    def start_task(self, task, **kwargs):\n",
    "        if task != BrainModel.Task.passive:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def start_recording(self, recording_target=BrainModel.RecordingTarget, time_bins=List[Tuple[int]]):\n",
    "        if str(recording_target) != self.region:\n",
    "            raise NotImplementedError(\"Region \", recording_target, \" is not committed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation maps a given brain region to a neural network layer. In the look_at method, the class just creates a mock result and returns it. The other two methods only check for correctness of the input values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example loads the `dicarlo.Majaj2015.IT` benchmark (consisting of neural recordings in macaque IT and a neural predictivity metric to compare),\n",
    "and compares it against an instance of this ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 10/10 [00:19<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from brainscore import benchmarks\n",
    "\n",
    "from brainscore.benchmarks import benchmark_pool\n",
    "def run_notebook():\n",
    "    it_benchmark = benchmark_pool['dicarlo.Majaj2015.IT-pls']\n",
    "    model = LayerModel('mock', 'IT', 'main.Conv2')\n",
    "    score = it_benchmark(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark applied the neural predictivity metric to compare the two recordings, and, as you can see, already cross-validated to estimate errors.\n",
    "The resulting score now contains the center (i.e. the average of the splits, in this case the mean) and the error (in this case standard-error-of-the-mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.495+-0.003\n"
     ]
    }
   ],
   "source": [
    "center, error = score.sel(aggregation='center'), score.sel(aggregation='error')\n",
    "print(f\"score: {center.values:.3f}+-{error.values:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the raw values (correlations per neuroid, per split).\n",
    "These are saved in the attributes under 'raw'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataAssembly (split: 10, neuroid: 168)>\n",
      "array([[0.188818, 0.305646, 0.234136, ..., 0.6615  , 0.682484, 0.642704],\n",
      "       [0.278896, 0.281422, 0.319581, ..., 0.670958, 0.584426, 0.584051],\n",
      "       [0.223199, 0.260997, 0.184756, ..., 0.64629 , 0.732094, 0.69738 ],\n",
      "       ...,\n",
      "       [0.203238, 0.249746, 0.188271, ..., 0.697742, 0.648629, 0.590761],\n",
      "       [0.153523, 0.333198, 0.160535, ..., 0.721704, 0.694001, 0.636323],\n",
      "       [0.216352, 0.287344, 0.292694, ..., 0.690422, 0.718551, 0.61446 ]])\n",
      "Coordinates:\n",
      "  * split       (split) int64 0 1 2 3 4 5 6 7 8 9\n",
      "  * neuroid     (neuroid) MultiIndex\n",
      "  - neuroid_id  (neuroid) object 'Chabo_L_A_2_4' ... 'Chabo_L_A_8_4'\n",
      "  - arr         (neuroid) object 'A' 'A' 'A' 'A' 'A' 'A' ... 'A' 'A' 'A' 'A' 'A'\n",
      "  - col         (neuroid) int64 4 3 5 0 1 2 3 4 5 6 2 ... 6 1 2 3 4 5 6 7 2 3 4\n",
      "  - hemisphere  (neuroid) object 'L' 'L' 'L' 'L' 'L' 'L' ... 'L' 'L' 'L' 'L' 'L'\n",
      "  - subregion   (neuroid) object 'cIT' 'cIT' 'aIT' 'cIT' ... 'cIT' 'cIT' 'cIT'\n",
      "  - animal      (neuroid) object 'Chabo' 'Chabo' 'Chabo' ... 'Chabo' 'Chabo'\n",
      "  - y           (neuroid) float64 -1.0 -0.6 -0.6 -0.2 -0.2 ... 1.0 1.4 1.4 1.4\n",
      "  - x           (neuroid) float64 -0.2 -0.6 0.2 -1.8 -1.4 ... 1.0 -1.0 -0.6 -0.2\n",
      "  - row         (neuroid) int64 2 3 3 4 4 4 4 4 4 4 5 ... 6 7 7 7 7 7 7 7 8 8 8\n"
     ]
    }
   ],
   "source": [
    "raw_scores = score.attrs['raw']\n",
    "print(raw_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define our own benchmarks.\n",
    "\n",
    "One way is to put together existing assemblies and metrics in new ways using the `build` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 10/10 [00:20<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score(_variable=<xarray.Variable (aggregation: 2)>\n",
      "array([0.494592, 0.003259])\n",
      "Attributes:\n",
      "    raw:      <xarray.DataAssembly (split: 10, neuroid: 168)>\\narray([[0.1888...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\n",
      "array(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore.benchmarks.neural import build_benchmark\n",
    "from brainscore.assemblies.public import assembly_loaders\n",
    "from brainscore.metrics.ceiling import InternalConsistency\n",
    "from brainscore.metrics.regression import CrossRegressedCorrelation, mask_regression, ScaledCrossRegressedCorrelation, \\\n",
    "    pls_regression, pearsonr_correlation\n",
    "similarity_metric = CrossRegressedCorrelation(\n",
    "    regression=pls_regression(), correlation=pearsonr_correlation(),\n",
    "    crossvalidation_kwargs=dict(stratification_coord='object_name'))\n",
    "my_benchmark = build_benchmark(identifier='my-benchmark', assembly_loader=assembly_loaders['dicarlo.Majaj2015.lowvar.IT'],\n",
    "                                similarity_metric=similarity_metric, ceiler=InternalConsistency())\n",
    "model = LayerModel('mock', 'IT', 'main.Conv2')\n",
    "score = my_benchmark(model)\n",
    "print(\"\\n\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a custom benchmark from scratch, using our own methods.\n",
    "To interface with the rest of Brain-Score, it is easiest if we just provide those to the Benchmark class.\n",
    "(But we could also not inherit and define the `__call__` method ourselves)."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mkgu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
