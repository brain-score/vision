{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This framework is concerned with comparing two sets of data, for instance source brain and target brain.\n",
    "It does not take care of trying multiple combinations of source data (such as multiple layers in models), but only makes direct comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metric tells us how similar to assemblies (sets of data) are to each other.\n",
    "For comparison, they might be re-mapped (neural predictivity) or compared in sub-spaces (RDMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brain-Score comes with many standard metrics used in the field.\n",
    "One standardly used metric is neural predictivity: (1) it uses linear regression to linearly map between two systems (e.g. from model activations to neural firing rates), (2) it computes the correlation between predicted firing rates on held-out images, and (3) wraps all of that in cross-validation to estimate generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Predictivity with Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainscore.metrics.regression import CrossRegressedCorrelation, pls_regression, pearsonr_correlation\n",
    "\n",
    "regression = pls_regression()  # 1: define the regression\n",
    "correlation = pearsonr_correlation()  # 2: define the correlation\n",
    "metric = CrossRegressedCorrelation(regression, correlation)  # 3: wrap in cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run this metric on some datasets to obtain a score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 10/10 [00:01<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([1., 0.])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (split: 10, neuroid: 25)>\\narray([[1., 1., 1., .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from brainio.assemblies import NeuroidAssembly\n",
    "\n",
    "rnd = RandomState(0)  # for reproducibility\n",
    "assembly = NeuroidAssembly((np.arange(30 * 25) + rnd.standard_normal(30 * 25)).reshape((30, 25)),\n",
    "                           coords={'stimulus_id': ('presentation', np.arange(30)),\n",
    "                                   'object_name': ('presentation', ['a', 'b', 'c'] * 10),\n",
    "                                   'neuroid_id': ('neuroid', np.arange(25)),\n",
    "                                   'region': ('neuroid', [0] * 25)},\n",
    "                           dims=['presentation', 'neuroid'])\n",
    "prediction, target = assembly, assembly  # we're testing how well the metric can predict the dataset itself\n",
    "score = metric(source=prediction, target=target)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score values above are aggregates over splits and neuroids.\n",
    "We can also check the raw values, i.e. the value per split and per neuroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (split: 10, neuroid: 25)>\n",
      "array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]])\n",
      "Coordinates:\n",
      "  * split       (split) int64 0 1 2 3 4 5 6 7 8 9\n",
      "  * neuroid     (neuroid) MultiIndex\n",
      "  - neuroid_id  (neuroid) int64 0 1 2 3 4 5 6 7 8 ... 16 17 18 19 20 21 22 23 24\n",
      "  - region      (neuroid) int64 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(score.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brain-Score also includes comparison methods not requiring any fitting, such as the Representational Dissimilarity Matrix (RDM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 10/10 [00:00<00:00, 33.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([1., 0.])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.DataAssembly (split: 10)>\\narray([1., 1., 1., 1., 1., 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore.metrics.rdm import RDMCrossValidated\n",
    "\n",
    "metric = RDMCrossValidated()\n",
    "rdm_score = metric(assembly1=assembly, assembly2=assembly)\n",
    "print(rdm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metric simply returns a Score for the similarity of two assemblies.\n",
    "For instance, the following computes the Euclidean distance of regressed and target neuroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|██████████| 10/10 [00:00<00:00, 18.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([1.021997e-13, 1.660980e-14])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.DataAssembly (split: 10, presentation: 19, neuroid: 25)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainio.assemblies import DataAssembly\n",
    "from brainscore.metrics.transformations import CrossValidation\n",
    "from brainscore.metrics.xarray_utils import XarrayRegression\n",
    "from brainscore.metrics.regression import LinearRegression\n",
    "\n",
    "\n",
    "class DistanceMetric:\n",
    "    def __init__(self):\n",
    "        regression = LinearRegression()\n",
    "        self._regression = XarrayRegression(regression=regression)\n",
    "        self._cross_validation = CrossValidation()\n",
    "\n",
    "    def __call__(self, source, target):\n",
    "        return self._cross_validation(source, target, apply=self._apply, aggregate=self._aggregate)\n",
    "        \n",
    "    def _apply(self, source_train, target_train, source_test, target_test):\n",
    "        self._regression.fit(source_train, target_train)\n",
    "        prediction = self._regression.predict(source_test)\n",
    "        score = self._compare(prediction, target_test)\n",
    "        return score\n",
    "    \n",
    "    def _compare(self, prediction, target):\n",
    "        prediction, target = prediction.sortby('stimulus_id').sortby('neuroid_id'), target.sortby('stimulus_id').sortby('neuroid_id')\n",
    "        assert all(prediction['stimulus_id'].values == target['stimulus_id'].values)\n",
    "        assert all(prediction['neuroid_id'].values == target['neuroid_id'].values)\n",
    "        difference = np.abs(target.values - prediction.values)  # lower is better\n",
    "        return DataAssembly(difference, coords=target.coords, dims=target.dims)\n",
    "    \n",
    "    def _aggregate(self, scores):\n",
    "        return scores.median('neuroid').mean('presentation')\n",
    "    \n",
    "\n",
    "metric = DistanceMetric()\n",
    "score = metric(assembly, assembly)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mkgu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}