{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This framework is concerned with comparing two sets of data, for instance source brain and target brain.\n",
    "It does not take care of trying multiple combinations of source data (such as multiple layers in models), but only makes direct comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarks consist of a target assembly and a metric to compare assemblies.\n",
    "They accept a source assembly to compare against and yield a score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example loads the `dicarlo.Majaj2015.IT` benchmark (consisting of neural recordings in macaque IT and a neural predictivity metric to compare),\n",
    "and compares it against recordings from V4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  10%|█         | 1/10 [00:01<00:17,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:03<00:15,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:05<00:13,  1.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  40%|████      | 4/10 [00:07<00:11,  1.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:09<00:09,  1.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  60%|██████    | 6/10 [00:11<00:07,  1.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:13<00:05,  1.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  80%|████████  | 8/10 [00:15<00:03,  1.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:17<00:01,  1.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:19<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore import benchmarks\n",
    "\n",
    "it_benchmark = benchmarks.load('dicarlo.Majaj2015.IT')\n",
    "v4_data = benchmarks.load_assembly('dicarlo.Majaj2015').sel(region='V4')\n",
    "score = it_benchmark(v4_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark applied the neural predictivity metric to compare the two recordings, and, as you can see, already cross-validated to estimate errors.\n",
    "The resulting score now contains the center (i.e. the average of the splits, in this case the mean) and the error (in this case standard-error-of-the-mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.495+-0.003\n"
     ]
    }
   ],
   "source": [
    "center, error = score.sel(aggregation='center'), score.sel(aggregation='error')\n",
    "print(f\"score: {center.values:.3f}+-{error.values:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the raw values (correlations per neuroid, per split).\n",
    "These are saved in the attributes under 'raw'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataAssembly (split: 10, neuroid: 168)>\narray([[0.188818, 0.305646, 0.234136, ..., 0.6615  , 0.682484, 0.642704],\n       [0.278896, 0.281422, 0.319581, ..., 0.670958, 0.584426, 0.584051],\n       [0.223199, 0.260997, 0.184756, ..., 0.64629 , 0.732094, 0.69738 ],\n       ...,\n       [0.203238, 0.249746, 0.188271, ..., 0.697742, 0.648629, 0.590761],\n       [0.153523, 0.333198, 0.160535, ..., 0.721704, 0.694001, 0.636323],\n       [0.216352, 0.287344, 0.292694, ..., 0.690422, 0.718551, 0.61446 ]])\nCoordinates:\n  * split       (split) int64 0 1 2 3 4 5 6 7 8 9\n  * neuroid     (neuroid) MultiIndex\n  - neuroid_id  (neuroid) object 'Chabo_L_A_2_4' ... 'Chabo_L_A_8_4'\n  - arr         (neuroid) object 'A' 'A' 'A' 'A' 'A' 'A' ... 'A' 'A' 'A' 'A' 'A'\n  - col         (neuroid) int64 4 3 5 0 1 2 3 4 5 6 2 ... 6 1 2 3 4 5 6 7 2 3 4\n  - hemisphere  (neuroid) object 'L' 'L' 'L' 'L' 'L' 'L' ... 'L' 'L' 'L' 'L' 'L'\n  - subregion   (neuroid) object 'cIT' 'cIT' 'aIT' 'cIT' ... 'cIT' 'cIT' 'cIT'\n  - animal      (neuroid) object 'Chabo' 'Chabo' 'Chabo' ... 'Chabo' 'Chabo'\n  - y           (neuroid) float64 -1.0 -0.6 -0.6 -0.2 -0.2 ... 1.0 1.4 1.4 1.4\n  - x           (neuroid) float64 -0.2 -0.6 0.2 -1.8 -1.4 ... 1.0 -1.0 -0.6 -0.2\n  - row         (neuroid) int64 2 3 3 4 4 4 4 4 4 4 5 ... 6 7 7 7 7 7 7 7 8 8 8\n"
     ]
    }
   ],
   "source": [
    "raw_scores = score.attrs['raw']\n",
    "print(raw_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define our own benchmarks.\n",
    "\n",
    "One way is to put together existing assemblies and metrics in new ways using the `build` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  10%|█         | 1/10 [00:01<00:17,  1.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:04<00:18,  2.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:06<00:14,  2.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  40%|████      | 4/10 [00:08<00:12,  2.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:10<00:09,  1.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  60%|██████    | 6/10 [00:12<00:07,  1.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:14<00:05,  1.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  80%|████████  | 8/10 [00:16<00:03,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:18<00:01,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:20<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Score(_variable=<xarray.Variable (aggregation: 2)>\narray([0.494592, 0.003259])\nAttributes:\n    raw:      <xarray.DataAssembly (split: 10, neuroid: 168)>\\narray([[0.1888...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore import benchmarks\n",
    "my_benchmark = benchmarks.build(name='my-benchmark', assembly_name='dicarlo.Majaj2015', metric_name='rdm')\n",
    "v4_data = benchmarks.load_assembly('dicarlo.Majaj2015').sel(region='V4')\n",
    "score = it_benchmark(v4_data)\n",
    "print(\"\\n\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a custom benchmark from scratch, using our own methods.\n",
    "To interface with the rest of Brain-Score, it is easiest if we just provide those to the Benchmark class.\n",
    "(But we could also not inherit and define the `__call__` method ourselves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  10%|█         | 1/10 [00:01<00:15,  1.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:04<00:15,  1.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:06<00:14,  2.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  40%|████      | 4/10 [00:08<00:12,  2.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:12<00:12,  2.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  60%|██████    | 6/10 [00:14<00:09,  2.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:16<00:06,  2.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  80%|████████  | 8/10 [00:18<00:04,  2.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:22<00:02,  2.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:24<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Score(_variable=<xarray.Variable (aggregation: 2)>\narray([0.494592, 0.003259])\nAttributes:\n    raw:      <xarray.DataAssembly (split: 10, neuroid: 168)>\\narray([[0.1888...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore import benchmarks\n",
    "from brainscore.benchmarks import Benchmark\n",
    "from brainscore.metrics.rdm import RDMCrossValidated\n",
    "from brainscore.metrics.ceiling import InternalConsistency\n",
    "\n",
    "\n",
    "class MyBenchmark(Benchmark):\n",
    "    def __init__(self):\n",
    "        assembly = benchmarks.load_assembly('dicarlo.Majaj2015')  # approximate V4 and IT together\n",
    "        metric = RDMCrossValidated()\n",
    "        ceiling = InternalConsistency()\n",
    "        super(MyBenchmark, self).__init__(name='my-benchmark', target_assembly=assembly, metric=metric, ceiling=ceiling)\n",
    "\n",
    "\n",
    "v4_data = benchmarks.load_assembly('dicarlo.Majaj2015').sel(region='V4')\n",
    "score = it_benchmark(v4_data)\n",
    "print(\"\\n\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brain-Score comes with many standard metrics used in the field.\n",
    "For instance, we can easily use regression methods to compare two assemblies based on how well one predicts the neural firing rates in the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:291: UserWarning: Y residual constant at iteration 0\n  warnings.warn('Y residual constant at iteration %s' % k)\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n  r = r_num / r_den\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  10%|█         | 1/10 [00:00<00:01,  5.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:291: UserWarning: Y residual constant at iteration 0\n  warnings.warn('Y residual constant at iteration %s' % k)\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n  r = r_num / r_den\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:00<00:01,  5.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:291: UserWarning: Y residual constant at iteration 0\n  warnings.warn('Y residual constant at iteration %s' % k)\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n  r = r_num / r_den\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:00<00:01,  5.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:291: UserWarning: Y residual constant at iteration 0\n  warnings.warn('Y residual constant at iteration %s' % k)\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n  r = r_num / r_den\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  40%|████      | 4/10 [00:00<00:01,  5.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:291: UserWarning: Y residual constant at iteration 0\n  warnings.warn('Y residual constant at iteration %s' % k)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n  r = r_num / r_den\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:00<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:291: UserWarning: Y residual constant at iteration 0\n  warnings.warn('Y residual constant at iteration %s' % k)\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n  r = r_num / r_den\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  60%|██████    | 6/10 [00:01<00:00,  4.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:291: UserWarning: Y residual constant at iteration 0\n  warnings.warn('Y residual constant at iteration %s' % k)\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n  r = r_num / r_den\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:01<00:00,  4.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:291: UserWarning: Y residual constant at iteration 0\n  warnings.warn('Y residual constant at iteration %s' % k)\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n  r = r_num / r_den\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  80%|████████  | 8/10 [00:01<00:00,  4.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:291: UserWarning: Y residual constant at iteration 0\n  warnings.warn('Y residual constant at iteration %s' % k)\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n  r = r_num / r_den\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:01<00:00,  4.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:291: UserWarning: Y residual constant at iteration 0\n  warnings.warn('Y residual constant at iteration %s' % k)\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n  r = r_num / r_den\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:02<00:00,  4.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/numpy/lib/function_base.py:3250: RuntimeWarning: All-NaN slice encountered\n  r = func(a, **kwargs)\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/xarray/core/nanops.py:162: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis=axis, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Score(_variable=<xarray.Variable (aggregation: 2)>\narray([nan, nan])\nAttributes:\n    raw:      <xarray.DataAssembly (split: 10, neuroid: 25)>\\narray([[nan, na...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1545: RuntimeWarning: Degrees of freedom <= 0 for slice.\n  keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from brainscore.assemblies import NeuroidAssembly\n",
    "from brainscore.metrics.neural_predictivity import PlsPredictivity\n",
    "\n",
    "source = target = NeuroidAssembly(np.ones((30, 25)),\n",
    "                                  coords={'image_id': ('presentation', np.arange(30)),\n",
    "                                          'object_name': ('presentation', ['a', 'b', 'c'] * 10),\n",
    "                                          'neuroid_id': ('neuroid', np.arange(25)),\n",
    "                                          'region': ('neuroid', [None] * 25)},\n",
    "                                  dims=['presentation', 'neuroid'])\n",
    "metric = PlsPredictivity()\n",
    "score = metric(source=source, target=target)\n",
    "print(\"\\n\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# load and standardize data\n",
    "\n",
    "import brainscore\n",
    "\n",
    "neural_data = brainscore.get_assembly(name=\"dicarlo.Majaj2015\")\n",
    "neural_data.load()\n",
    "neural_data = neural_data.sel(variation=6).multi_groupby(['category_name', 'object_name', 'image_id']) \\\n",
    "    .mean(dim='presentation').squeeze('time_bin').T\n",
    "# Mostly, we compare neural data with computational models, \n",
    "# for deep neural networks see https://github.com/mschrimpf/brain-score-models.\n",
    "# This repository is agnostic of the comparison system, \n",
    "# To show-case the functionality, we are going to compare different regions.\n",
    "v4_data = neural_data.sel(region='V4')\n",
    "it_data = neural_data.sel(region='IT')\n",
    "\n",
    "# We can compare a set of assemblies directly by calling the metric \n",
    "# but for more sophisticated comparisons we will usually build a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per neuroid:  <xarray.DataAssembly (neuroid_id: 10)>\narray([0.431195, 0.5617  , 0.554427, 0.554573, 0.551613, 0.446308, 0.500028,\n       0.562305, 0.551316, 0.472134])\nCoordinates:\n  * neuroid_id  (neuroid_id) object 'Chabo_L_M_5_9' 'Chabo_L_M_6_9' ... ...\n\nmedian over neuroids: <xarray.DataAssembly ()>\narray(0.55406)\n"
     ]
    }
   ],
   "source": [
    "# To compare two systems, we instantiate the metric and call it on the source and target assembly.\n",
    "# The neural fit also relies on training data to instantiate the regression.\n",
    "from brainscore.metrics.neural_fit import NeuralFit\n",
    "\n",
    "neural_fit = NeuralFit()\n",
    "# For demonstration purposes, we will use the same data for training and testing. \n",
    "# (which you should obviously never do in practice)\n",
    "score = neural_fit(v4_data, it_data, v4_data, it_data)\n",
    "# This gives us a score, containing the correlations per neuroid. \n",
    "# For instance, there is one value per cross-validation split.\n",
    "print(\"per neuroid: \", score[:10], \"...\\n\")\n",
    "# Usually we want to aggregate over neuroids to yield a single scalar value:\n",
    "aggregate = neural_fit.aggregate(score)\n",
    "print(\"median over neuroids:\", aggregate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, we can use RDMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataAssembly ()>\narray(0.289892)\n"
     ]
    }
   ],
   "source": [
    "# We can easily swap out the specific metric and use e.g. RDMs.\n",
    "# To compare two systems, we instantiate the metric and call it on the source and target assembly.\n",
    "from brainscore.metrics.rdm import RDMMetric\n",
    "\n",
    "rdm = RDMMetric()\n",
    "score = rdm(v4_data, it_data)\n",
    "print(score)\n",
    "# Note how the score is much lower with RDMs due to missing re-mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom metrics"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mkgu"
  },
  "kernelspec": {
   "display_name": "mkgu",
   "language": "python",
   "name": "mkgu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
