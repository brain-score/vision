{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This framework is concerned with comparing two sets of data, for instance source brain and target brain.\n",
    "It does not take care of trying multiple combinations of source data (such as multiple layers in models), but only makes direct comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarks consist of a target assembly and a metric to compare assemblies.\n",
    "They accept a source assembly to compare against and yield a score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example loads the `dicarlo.Majaj2015.IT` benchmark (consisting of neural recordings in macaque IT and a neural predictivity metric to compare),\n",
    "and compares it against recordings from V4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  10%|█         | 1/10 [00:01<00:17,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:03<00:15,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:05<00:13,  1.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  40%|████      | 4/10 [00:07<00:11,  1.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:09<00:09,  1.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  60%|██████    | 6/10 [00:11<00:07,  1.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:13<00:05,  1.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  80%|████████  | 8/10 [00:15<00:03,  1.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:17<00:01,  1.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:19<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore import benchmarks\n",
    "\n",
    "it_benchmark = benchmarks.load('dicarlo.Majaj2015.IT')\n",
    "v4_data = benchmarks.load_assembly('dicarlo.Majaj2015').sel(region='V4')\n",
    "score = it_benchmark(v4_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark applied the neural predictivity metric to compare the two recordings, and, as you can see, already cross-validated to estimate errors.\n",
    "The resulting score now contains the center (i.e. the average of the splits, in this case the mean) and the error (in this case standard-error-of-the-mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.495+-0.003\n"
     ]
    }
   ],
   "source": [
    "center, error = score.sel(aggregation='center'), score.sel(aggregation='error')\n",
    "print(f\"score: {center.values:.3f}+-{error.values:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the raw values (correlations per neuroid, per split).\n",
    "These are saved in the attributes under 'raw'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataAssembly (split: 10, neuroid: 168)>\narray([[0.188818, 0.305646, 0.234136, ..., 0.6615  , 0.682484, 0.642704],\n       [0.278896, 0.281422, 0.319581, ..., 0.670958, 0.584426, 0.584051],\n       [0.223199, 0.260997, 0.184756, ..., 0.64629 , 0.732094, 0.69738 ],\n       ...,\n       [0.203238, 0.249746, 0.188271, ..., 0.697742, 0.648629, 0.590761],\n       [0.153523, 0.333198, 0.160535, ..., 0.721704, 0.694001, 0.636323],\n       [0.216352, 0.287344, 0.292694, ..., 0.690422, 0.718551, 0.61446 ]])\nCoordinates:\n  * split       (split) int64 0 1 2 3 4 5 6 7 8 9\n  * neuroid     (neuroid) MultiIndex\n  - neuroid_id  (neuroid) object 'Chabo_L_A_2_4' ... 'Chabo_L_A_8_4'\n  - arr         (neuroid) object 'A' 'A' 'A' 'A' 'A' 'A' ... 'A' 'A' 'A' 'A' 'A'\n  - col         (neuroid) int64 4 3 5 0 1 2 3 4 5 6 2 ... 6 1 2 3 4 5 6 7 2 3 4\n  - hemisphere  (neuroid) object 'L' 'L' 'L' 'L' 'L' 'L' ... 'L' 'L' 'L' 'L' 'L'\n  - subregion   (neuroid) object 'cIT' 'cIT' 'aIT' 'cIT' ... 'cIT' 'cIT' 'cIT'\n  - animal      (neuroid) object 'Chabo' 'Chabo' 'Chabo' ... 'Chabo' 'Chabo'\n  - y           (neuroid) float64 -1.0 -0.6 -0.6 -0.2 -0.2 ... 1.0 1.4 1.4 1.4\n  - x           (neuroid) float64 -0.2 -0.6 0.2 -1.8 -1.4 ... 1.0 -1.0 -0.6 -0.2\n  - row         (neuroid) int64 2 3 3 4 4 4 4 4 4 4 5 ... 6 7 7 7 7 7 7 7 8 8 8\n"
     ]
    }
   ],
   "source": [
    "raw_scores = score.attrs['raw']\n",
    "print(raw_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define our own benchmarks.\n",
    "\n",
    "One way is to put together existing assemblies and metrics in new ways using the `build` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  10%|█         | 1/10 [00:01<00:17,  1.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:04<00:18,  2.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:06<00:14,  2.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  40%|████      | 4/10 [00:08<00:12,  2.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:10<00:09,  1.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  60%|██████    | 6/10 [00:12<00:07,  1.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:14<00:05,  1.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  80%|████████  | 8/10 [00:16<00:03,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:18<00:01,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:20<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Score(_variable=<xarray.Variable (aggregation: 2)>\narray([0.494592, 0.003259])\nAttributes:\n    raw:      <xarray.DataAssembly (split: 10, neuroid: 168)>\\narray([[0.1888...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore import benchmarks\n",
    "my_benchmark = benchmarks.build(name='my-benchmark', assembly_name='dicarlo.Majaj2015', metric_name='rdm')\n",
    "v4_data = benchmarks.load_assembly('dicarlo.Majaj2015').sel(region='V4')\n",
    "score = it_benchmark(v4_data)\n",
    "print(\"\\n\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a custom benchmark from scratch, using our own methods.\n",
    "To interface with the rest of Brain-Score, it is easiest if we just provide those to the Benchmark class.\n",
    "(But we could also not inherit and define the `__call__` method ourselves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  10%|█         | 1/10 [00:01<00:15,  1.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:04<00:15,  1.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:06<00:14,  2.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  40%|████      | 4/10 [00:08<00:12,  2.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:12<00:12,  2.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  60%|██████    | 6/10 [00:14<00:09,  2.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:16<00:06,  2.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  80%|████████  | 8/10 [00:18<00:04,  2.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:22<00:02,  2.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:24<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Score(_variable=<xarray.Variable (aggregation: 2)>\narray([0.494592, 0.003259])\nAttributes:\n    raw:      <xarray.DataAssembly (split: 10, neuroid: 168)>\\narray([[0.1888...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore import benchmarks\n",
    "from brainscore.benchmarks import Benchmark\n",
    "from brainscore.metrics.rdm import RDMCrossValidated\n",
    "from brainscore.metrics.ceiling import InternalConsistency\n",
    "\n",
    "\n",
    "class MyBenchmark(Benchmark):\n",
    "    def __init__(self):\n",
    "        assembly = benchmarks.load_assembly('dicarlo.Majaj2015')  # approximate V4 and IT together\n",
    "        metric = RDMCrossValidated()\n",
    "        ceiling = InternalConsistency()\n",
    "        super(MyBenchmark, self).__init__(name='my-benchmark', target_assembly=assembly, metric=metric, ceiling=ceiling)\n",
    "\n",
    "\n",
    "v4_data = benchmarks.load_assembly('dicarlo.Majaj2015').sel(region='V4')\n",
    "score = it_benchmark(v4_data)\n",
    "print(\"\\n\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brain-Score comes with many standard metrics used in the field.\n",
    "For instance, we can easily use regression methods to compare two assemblies based on how well one predicts the neural firing rates in the other:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural (PLS) Predictivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  10%|█         | 1/10 [00:00<00:02,  3.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:00<00:02,  3.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:00<00:02,  3.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  40%|████      | 4/10 [00:01<00:01,  3.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:01<00:01,  3.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  60%|██████    | 6/10 [00:01<00:01,  3.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:02<00:00,  3.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  80%|████████  | 8/10 [00:02<00:00,  3.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:02<00:00,  3.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:03<00:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Score(_variable=<xarray.Variable (aggregation: 2)>\narray([1., 0.])\nAttributes:\n    raw:      <xarray.DataAssembly (split: 10, neuroid: 25)>\\narray([[1., 1.,...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from brainscore.assemblies import NeuroidAssembly\n",
    "from brainscore.metrics.neural_predictivity import PlsPredictivity\n",
    "\n",
    "assembly = NeuroidAssembly((np.arange(30 * 25) + np.random.standard_normal(30 * 25)).reshape((30, 25)),\n",
    "                           coords={'image_id': ('presentation', np.arange(30)),\n",
    "                                   'object_name': ('presentation', ['a', 'b', 'c'] * 10),\n",
    "                                   'neuroid_id': ('neuroid', np.arange(25)),\n",
    "                                   'region': ('neuroid', [0] * 25)},\n",
    "                           dims=['presentation', 'neuroid'])\n",
    "metric = PlsPredictivity()\n",
    "score = metric(source=assembly, target=assembly)\n",
    "print(\"\\n\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  30%|███       | 3/10 [00:00<00:00, 25.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:00<00:00, 27.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:00<00:00, 31.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Score(_variable=<xarray.Variable (aggregation: 2)>\narray([1.000000e+00, 2.482534e-17])\nAttributes:\n    raw:      <xarray.DataAssembly (split: 10)>\\narray([1., 1., 1., 1., 1., 1...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    }
   ],
   "source": [
    "from brainscore.metrics.rdm import RDMCrossValidated\n",
    "\n",
    "metric = RDMCrossValidated()\n",
    "rdm_score = metric(assembly1=assembly, assembly2=assembly)\n",
    "print(\"\\n\", rdm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above values are aggregate values over splits and neuroids.\n",
    "We can also check the raw values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataAssembly (split: 10, neuroid: 25)>\narray([[1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       ...,\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]])\nCoordinates:\n  * split       (split) int64 0 1 2 3 4 5 6 7 8 9\n  * neuroid     (neuroid) MultiIndex\n  - neuroid_id  (neuroid) int64 0 1 2 3 4 5 6 7 8 ... 16 17 18 19 20 21 22 23 24\n  - region      (neuroid) int64 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(score.attrs['raw'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metric simply returns a Score for the similarity of two assemblies.\n",
    "For instance, the following computes the Euclidean distance of regressed and target neuroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  20%|██        | 2/10 [00:00<00:00, 15.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  50%|█████     | 5/10 [00:00<00:00, 16.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  70%|███████   | 7/10 [00:00<00:00, 16.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation:  90%|█████████ | 9/10 [00:00<00:00, 13.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rcross-validation: 100%|██████████| 10/10 [00:00<00:00, 14.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Score(_variable=<xarray.Variable (aggregation: 2)>\narray([1.017260e-13, 9.840511e-15])\nAttributes:\n    raw:      <xarray.DataAssembly (split: 10, presentation: 19, neuroid: 25)...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n/home/martin/miniconda3/envs/brainscore/lib/python3.7/site-packages/numpy/lib/function_base.py:3250: RuntimeWarning: All-NaN slice encountered\n  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from brainscore.assemblies import DataAssembly\n",
    "from brainscore.metrics.transformations import CrossValidation\n",
    "from brainscore.metrics.xarray_utils import XarrayRegression\n",
    "from brainscore.metrics.neural_predictivity import LinearRegression\n",
    "\n",
    "\n",
    "class DistanceMetric:\n",
    "    def __init__(self):\n",
    "        regression = LinearRegression()\n",
    "        self._regression = XarrayRegression(regression=regression)\n",
    "        self._cross_validation = CrossValidation()\n",
    "\n",
    "    def __call__(self, source, target):\n",
    "        return self._cross_validation(source, target, apply=self._apply, aggregate=self._aggregate)\n",
    "        \n",
    "    def _apply(self, source_train, target_train, source_test, target_test):\n",
    "        self._regression.fit(source_train, target_train)\n",
    "        prediction = self._regression.predict(source_test)\n",
    "        score = self._compare(prediction, target_test)\n",
    "        return score\n",
    "    \n",
    "    def _compare(self, prediction, target):\n",
    "        prediction, target = prediction.sortby('image_id').sortby('neuroid_id'), target.sortby('image_id').sortby('neuroid_id')\n",
    "        assert all(prediction['image_id'].values == target['image_id'].values)\n",
    "        assert all(prediction['neuroid_id'].values == target['neuroid_id'].values)\n",
    "        difference = np.abs(target.values - prediction.values)  # lower is better\n",
    "        return DataAssembly(difference, coords=target.coords, dims=target.dims)\n",
    "    \n",
    "    def _aggregate(self, scores):\n",
    "        return scores.median('neuroid').mean('presentation')\n",
    "    \n",
    "\n",
    "metric = DistanceMetric()\n",
    "score = metric(assembly, assembly)\n",
    "print(\"\\n\", score)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mkgu"
  },
  "kernelspec": {
   "display_name": "mkgu",
   "language": "python",
   "name": "mkgu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
