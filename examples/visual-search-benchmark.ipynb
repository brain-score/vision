{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/shashi/projects/brain_score/candidate_models')\n",
    "sys.path.append('/home/shashi/projects/brain_score/brain-score')\n",
    "sys.path.append('/home/shashi/projects/brain_score/model-tools')\n",
    "sys.path.append('/home/shashi/projects/brain_score/brainio_collection')\n",
    "sys.path.append('/home/shashi/projects/brain_score/tf-models/research/slim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from candidate_models.model_commitments import MLSearchPool\n",
    "from brainscore.benchmarks import benchmark_pool\n",
    "import numpy as np\n",
    "import brainscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on Object Array Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vgg-16 is accessed again and reloaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shashi/anaconda3/envs/bs/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "activations: 100%|██████████| 320/320 [00:00<00:00, 424.95it/s]\n",
      "layer packaging: 100%|██████████| 1/1 [00:00<00:00, 155.41it/s]\n",
      "activations: 100%|██████████| 320/320 [00:24<00:00, 13.13it/s]\n",
      "layer packaging: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n",
      "visual search stimuli: 100%|██████████| 300/300 [00:48<00:00,  6.20it/s]\n",
      "comparing with human data: 100%|██████████| 15/15 [00:03<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([0.941507, 0.005717])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (aggregation: 2)>\\narray([0.415299, 0.005717])\\nC...\n",
      "    ceiling:  <xarray.Score (aggregation: 2)>\\narray([0.4411,    nan])\\nCoord...\n"
     ]
    }
   ],
   "source": [
    "brain_translated_pool = MLSearchPool(target_img_size=28, search_image_size=224)\n",
    "identifier = 'vgg-16'\n",
    "model = brain_translated_pool[identifier]\n",
    "\n",
    "benchmark = benchmark_pool['klab.Zhang2018.ObjSearch-objarr']\n",
    "score = benchmark(model)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on Waldo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vgg-16 is accessed again and reloaded\n",
      "activations: 100%|██████████| 128/128 [00:01<00:00, 85.61it/s]\n",
      "layer packaging: 100%|██████████| 1/1 [00:00<00:00, 172.02it/s]\n",
      "activations: 100%|██████████| 128/128 [01:04<00:00,  1.97it/s]\n",
      "layer packaging: 100%|██████████| 1/1 [00:05<00:00,  5.78s/it]\n",
      "visual search stimuli: 100%|██████████| 67/67 [01:01<00:00,  1.08it/s]\n",
      "comparing with human data: 100%|██████████| 15/15 [00:05<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([0.767607, 0.00801 ])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (aggregation: 2)>\\narray([0.270121, 0.00801 ])\\nC...\n",
      "    ceiling:  <xarray.Score (aggregation: 2)>\\narray([0.3519,    nan])\\nCoord...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Shashi: Martin how we can change the batch size for activation model\n",
    "# Actual target_img_size should be 28 and search_img_size be 1024\n",
    "# But due to memory issues I have resized it show as to verify if the benchmark is working or not..\n",
    "\n",
    "brain_translated_pool = MLSearchPool(target_img_size=16, search_image_size=580)\n",
    "identifier = 'vgg-16'\n",
    "model = brain_translated_pool[identifier]\n",
    "\n",
    "benchmark = benchmark_pool['klab.Zhang2018.VisualSearch-waldo']\n",
    "score = benchmark(model)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on Natural Design Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vgg-16 is accessed again and reloaded\n",
      "activations: 100%|██████████| 256/256 [00:01<00:00, 133.79it/s]\n",
      "layer packaging: 100%|██████████| 1/1 [00:00<00:00, 161.11it/s]\n",
      "activations: 100%|██████████| 256/256 [02:05<00:00,  2.04it/s]\n",
      "layer packaging: 100%|██████████| 1/1 [00:02<00:00,  2.43s/it]\n",
      "visual search stimuli: 100%|██████████| 240/240 [03:28<00:00,  1.15it/s]\n",
      "comparing with human data: 100%|██████████| 15/15 [00:12<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([0.596883, 0.009701])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (aggregation: 2)>\\narray([0.235948, 0.009701])\\nC...\n",
      "    ceiling:  <xarray.Score (aggregation: 2)>\\narray([0.3953,    nan])\\nCoord...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Shashi: Martin how we can change the batch size for activation model\n",
    "# Actual target_img_size should be 28 and search_img_size be 1024\n",
    "# But due to memory issues I have resized it show as to verify if the benchmark is working or not..\n",
    "\n",
    "brain_translated_pool = MLSearchPool(target_img_size=16, search_image_size=580)\n",
    "identifier = 'vgg-16'\n",
    "model = brain_translated_pool[identifier]\n",
    "\n",
    "benchmark = benchmark_pool['klab.Zhang2018.VisualSearch-naturaldesign']\n",
    "score = benchmark(model)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
