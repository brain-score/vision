{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import brainscore_vision\n",
    "from brainio.assemblies import DataAssembly, BehavioralAssembly, walk_coords\n",
    "from brainscore_vision.benchmark_helpers.screen import place_on_screen\n",
    "from brainscore_vision.model_helpers.activations import PytorchWrapper\n",
    "from brainscore_vision.model_helpers.brain_transformation import ModelCommitment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_custom():\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from brainscore_vision.model_helpers.activations.pytorch import load_preprocess_images\n",
    "\n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MyModel, self).__init__()\n",
    "            np.random.seed(0)\n",
    "            torch.random.manual_seed(0)\n",
    "            self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3)\n",
    "            self.relu1 = torch.nn.ReLU()\n",
    "            linear_input_size = np.power((224 - 3 + 2 * 0) / 1 + 1, 2) * 2\n",
    "            self.linear = torch.nn.Linear(int(linear_input_size), 1000)\n",
    "            self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.relu1(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.linear(x)\n",
    "            x = self.relu2(x)\n",
    "            return x\n",
    "\n",
    "    preprocessing = functools.partial(load_preprocess_images, image_size=224)\n",
    "    return PytorchWrapper(model=MyModel(), preprocessing=preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(features, similarity_measure='dot'):\n",
    "   print(features, type(features))\n",
    "   features = features.transpose('presentation', 'neuroid')\n",
    "   values = features.values\n",
    "   if similarity_measure == 'dot':\n",
    "      similarity_matrix = np.dot(values, np.transpose(values))\n",
    "   elif similarity_measure == 'cosine':\n",
    "      row_norms = np.linalg.norm(values, axis=1).reshape(-1, 1)\n",
    "      norm_product = np.dot(row_norms, row_norms.T)\n",
    "      dot_product = np.dot(values, np.transpose(values))\n",
    "      similarity_matrix = dot_product / norm_product\n",
    "   else:\n",
    "      raise ValueError(\n",
    "      f\"Unknown similarity_measure {similarity_measure} -- expected one of 'dot' or 'cosine'\")\n",
    "\n",
    "   similarity_matrix = DataAssembly(similarity_matrix, coords={\n",
    "        **{f\"{coord}_left\": ('presentation_left', values) for coord, _, values in\n",
    "           walk_coords(features['presentation'])},\n",
    "        **{f\"{coord}_right\": ('presentation_right', values) for coord, _, values in\n",
    "           walk_coords(features['presentation'])}\n",
    "   }, dims=['presentation_left', 'presentation_right'])\n",
    "   return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_choices(similarity_matrix, triplets):\n",
    "    triplets = np.array(triplets).reshape(-1, 3)\n",
    "    choice_predictions = []\n",
    "    for triplet in triplets:\n",
    "        i, j, k = triplet\n",
    "        sims = similarity_matrix[i, j], similarity_matrix[i, k],  similarity_matrix[j, k]\n",
    "        idx = triplet[2 - np.argmax(sims)]\n",
    "        choice_predictions.append(idx)\n",
    "    # TODO return as DataAssembly\n",
    "    return choice_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from brainscore_vision import load_stimulus_set, load_dataset \n",
    "\n",
    "assembly = load_dataset('Hebart2023')\n",
    "stimulus_set = load_stimulus_set(\"Hebart2023\")\n",
    "triplets = np.array([\n",
    "    assembly.coords[\"image_1\"].values,\n",
    "    assembly.coords[\"image_2\"].values,\n",
    "    assembly.coords[\"image_3\"].values\n",
    "]).T.reshape(-1, 1)\n",
    "\n",
    "triplets = np.array([f\"{triplet[0]}.jpg\" for triplet in triplets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "activations_model = pytorch_custom()\n",
    "layers = [\"relu2\"]\n",
    "\n",
    "# create brain model\n",
    "brain_model = ModelCommitment(\n",
    "    identifier=activations_model.identifier, \n",
    "    activations_model=activations_model, \n",
    "    layers=[None], \n",
    "    behavioral_readout_layer='relu2')\n",
    "\n",
    "# get activations\n",
    "assy = brainscore_vision.load_dataset(f'Hebart2023')\n",
    "stimuli = place_on_screen(\n",
    "    stimulus_set=assy.stimulus_set,\n",
    "    target_visual_degrees=brain_model.visual_degrees(),\n",
    "    source_visual_degrees=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>top_down_1</th>\n",
       "      <th>rank</th>\n",
       "      <th>Wordnet_ID4</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>example_image</th>\n",
       "      <th>top_down_2</th>\n",
       "      <th>filename</th>\n",
       "      <th>Wordnet_ID2</th>\n",
       "      <th>dispersion</th>\n",
       "      <th>bottom_up</th>\n",
       "      <th>word_freq</th>\n",
       "      <th>dominant_part</th>\n",
       "      <th>freq_1</th>\n",
       "      <th>WordNet_synonyms</th>\n",
       "      <th>freq_2</th>\n",
       "      <th>WordNet_ID</th>\n",
       "      <th>Wordnet_ID3</th>\n",
       "      <th>word_freq_online</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>animal</td>\n",
       "      <td>51507.0</td>\n",
       "      <td>aardvark.n.01</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>https://imgur.com/LAJGlN0</td>\n",
       "      <td>animal</td>\n",
       "      <td>0.jpg</td>\n",
       "      <td>aardvark%1:05:00::</td>\n",
       "      <td>0.78</td>\n",
       "      <td>animal</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aardvark, ant_bear, anteater, Orycteropus_afer</td>\n",
       "      <td>21.0</td>\n",
       "      <td>n02082791</td>\n",
       "      <td>aardvark#1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34578.0</td>\n",
       "      <td>abacus.n.02</td>\n",
       "      <td>abacus</td>\n",
       "      <td>https://imgur.com/peZeM0l</td>\n",
       "      <td>home decor</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>abacus%1:06:00::</td>\n",
       "      <td>0.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abacus</td>\n",
       "      <td>12.0</td>\n",
       "      <td>n02666196</td>\n",
       "      <td>abacus#2</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>musical instrument</td>\n",
       "      <td>15132.0</td>\n",
       "      <td>accordion.n.01</td>\n",
       "      <td>accordion</td>\n",
       "      <td>https://imgur.com/GgGvdZR</td>\n",
       "      <td>musical instrument</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>accordion%1:06:00::</td>\n",
       "      <td>0.90</td>\n",
       "      <td>musical instrument</td>\n",
       "      <td>735.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accordion, piano_accordion, squeeze_box</td>\n",
       "      <td>67.0</td>\n",
       "      <td>n02672831</td>\n",
       "      <td>accordion#1</td>\n",
       "      <td>816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>fruit</td>\n",
       "      <td>16007.0</td>\n",
       "      <td>acorn.n.01</td>\n",
       "      <td>acorn</td>\n",
       "      <td>https://imgur.com/YfIB5lM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>acorn%1:20:00::</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>692.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>238.0</td>\n",
       "      <td>acorn</td>\n",
       "      <td>37.0</td>\n",
       "      <td>n12267677</td>\n",
       "      <td>acorn#1</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>air_conditioner.n.01</td>\n",
       "      <td>air_conditioner</td>\n",
       "      <td>https://imgur.com/KqYNwWH</td>\n",
       "      <td>electronic device</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>air_conditioner%1:06:00::</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>air_conditioner, air_conditioning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>n02686379</td>\n",
       "      <td>air_conditioner#1</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stimulus_id          top_down_1     rank           Wordnet_ID4  \\\n",
       "0            0              animal  51507.0         aardvark.n.01   \n",
       "1            1                 NaN  34578.0           abacus.n.02   \n",
       "2            2  musical instrument  15132.0        accordion.n.01   \n",
       "3            3               fruit  16007.0            acorn.n.01   \n",
       "4            4                 NaN      NaN  air_conditioner.n.01   \n",
       "\n",
       "         unique_id              example_image          top_down_2 filename  \\\n",
       "0         aardvark  https://imgur.com/LAJGlN0              animal    0.jpg   \n",
       "1           abacus  https://imgur.com/peZeM0l          home decor    1.jpg   \n",
       "2        accordion  https://imgur.com/GgGvdZR  musical instrument    2.jpg   \n",
       "3            acorn  https://imgur.com/YfIB5lM                 NaN    3.jpg   \n",
       "4  air_conditioner  https://imgur.com/KqYNwWH   electronic device    4.jpg   \n",
       "\n",
       "                 Wordnet_ID2  dispersion           bottom_up  word_freq  \\\n",
       "0         aardvark%1:05:00::        0.78              animal       28.0   \n",
       "1           abacus%1:06:00::        0.86                 NaN       97.0   \n",
       "2        accordion%1:06:00::        0.90  musical instrument      735.0   \n",
       "3            acorn%1:20:00::        0.85                 NaN      692.0   \n",
       "4  air_conditioner%1:06:00::         NaN                 NaN        NaN   \n",
       "\n",
       "  dominant_part  freq_1                                WordNet_synonyms  \\\n",
       "0          Noun     NaN  aardvark, ant_bear, anteater, Orycteropus_afer   \n",
       "1          Noun     NaN                                          abacus   \n",
       "2          Noun     NaN         accordion, piano_accordion, squeeze_box   \n",
       "3          Noun   238.0                                           acorn   \n",
       "4           NaN     NaN               air_conditioner, air_conditioning   \n",
       "\n",
       "   freq_2 WordNet_ID        Wordnet_ID3  word_freq_online  \n",
       "0    21.0  n02082791         aardvark#1                53  \n",
       "1    12.0  n02666196           abacus#2               188  \n",
       "2    67.0  n02672831        accordion#1               816  \n",
       "3    37.0  n12267677            acorn#1              1289  \n",
       "4     0.0  n02686379  air_conditioner#1               943  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143e0281bd6747c1a5c6177627d2b663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "activations:   0%|          | 0/1856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1146.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xr/8j65lqqn09q37xqx59yx9v480000gn/T/ipykernel_9606/3464802130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0munique_stimuli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_stimuli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'presentation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'neuroid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/pytorch.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# cannot assign __call__ as attribute due to Python convention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, stimuli, layers, stimuli_identifier)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_stimulus_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimulus_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimuli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_identifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimuli_identifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimuli_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimuli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_identifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimuli_identifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_stimulus_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimulus_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_identifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/core.py\u001b[0m in \u001b[0;36mfrom_paths\u001b[0;34m(self, stimuli_paths, layers, stimuli_identifier)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# This is done here, before storing, so that we only store the reduced activations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mreduced_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimuli_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduced_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimuli_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/core.py\u001b[0m in \u001b[0;36m_from_paths\u001b[0;34m(self, layers, stimuli_paths)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No layers passed to retrieve activations from\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running stimuli'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mlayer_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_activations_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimuli_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Packaging into assembly'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/core.py\u001b[0m in \u001b[0;36m_get_activations_batched\u001b[0;34m(self, paths, layers, batch_size)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mbatch_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mbatch_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mbatch_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_activations_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# copy to avoid handle re-enabling messing with the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mbatch_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/core.py\u001b[0m in \u001b[0;36m_get_batch_activations\u001b[0;34m(self, inputs, layer_names, batch_size)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batch_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_padding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mpreprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/pytorch.py\u001b[0m in \u001b[0;36mload_preprocess_images\u001b[0;34m(image_filepaths, image_size, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_preprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/pytorch.py\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(image_filepaths)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_filepath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_filepaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/pytorch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_filepath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_filepaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/pytorch.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(image_filepath)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'L'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'A'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0;34m'P'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# not binary and not alpha and not palletized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/bs_old/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1146.jpg'"
     ]
    }
   ],
   "source": [
    "unique_stimuli = np.unique(triplets)\n",
    "features = activations_model(unique_stimuli, layers=layers)\n",
    "features = features.transpose('presentation', 'neuroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
