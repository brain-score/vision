{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "1. Work on Assembly in \"# get activations\" part instead of the StimulusSet\n",
    "2. Translate to ``behavior.py`` and the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import brainscore_vision\n",
    "from brainio.assemblies import DataAssembly, BehavioralAssembly, walk_coords\n",
    "from brainscore_vision.benchmark_helpers.screen import place_on_screen\n",
    "from brainscore_vision.model_helpers.activations import PytorchWrapper\n",
    "from brainscore_vision.model_helpers.brain_transformation import ModelCommitment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_custom():\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from brainscore_vision.model_helpers.activations.pytorch import load_preprocess_images\n",
    "\n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MyModel, self).__init__()\n",
    "            np.random.seed(0)\n",
    "            torch.random.manual_seed(0)\n",
    "            self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3)\n",
    "            self.relu1 = torch.nn.ReLU()\n",
    "            linear_input_size = np.power((224 - 3 + 2 * 0) / 1 + 1, 2) * 2\n",
    "            self.linear = torch.nn.Linear(int(linear_input_size), 1000)\n",
    "            self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.relu1(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.linear(x)\n",
    "            x = self.relu2(x)\n",
    "            return x\n",
    "\n",
    "    preprocessing = functools.partial(load_preprocess_images, image_size=224)\n",
    "    return PytorchWrapper(model=MyModel(), preprocessing=preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(features, similarity_measure='dot'):\n",
    "   print(features, type(features))\n",
    "   features = features.transpose('presentation', 'neuroid')\n",
    "   values = features.values\n",
    "   if similarity_measure == 'dot':\n",
    "      similarity_matrix = np.dot(values, np.transpose(values))\n",
    "   elif similarity_measure == 'cosine':\n",
    "      row_norms = np.linalg.norm(values, axis=1).reshape(-1, 1)\n",
    "      norm_product = np.dot(row_norms, row_norms.T)\n",
    "      dot_product = np.dot(values, np.transpose(values))\n",
    "      similarity_matrix = dot_product / norm_product\n",
    "   else:\n",
    "      raise ValueError(\n",
    "      f\"Unknown similarity_measure {similarity_measure} -- expected one of 'dot' or 'cosine'\")\n",
    "\n",
    "   similarity_matrix = DataAssembly(similarity_matrix, coords={\n",
    "        **{f\"{coord}_left\": ('presentation_left', values) for coord, _, values in\n",
    "           walk_coords(features['presentation'])},\n",
    "        **{f\"{coord}_right\": ('presentation_right', values) for coord, _, values in\n",
    "           walk_coords(features['presentation'])}\n",
    "   }, dims=['presentation_left', 'presentation_right'])\n",
    "   return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_choices(similarity_matrix, triplets):\n",
    "    triplets = np.array(triplets).reshape(-1, 3)\n",
    "    choice_predictions = []\n",
    "    for triplet in triplets:\n",
    "        i, j, k = triplet\n",
    "        sims = similarity_matrix[i, j], similarity_matrix[i, k],  similarity_matrix[j, k]\n",
    "        idx = triplet[2 - np.argmax(sims)]\n",
    "        choice_predictions.append(idx)\n",
    "    # TODO return as DataAssembly\n",
    "    return choice_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from brainscore_vision import load_stimulus_set, load_dataset \n",
    "\n",
    "assembly = load_dataset('Hebart2023')\n",
    "stimulus_set = load_stimulus_set(\"Hebart2023\")\n",
    "triplets = np.array([\n",
    "    assembly.coords[\"image_1\"].values,\n",
    "    assembly.coords[\"image_2\"].values,\n",
    "    assembly.coords[\"image_3\"].values\n",
    "]).T.reshape(-1, 1)\n",
    "\n",
    "triplets = np.array([f\"{triplet[0]}.jpg\" for triplet in triplets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "activations_model = pytorch_custom()\n",
    "layers = [\"relu2\"]\n",
    "\n",
    "# create brain model\n",
    "brain_model = ModelCommitment(\n",
    "    identifier=activations_model.identifier, \n",
    "    activations_model=activations_model, \n",
    "    layers=[None], \n",
    "    behavioral_readout_layer='relu2')\n",
    "\n",
    "# get activations\n",
    "assy = brainscore_vision.load_dataset(f'Hebart2023')\n",
    "stimuli = place_on_screen(\n",
    "    stimulus_set=assy.stimulus_set,\n",
    "    target_visual_degrees=brain_model.visual_degrees(),\n",
    "    source_visual_degrees=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>top_down_1</th>\n",
       "      <th>rank</th>\n",
       "      <th>Wordnet_ID4</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>example_image</th>\n",
       "      <th>top_down_2</th>\n",
       "      <th>filename</th>\n",
       "      <th>Wordnet_ID2</th>\n",
       "      <th>dispersion</th>\n",
       "      <th>bottom_up</th>\n",
       "      <th>word_freq</th>\n",
       "      <th>dominant_part</th>\n",
       "      <th>freq_1</th>\n",
       "      <th>WordNet_synonyms</th>\n",
       "      <th>freq_2</th>\n",
       "      <th>WordNet_ID</th>\n",
       "      <th>Wordnet_ID3</th>\n",
       "      <th>word_freq_online</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>animal</td>\n",
       "      <td>51507.0</td>\n",
       "      <td>aardvark.n.01</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>https://imgur.com/LAJGlN0</td>\n",
       "      <td>animal</td>\n",
       "      <td>0.jpg</td>\n",
       "      <td>aardvark%1:05:00::</td>\n",
       "      <td>0.78</td>\n",
       "      <td>animal</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aardvark, ant_bear, anteater, Orycteropus_afer</td>\n",
       "      <td>21.0</td>\n",
       "      <td>n02082791</td>\n",
       "      <td>aardvark#1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34578.0</td>\n",
       "      <td>abacus.n.02</td>\n",
       "      <td>abacus</td>\n",
       "      <td>https://imgur.com/peZeM0l</td>\n",
       "      <td>home decor</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>abacus%1:06:00::</td>\n",
       "      <td>0.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abacus</td>\n",
       "      <td>12.0</td>\n",
       "      <td>n02666196</td>\n",
       "      <td>abacus#2</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>musical instrument</td>\n",
       "      <td>15132.0</td>\n",
       "      <td>accordion.n.01</td>\n",
       "      <td>accordion</td>\n",
       "      <td>https://imgur.com/GgGvdZR</td>\n",
       "      <td>musical instrument</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>accordion%1:06:00::</td>\n",
       "      <td>0.90</td>\n",
       "      <td>musical instrument</td>\n",
       "      <td>735.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accordion, piano_accordion, squeeze_box</td>\n",
       "      <td>67.0</td>\n",
       "      <td>n02672831</td>\n",
       "      <td>accordion#1</td>\n",
       "      <td>816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>fruit</td>\n",
       "      <td>16007.0</td>\n",
       "      <td>acorn.n.01</td>\n",
       "      <td>acorn</td>\n",
       "      <td>https://imgur.com/YfIB5lM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>acorn%1:20:00::</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>692.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>238.0</td>\n",
       "      <td>acorn</td>\n",
       "      <td>37.0</td>\n",
       "      <td>n12267677</td>\n",
       "      <td>acorn#1</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>air_conditioner.n.01</td>\n",
       "      <td>air_conditioner</td>\n",
       "      <td>https://imgur.com/KqYNwWH</td>\n",
       "      <td>electronic device</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>air_conditioner%1:06:00::</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>air_conditioner, air_conditioning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>n02686379</td>\n",
       "      <td>air_conditioner#1</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16792.0</td>\n",
       "      <td>yoke.n.07</td>\n",
       "      <td>yoke</td>\n",
       "      <td>https://imgur.com/nOt3K3f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1849.jpg</td>\n",
       "      <td>yoke%1:06:00::</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>143.0</td>\n",
       "      <td>yoke</td>\n",
       "      <td>22.0</td>\n",
       "      <td>n04612840</td>\n",
       "      <td>yoke#7</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>1850</td>\n",
       "      <td>food</td>\n",
       "      <td>11647.0</td>\n",
       "      <td>yolk.n.01</td>\n",
       "      <td>yolk</td>\n",
       "      <td>https://imgur.com/gWY0jPO</td>\n",
       "      <td>food</td>\n",
       "      <td>1850.jpg</td>\n",
       "      <td>yolk%1:13:00::</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>108.0</td>\n",
       "      <td>egg_yolk, yolk</td>\n",
       "      <td>21.0</td>\n",
       "      <td>n07841345</td>\n",
       "      <td>yolk#1</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>1851</td>\n",
       "      <td>animal</td>\n",
       "      <td>14397.0</td>\n",
       "      <td>zebra.n.01</td>\n",
       "      <td>zebra</td>\n",
       "      <td>https://imgur.com/xg5AAHb</td>\n",
       "      <td>animal</td>\n",
       "      <td>1851.jpg</td>\n",
       "      <td>zebra%1:05:00::</td>\n",
       "      <td>0.87</td>\n",
       "      <td>animal</td>\n",
       "      <td>839.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>224.0</td>\n",
       "      <td>zebra</td>\n",
       "      <td>128.0</td>\n",
       "      <td>n02391049</td>\n",
       "      <td>zebra#1</td>\n",
       "      <td>1066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>1852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10687.0</td>\n",
       "      <td>zipper.n.01</td>\n",
       "      <td>zipper</td>\n",
       "      <td>https://imgur.com/T2RLBxe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1852.jpg</td>\n",
       "      <td>zipper%1:06:00::</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1452.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>62.0</td>\n",
       "      <td>slide_fastener, zip, zipper, zip_fastener</td>\n",
       "      <td>144.0</td>\n",
       "      <td>n04238321</td>\n",
       "      <td>zipper#1</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>1853</td>\n",
       "      <td>vegetable</td>\n",
       "      <td>10667.0</td>\n",
       "      <td>zucchini.n.02</td>\n",
       "      <td>zucchini</td>\n",
       "      <td>https://imgur.com/vEuESRM</td>\n",
       "      <td>food, vegetable</td>\n",
       "      <td>1853.jpg</td>\n",
       "      <td>zucchini%1:13:00::</td>\n",
       "      <td>0.87</td>\n",
       "      <td>vegetable</td>\n",
       "      <td>1471.0</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zucchini, courgette</td>\n",
       "      <td>49.0</td>\n",
       "      <td>n07716358</td>\n",
       "      <td>zucchini#2</td>\n",
       "      <td>2098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1854 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stimulus_id          top_down_1     rank           Wordnet_ID4  \\\n",
       "0               0              animal  51507.0         aardvark.n.01   \n",
       "1               1                 NaN  34578.0           abacus.n.02   \n",
       "2               2  musical instrument  15132.0        accordion.n.01   \n",
       "3               3               fruit  16007.0            acorn.n.01   \n",
       "4               4                 NaN      NaN  air_conditioner.n.01   \n",
       "...           ...                 ...      ...                   ...   \n",
       "1849         1849                 NaN  16792.0             yoke.n.07   \n",
       "1850         1850                food  11647.0             yolk.n.01   \n",
       "1851         1851              animal  14397.0            zebra.n.01   \n",
       "1852         1852                 NaN  10687.0           zipper.n.01   \n",
       "1853         1853           vegetable  10667.0         zucchini.n.02   \n",
       "\n",
       "            unique_id              example_image          top_down_2  \\\n",
       "0            aardvark  https://imgur.com/LAJGlN0              animal   \n",
       "1              abacus  https://imgur.com/peZeM0l          home decor   \n",
       "2           accordion  https://imgur.com/GgGvdZR  musical instrument   \n",
       "3               acorn  https://imgur.com/YfIB5lM                 NaN   \n",
       "4     air_conditioner  https://imgur.com/KqYNwWH   electronic device   \n",
       "...               ...                        ...                 ...   \n",
       "1849             yoke  https://imgur.com/nOt3K3f                 NaN   \n",
       "1850             yolk  https://imgur.com/gWY0jPO                food   \n",
       "1851            zebra  https://imgur.com/xg5AAHb              animal   \n",
       "1852           zipper  https://imgur.com/T2RLBxe                 NaN   \n",
       "1853         zucchini  https://imgur.com/vEuESRM     food, vegetable   \n",
       "\n",
       "      filename                Wordnet_ID2  dispersion           bottom_up  \\\n",
       "0        0.jpg         aardvark%1:05:00::        0.78              animal   \n",
       "1        1.jpg           abacus%1:06:00::        0.86                 NaN   \n",
       "2        2.jpg        accordion%1:06:00::        0.90  musical instrument   \n",
       "3        3.jpg            acorn%1:20:00::        0.85                 NaN   \n",
       "4        4.jpg  air_conditioner%1:06:00::         NaN                 NaN   \n",
       "...        ...                        ...         ...                 ...   \n",
       "1849  1849.jpg             yoke%1:06:00::        0.90                 NaN   \n",
       "1850  1850.jpg             yolk%1:13:00::        0.89                 NaN   \n",
       "1851  1851.jpg            zebra%1:05:00::        0.87              animal   \n",
       "1852  1852.jpg           zipper%1:06:00::        0.88                 NaN   \n",
       "1853  1853.jpg         zucchini%1:13:00::        0.87           vegetable   \n",
       "\n",
       "      word_freq dominant_part  freq_1  \\\n",
       "0          28.0          Noun     NaN   \n",
       "1          97.0          Noun     NaN   \n",
       "2         735.0          Noun     NaN   \n",
       "3         692.0          Noun   238.0   \n",
       "4           NaN           NaN     NaN   \n",
       "...         ...           ...     ...   \n",
       "1849      597.0          Noun   143.0   \n",
       "1850     1224.0          Noun   108.0   \n",
       "1851      839.0          Noun   224.0   \n",
       "1852     1452.0          Noun    62.0   \n",
       "1853     1471.0          Noun     NaN   \n",
       "\n",
       "                                    WordNet_synonyms  freq_2 WordNet_ID  \\\n",
       "0     aardvark, ant_bear, anteater, Orycteropus_afer    21.0  n02082791   \n",
       "1                                             abacus    12.0  n02666196   \n",
       "2            accordion, piano_accordion, squeeze_box    67.0  n02672831   \n",
       "3                                              acorn    37.0  n12267677   \n",
       "4                  air_conditioner, air_conditioning     0.0  n02686379   \n",
       "...                                              ...     ...        ...   \n",
       "1849                                            yoke    22.0  n04612840   \n",
       "1850                                  egg_yolk, yolk    21.0  n07841345   \n",
       "1851                                           zebra   128.0  n02391049   \n",
       "1852       slide_fastener, zip, zipper, zip_fastener   144.0  n04238321   \n",
       "1853                             zucchini, courgette    49.0  n07716358   \n",
       "\n",
       "            Wordnet_ID3  word_freq_online  \n",
       "0            aardvark#1                53  \n",
       "1              abacus#2               188  \n",
       "2           accordion#1               816  \n",
       "3               acorn#1              1289  \n",
       "4     air_conditioner#1               943  \n",
       "...                 ...               ...  \n",
       "1849             yoke#7               692  \n",
       "1850             yolk#1               881  \n",
       "1851            zebra#1              1066  \n",
       "1852           zipper#1              1478  \n",
       "1853         zucchini#2              2098  \n",
       "\n",
       "[1854 rows x 19 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51bc6f1bc1248cb94e7ee00f8e30fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "activations:   0%|          | 0/1856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1146.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xr/8j65lqqn09q37xqx59yx9v480000gn/T/ipykernel_9606/1452753080.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# determine unique stimuli with numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0munique_stimuli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_stimuli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'presentation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'neuroid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/pytorch.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# cannot assign __call__ as attribute due to Python convention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, stimuli, layers, stimuli_identifier)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_stimulus_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimulus_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimuli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_identifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimuli_identifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimuli_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimuli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_identifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimuli_identifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_stimulus_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimulus_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_identifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/core.py\u001b[0m in \u001b[0;36mfrom_paths\u001b[0;34m(self, stimuli_paths, layers, stimuli_identifier)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# This is done here, before storing, so that we only store the reduced activations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mreduced_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimuli_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduced_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstimuli_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/core.py\u001b[0m in \u001b[0;36m_from_paths\u001b[0;34m(self, layers, stimuli_paths)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No layers passed to retrieve activations from\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running stimuli'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mlayer_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_activations_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimuli_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Packaging into assembly'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/core.py\u001b[0m in \u001b[0;36m_get_activations_batched\u001b[0;34m(self, paths, layers, batch_size)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mbatch_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mbatch_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mbatch_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_activations_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# copy to avoid handle re-enabling messing with the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mbatch_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/core.py\u001b[0m in \u001b[0;36m_get_batch_activations\u001b[0;34m(self, inputs, layer_names, batch_size)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batch_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_padding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mpreprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/pytorch.py\u001b[0m in \u001b[0;36mload_preprocess_images\u001b[0;34m(image_filepaths, image_size, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_preprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/pytorch.py\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(image_filepaths)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_filepath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_filepaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/pytorch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_filepath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_filepaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/brain-score/brainscore_vision/model_helpers/activations/pytorch.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(image_filepath)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'L'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'A'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0;34m'P'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# not binary and not alpha and not palletized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/bs_old/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1146.jpg'"
     ]
    }
   ],
   "source": [
    "# determine unique stimuli with numpy\n",
    "unique_stimuli = np.unique(triplets)\n",
    "features = activations_model(unique_stimuli, layers=layers)\n",
    "features = features.transpose('presentation', 'neuroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "assy = brainscore_vision.load_dataset(f'Hebart2023')\n",
    "triplets = np.array([assy['image_1'], assy['image_2'], assy['image_3']]).T\n",
    "triplets = triplets.reshape(-1)\n",
    "sample = triplets[:10*3]\n",
    "sim = calculate_similarity_matrix(features, similarity_measure='cosine')\n",
    "choices = calculate_choices(similarity_matrix=sim, triplets=sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
